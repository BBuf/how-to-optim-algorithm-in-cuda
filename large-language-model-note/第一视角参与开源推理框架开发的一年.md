
## 0x0. 前言

转眼2025又快结束了，时间真的太快了，这里先祝读者们新年快乐。去年年终总结我写下了MoE之年的总结和MoE 推理优化的一些认识(https://zhuanlan.zhihu.com/p/21251657579)的博客，主要是对比了当时的几个moe实现的区别吧，现在看有点过时了。今天把2025在开源社区做的一些工作汇总下，这一年自己在开源社区做的主要就是解决了一堆更复杂点的bug，然后尝试在框架层做了一些feature，以及对MoE模型和Diffusion模型的框架性能优化。不过更底层的kernel手法之类的倒是没什么长进，这个明年再尝试学一学。今年算是全年参与了SGLang开源框架的开发，以及在四季度参与了Cache-DIT的开发。然后也从SGLang contributor成为了框架的Core Dev，获得了更多review以及通过社区交友的机会。目前对于在家远程工作了4年多的我来说，可能开源社区对我来说最大的作用是交友和社交了hh。最后我把这篇总结命名为《第一视角参与开源框架开发的一年》。

SGLang 合影：

![](https://files.mdnice.com/user/59/75ce75f7-d2a3-42a6-bc25-1c52b0aaa1cd.png)


Cache-DIT合影：

![](https://files.mdnice.com/user/59/b67e1bb9-eb54-458d-86e1-cd133e2b0b6a.png)


## 印象深刻的事

### SGLang 开发

- 第一个应该就是参与了sgl-kernel的建设和迭代，在sgl-kernel里面写了一些kernel如moe_align_block_size，per_tensor quant, per_token/group quant以及持续优化，然后还review了很多社区的kernel pr以及通过sgl-kernel认识了一些超级厉害的cuda工程师例如 https://www.zhihu.com/people/anonymous-76-65-9 。然后就是年末在参与推进sgl-kernel的jit接口，这个以后有机会单独介绍一下。
- 第二个就是参与了一下DeepSeek V3/R1，单机双机小规模下的性能优化，在这之前我自己没怎么用过torch profiler，在这之后torch profiler是我分析问题的首选了。然后自己参与了一些优化工作例如DeepSeek V3/R1 shared experts fusion(https://github.com/sgl-project/sglang/pull/4918)，应用fused_moe_gate kernel fuse掉整个grouped_topk模块，fuse routed scaling factor到moe sum reduce kernel，以及移除或者fuse了一部分没必要的小kernel如moe_align_block_size kernel之前的zeros。当时还整理了一篇《单机H200最快DeepSeek V3和R1推理系统优化秘籍》(https://zhuanlan.zhihu.com/p/1906411749634188889) 展示社区所有人努力的结果。
- 第三个就是把flashinfer的 trt-llm allreduce fuse rms_norm_add kernel 引到SGLang并做token数调优等性能试验，最终在DeepSeek V3/R1，GPT-OSS，Qwen-MoE等模型上都能在token<=某个值例如4096的时候用上这个优化。这里影响深刻的原因主要是碰到了不少问题如IMA，性能退化以及这部分代码为了能更通用重构了多次。
- 第四个就是参与了一下PieceWise CUDA Graph的支持，碰到了一些torch compile的脏东西，对torch compile的敌意增加了一些。
- 第五个就是4季度的时候参与了一下Kimi K2 Thinking模型的调优，把性能提升了很多，Torch profiler TimeLime也做得很干净。印象深刻的原因也是因为有IMA以及先来看原来自己已经不知不觉就做了很多改动。

![](https://files.mdnice.com/user/59/da1d9f96-06e2-4cfb-b008-7f390057af0f.png)

- 最后一个应该就是最近参与的SGLang Diffusion优化了，Diffusion的性能还有很多需要改进和优化的，所以在Diffusion上很频繁了交了一些性能优化的PR优化一些流行模型如FLUX, Qwen-Image, Wan2.2等等。然后也见到了SGLang Diffsuion团队的专业，努力。虽然我个人认为目前还没有出现一个很让人满意的Diffusion推理框架，但我觉得只要继续努力下去SGLang Diffusion应该是可以做得比较接近的。我还做了几篇文章：《如何系统性定位并分析 PyTorch 模型推理中的性能瓶颈》(https://zhuanlan.zhihu.com/p/1987623785982076436) , 《通过零开销逐层权重卸载技术将SGLang Diffusion wan2.2的推理速度加速60%》(https://zhuanlan.zhihu.com/p/1986157623695922659)

### Cache-DIT

和@DefTruth有多很多交流，然后也在这个框架上开发了serving的功能。以及见证了Cache-DIT examples方面的易用性提升， Ulysses Anything Attention，Async Ulysses QKV Projection，FP8 Ulysses等feature的开发。这些都让我印象深刻，Cache-DIT没有去造复杂的轮子，也没有过度去侵入式的修改diffuser模型代码，但通过Cache和一些Diffusers系统中的优化和基于DTensor的并行优如TP/Ulysses，可以做到一个高性能且易用的Diffusion推理框架。最核心的Cache功能目前也被VLLM和SGLang采用以及Diffusers推荐，足以证明它的价值。

## 文章

除了框架开发也写了一些博客，可以在我的知乎主页(https://www.zhihu.com/people/zhang-xiao-yu-45-67-74/posts)或者GiantPandaLLM公众号找到，对于有需要的开发者比较推荐的有：

- 《如何系统性定位并分析 PyTorch 模型推理中的性能瓶颈》(https://zhuanlan.zhihu.com/p/1987623785982076436)
- 《记录下SGLang 开发，debug的几个技巧第二弹》(https://zhuanlan.zhihu.com/p/1984750078074839122)
- 《在 Cache-DiT 框架下实现原生 Serving功能》(https://zhuanlan.zhihu.com/p/1979699155266995922)
- 《记录下SGLang 开发，编译和Profile的几个小技巧》(https://zhuanlan.zhihu.com/p/1939041055208112436)
- 《一个Dispatch Dtype引起的fp8 quant kernel性能问题》(https://zhuanlan.zhihu.com/p/1933901514658808801)
- 《单机H200最快DeepSeek V3和R1推理系统优化秘籍》（https://zhuanlan.zhihu.com/p/1906411749634188889）

## 总结

回顾完毕，把2026端上来吧，祝大家新年快乐。













