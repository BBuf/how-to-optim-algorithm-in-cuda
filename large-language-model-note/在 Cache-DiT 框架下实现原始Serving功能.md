## 0x0. å‰è¨€

Cache-DiT æ˜¯å”¯å“ä¼šå¼€æºçš„ PyTorch åŸç”Ÿ DiT æ¨ç†åŠ é€Ÿå¼•æ“ï¼ˆhttps://github.com/vipshop/cache-ditï¼‰ï¼Œé€šè¿‡æ··åˆç¼“å­˜åŠ é€Ÿå’Œå¹¶è¡ŒåŒ–æŠ€æœ¯æ¥åŠ é€Ÿ DiT æ¨¡å‹çš„æ¨ç†ã€‚åœ¨ä¹‹å‰çš„ç‰ˆæœ¬ä¸­ï¼ŒCache-DiT ä¸»è¦èšç„¦äºç¦»çº¿æ¨ç†åœºæ™¯ï¼Œç”¨æˆ·éœ€è¦ç¼–å†™ Python è„šæœ¬æ¥è°ƒç”¨æ¨¡å‹ã€‚è™½ç„¶è¿™ç§æ–¹å¼å¯¹äºç ”ç©¶å’Œå®éªŒæ¥è¯´å¾ˆæ–¹ä¾¿ï¼Œä½†åœ¨ç”Ÿäº§ç¯å¢ƒä¸­éƒ¨ç½²æ—¶å°±æ˜¾å¾—ä¸å¤ªå‹å¥½äº†ã€‚

æœ€è¿‘ä¸º Cache-DiT å®ç°äº†å®Œæ•´çš„ Serving åŠŸèƒ½ï¼Œç›®æ ‡å¾ˆç®€å•ï¼šè®©ç”¨æˆ·å¯ä»¥åƒä½¿ç”¨ SGLang ä¸€æ ·ï¼Œé€šè¿‡ä¸€è¡Œå‘½ä»¤å¯åŠ¨æœåŠ¡ï¼Œç„¶åé€šè¿‡ HTTP API æ¥è°ƒç”¨æ¨¡å‹ç”Ÿæˆå›¾ç‰‡ã€‚è¿™ä¸ªåŠŸèƒ½æ”¯æŒå•å¡æ¨ç†ï¼Œä¹Ÿæ”¯æŒ Tensor Parallelism å’Œ Context Parallelism ç­‰åˆ†å¸ƒå¼æ¨ç†æ¨¡å¼ã€‚

è¿™ç¯‡æ–‡ç« ä¼šè¯¦ç»†ä»‹ç» Cache-DiT Serving çš„å®ç°è¿‡ç¨‹ï¼ŒåŒ…æ‹¬å¦‚ä½•å€Ÿé‰´ SGLang çš„è®¾è®¡ï¼Œåœ¨åˆ†å¸ƒå¼åœºæ™¯ä¸‹é‡åˆ°çš„ä¸€äº›å‘ï¼Œä»¥åŠæœ€ç»ˆçš„è§£å†³æ–¹æ¡ˆã€‚

## 0x1. ä¸ºä»€ä¹ˆéœ€è¦ Serving

åœ¨ç”Ÿäº§ç¯å¢ƒä¸­éƒ¨ç½²æ¨¡å‹æ—¶ï¼Œæˆ‘ä»¬é€šå¸¸å¸Œæœ›æœ‰ä¸€ä¸ªç»Ÿä¸€çš„ API æ¥å£ï¼Œè®©ä¸åŒçš„å®¢æˆ·ç«¯å¯ä»¥æ–¹ä¾¿åœ°è°ƒç”¨ã€‚æ¯”å¦‚ Web å‰ç«¯ã€ç§»åŠ¨ç«¯ Appã€æˆ–è€…å…¶ä»–åç«¯æœåŠ¡ï¼Œéƒ½å¯ä»¥é€šè¿‡ HTTP è¯·æ±‚æ¥ç”Ÿæˆå›¾ç‰‡ï¼Œè€Œä¸éœ€è¦å…³å¿ƒåº•å±‚çš„æ¨¡å‹åŠ è½½ã€GPU ç®¡ç†ç­‰ç»†èŠ‚ã€‚

å¦å¤–ï¼Œç»Ÿä¸€çš„æœåŠ¡ä¹Ÿæ–¹ä¾¿åšèµ„æºç®¡ç†å’Œç›‘æ§ã€‚æ¯”å¦‚å¯ä»¥é™åˆ¶å¹¶å‘è¯·æ±‚æ•°é‡ï¼Œè®°å½•æ¯ä¸ªè¯·æ±‚çš„è€—æ—¶å’Œèµ„æºå ç”¨ï¼Œå‡ºé—®é¢˜æ—¶ä¹Ÿæ›´å®¹æ˜“æ’æŸ¥ã€‚

Cache-DiT Serving å°±æ˜¯ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ã€‚å¸Œæœ›ç”¨æˆ·å¯ä»¥åƒä½¿ç”¨ SGLang çš„ `sglang.launch_server` ä¸€æ ·ç®€å•åœ°å¯åŠ¨æœåŠ¡ï¼Œç„¶åå°±å¯ä»¥é€šè¿‡ HTTP API æ¥è°ƒç”¨äº†ã€‚

## 0x2. æ•´ä½“æ¶æ„

å•å¡æ¨¡å¼çš„æ¶æ„æ¯”è¾ƒç®€å•ï¼Œå°±æ˜¯å¯åŠ¨ä¸€ä¸ª FastAPI æœåŠ¡å™¨ï¼Œæ¥æ”¶ HTTP è¯·æ±‚ï¼Œç„¶åè°ƒç”¨ ModelManager æ¥æ‰§è¡Œæ¨ç†ã€‚ModelManager è´Ÿè´£åŠ è½½æ¨¡å‹ã€ç®¡ç†ç¼“å­˜ã€æ‰§è¡Œæ¨ç†ç­‰å·¥ä½œã€‚

ä½†æ˜¯åœ¨åˆ†å¸ƒå¼æ¨¡å¼ä¸‹å°±å¤æ‚ä¸€äº›äº†ã€‚TP å’Œ CP è¿™ä¸¤ç§å¹¶è¡Œæ¨¡å¼æœ‰ä¸€ä¸ªå…±åŒçš„ç‰¹ç‚¹ï¼šæ‰€æœ‰ rank å¿…é¡»åŒæ—¶è°ƒç”¨ `pipe()` è¿›è¡Œæ¨ç†ã€‚è¿™æ˜¯å› ä¸º TP ä½¿ç”¨ NCCL çš„ all-reduce æ¥åŒæ­¥æ¢¯åº¦å’Œæ¿€æ´»å€¼ï¼ŒCP ä½¿ç”¨ all-to-all æ¥äº¤æ¢ attention çš„ KVã€‚å¦‚æœåªæœ‰ rank 0 è°ƒç”¨ `pipe()`ï¼Œå…¶ä»– rank ä¼šåœ¨ NCCL é€šä¿¡ä¸­ä¸€ç›´ç­‰å¾…ï¼Œæœ€ç»ˆå¯¼è‡´è¶…æ—¶æ­»é”ã€‚

æ‰€ä»¥éœ€è¦ä¸€ä¸ªæœºåˆ¶æ¥åŒæ­¥æ‰€æœ‰ rank çš„æ¨ç†è¯·æ±‚ã€‚æœ€ç®€å•çš„æ–¹æ¡ˆå°±æ˜¯ä½¿ç”¨ NCCL çš„ broadcastï¼šrank 0 æ¥æ”¶ HTTP è¯·æ±‚åï¼ŒæŠŠè¯·æ±‚å†…å®¹ broadcast ç»™æ‰€æœ‰å…¶ä»– rankï¼Œç„¶åæ‰€æœ‰ rank ä¸€èµ·æ‰§è¡Œæ¨ç†ã€‚æ‰§è¡Œå®Œæˆåï¼Œrank 0 æŠŠç»“æœè¿”å›ç»™å®¢æˆ·ç«¯ï¼Œå…¶ä»– rank ä¸¢å¼ƒç»“æœç»§ç»­ç­‰å¾…ä¸‹ä¸€ä¸ªè¯·æ±‚ã€‚

è¿™ä¸ªæ–¹æ¡ˆè™½ç„¶ç®€å•ï¼Œä½†å¯¹äº DiT æ¨¡å‹æ¥è¯´å·²ç»è¶³å¤Ÿäº†ã€‚å› ä¸º DiT æ¨¡å‹çš„æ¨ç†é€šå¸¸æ˜¯ä¸²è¡Œçš„ï¼Œä¸åƒ LLM é‚£æ ·éœ€è¦å¤æ‚çš„ continuous batching å’Œè°ƒåº¦ç³»ç»Ÿã€‚

## 0x3. å€Ÿé‰´ SGLang çš„è®¾è®¡

åœ¨å®ç° Cache-DiT Serving çš„è¿‡ç¨‹ä¸­ï¼Œä¸»è¦å‚è€ƒäº† SGLang çš„ generate éƒ¨åˆ†çš„ serving è®¾è®¡ã€‚SGLang çš„è¿™éƒ¨åˆ†å®ç°æ¯”è¾ƒç®€å•ç›´æ¥ï¼Œç›¸å¯¹å®¹æ˜“ç†è§£å’Œå€Ÿé‰´ã€‚

å…·ä½“æ¥è¯´ï¼Œå‚è€ƒäº† SGLang å¦‚ä½•ç”¨ FastAPI ç»„ç»‡ HTTP æ¥å£ï¼Œå¦‚ä½•è§£æå‘½ä»¤è¡Œå‚æ•°ï¼Œä»¥åŠå¦‚ä½•ç®¡ç†è¯·æ±‚çš„ç”Ÿå‘½å‘¨æœŸã€‚è¿™äº›åŸºç¡€çš„ HTTP Server æ¶æ„è®¾è®¡åœ¨ SGLang çš„ `http_server.py` å’Œ `launch_server.py` ä¸­éƒ½æœ‰æ¯”è¾ƒæ¸…æ™°çš„å®ç°ã€‚

å¯¹äºåˆ†å¸ƒå¼æ¨ç†çš„éƒ¨åˆ†ï¼Œè€ƒè™‘åˆ° DiT æ¨¡å‹çš„ç‰¹ç‚¹ï¼ˆä¸€æ¬¡æ€§ç”Ÿæˆå®Œæ•´å›¾ç‰‡ï¼Œä¸éœ€è¦é€ token ç”Ÿæˆï¼‰ï¼Œé‡‡ç”¨äº†æ›´ç®€å•çš„æ–¹æ¡ˆï¼šç›´æ¥ä½¿ç”¨ NCCL broadcast æ¥åŒæ­¥è¯·æ±‚ã€‚rank 0 è¿è¡Œ HTTP æœåŠ¡å™¨ï¼Œæ¥æ”¶åˆ°è¯·æ±‚åé€šè¿‡ NCCL broadcast æŠŠè¯·æ±‚å‘é€ç»™æ‰€æœ‰ rankï¼Œç„¶åæ‰€æœ‰ rank ä¸€èµ·æ‰§è¡Œæ¨ç†ã€‚è¿™æ ·æ—¢åˆ©ç”¨äº†ç°æœ‰çš„åˆ†å¸ƒå¼ç¯å¢ƒï¼ˆtorchrun å·²ç»å¸®å¿™ç®¡ç†å¥½äº†è¿›ç¨‹ï¼‰ï¼Œåˆé¿å…äº†é¢å¤–çš„è¿›ç¨‹é—´é€šä¿¡å¼€é”€ã€‚

## 0x4. æ ¸å¿ƒå®ç°

æ•´ä¸ªå®ç°çš„æ ¸å¿ƒåœ¨äºå¦‚ä½•åœ¨åˆ†å¸ƒå¼ç¯å¢ƒä¸‹åŒæ­¥æ‰€æœ‰ rank çš„æ¨ç†è¯·æ±‚ã€‚ä¸‹é¢è¯¦ç»†ä»‹ç»å‡ ä¸ªå…³é”®éƒ¨åˆ†çš„å®ç°ã€‚

é¦–å…ˆï¼Œå•å¡æ¨¡å¼çš„æ¶æ„éå¸¸ç®€å•ï¼Œå¦‚ä¸‹å›¾æ‰€ç¤ºï¼ˆè®©Claude 4ç”Ÿæˆçš„ï¼‰ï¼š

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Client    â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
       â”‚ HTTP Request
       â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   FastAPI Server    â”‚
â”‚  (Rank 0, Port 8000)â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   ModelManager      â”‚
â”‚  - load_model()     â”‚
â”‚  - generate()       â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  DiffusionPipeline  â”‚
â”‚  + Cache-DiT        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### å¯åŠ¨æµç¨‹

å¯åŠ¨æœåŠ¡çš„æµç¨‹æ¯”è¾ƒç›´è§‚ã€‚é¦–å…ˆè§£æå‘½ä»¤è¡Œå‚æ•°ï¼Œç„¶åæ ¹æ®æ˜¯å¦ä½¿ç”¨åˆ†å¸ƒå¼æ¥åˆå§‹åŒ–ç¯å¢ƒã€‚å¦‚æœä½¿ç”¨äº† TP æˆ– CPï¼Œå°±éœ€è¦è°ƒç”¨ `torch.distributed.init_process_group` æ¥åˆå§‹åŒ– NCCL é€šä¿¡ã€‚

æ¥ç€åˆ›å»º ModelManager å¹¶åŠ è½½æ¨¡å‹ã€‚è¿™é‡Œä¼šæ ¹æ®ç”¨æˆ·çš„é…ç½®æ¥å†³å®šæ˜¯å¦å¯ç”¨ç¼“å­˜ã€æ˜¯å¦ä½¿ç”¨ torch.compile ç­‰ã€‚

æœ€åæ ¹æ®å¹¶è¡Œç±»å‹æ¥é€‰æ‹©å¯åŠ¨æ–¹å¼ã€‚å¦‚æœæ˜¯åˆ†å¸ƒå¼æ¨¡å¼ï¼ˆTP/CPï¼‰ï¼Œrank 0 ä¼šå¯åŠ¨ HTTP æœåŠ¡å™¨ï¼Œå…¶ä»– rank ä¼šè¿è¡Œ worker å¾ªç¯ç­‰å¾…è¯·æ±‚ã€‚å¦‚æœæ˜¯å•å¡æ¨¡å¼ï¼Œå°±ç›´æ¥å¯åŠ¨ HTTP æœåŠ¡å™¨ã€‚

### Broadcast åŒæ­¥æœºåˆ¶

è¿™æ˜¯æ•´ä¸ªå®ç°çš„æ ¸å¿ƒã€‚éœ€è¦ç¡®ä¿æ‰€æœ‰ rank ä½¿ç”¨å®Œå…¨ç›¸åŒçš„è¯·æ±‚å‚æ•°ï¼ŒåŒ…æ‹¬ promptã€widthã€heightã€seed ç­‰ã€‚

å®ç°æ–¹å¼å¾ˆç›´æ¥ï¼šrank 0 å…ˆç”¨ pickle æŠŠè¯·æ±‚å¯¹è±¡åºåˆ—åŒ–æˆå­—èŠ‚æµï¼Œç„¶åé€šè¿‡ NCCL broadcast å‘é€ç»™æ‰€æœ‰ rankã€‚ä¸ºäº†è®©å…¶ä»– rank çŸ¥é“è¦æ¥æ”¶å¤šå°‘å­—èŠ‚ï¼Œå…ˆ broadcast ä¸€ä¸ªè¡¨ç¤ºå¤§å°çš„ tensorï¼Œç„¶åå† broadcast å®é™…çš„æ•°æ®ã€‚

æ‰€æœ‰ rank æ¥æ”¶åˆ°æ•°æ®åï¼Œéƒ½ç”¨ pickle ååºåˆ—åŒ–å¾—åˆ°ç›¸åŒçš„è¯·æ±‚å¯¹è±¡ï¼Œç„¶åä¸€èµ·è°ƒç”¨ `model_manager.generate()` æ‰§è¡Œæ¨ç†ã€‚æ‰§è¡Œå®Œæˆåï¼Œrank 0 æŠŠç»“æœè¿”å›ç»™å®¢æˆ·ç«¯ï¼Œå…¶ä»– rank ä¸¢å¼ƒç»“æœç»§ç»­ç­‰å¾…ä¸‹ä¸€ä¸ªè¯·æ±‚ã€‚

è¿™é‡Œæœ‰ä¸€ä¸ªç»†èŠ‚ï¼šè®© rank 0 ä¹Ÿååºåˆ—åŒ– broadcast çš„æ•°æ®ï¼Œè€Œä¸æ˜¯ç›´æ¥ä½¿ç”¨åŸå§‹çš„è¯·æ±‚å¯¹è±¡ã€‚è¿™æ ·å¯ä»¥ç¡®ä¿æ‰€æœ‰ rank ä½¿ç”¨çš„æ˜¯å®Œå…¨ç›¸åŒçš„å¯¹è±¡ï¼Œé¿å…å› ä¸º pickle åºåˆ—åŒ–/ååºåˆ—åŒ–çš„å·®å¼‚å¯¼è‡´çš„é—®é¢˜ã€‚

å¤§æ¦‚çš„æµç¨‹å¦‚ä¸‹ï¼Œè®©Claude 4ç”»äº†ä¸€ä¸‹ï¼š

```
Rank 0 (HTTP Server)              Rank 1, 2, ... (Workers)
      |                                  |
  å¯åŠ¨ FastAPI                         è¿è¡Œ worker å¾ªç¯
      |                                  |
  æ¥æ”¶ HTTP è¯·æ±‚                        ç­‰å¾… broadcast
      |                                  |
  broadcast è¯·æ±‚ --------NCCL-------->  æ¥æ”¶è¯·æ±‚
      |                                  |
  è°ƒç”¨ pipe() <--------åŒæ­¥æ¨ç†------> è°ƒç”¨ pipe()
      |              (all-reduce/all-to-all)
      |                                  |
  è¿”å›ç»“æœ                              ä¸¢å¼ƒç»“æœ
      |                                  |
  ç­‰å¾…ä¸‹ä¸€ä¸ªè¯·æ±‚                        ç­‰å¾…ä¸‹ä¸€ä¸ª broadcast
```


### éšæœºæ•°ç”Ÿæˆçš„å‘

åœ¨åˆ†å¸ƒå¼æ¨ç†ä¸­ï¼Œæ‰€æœ‰ rank å¿…é¡»ä½¿ç”¨ç›¸åŒçš„éšæœºç§å­ï¼Œå¦åˆ™ä¼šå‡ºç°å„ç§å¥‡æ€ªçš„é—®é¢˜ã€‚TP æ¨¡å¼ä¸‹å›¾ç‰‡ä¼šæ•´ä½“æ¨¡ç³Šï¼ŒCP æ¨¡å¼ä¸‹å›¾ç‰‡çš„ä¸‹åŠéƒ¨åˆ†ä¼šå˜æˆä¹±ç ã€‚

è¿™ä¸ªé—®é¢˜çš„æ ¹æºåœ¨äº PyTorch çš„ CUDA RNG çŠ¶æ€æ˜¯ per-device çš„ã€‚ä¹Ÿå°±æ˜¯è¯´ï¼Œå³ä½¿ä½ åœ¨ cuda:0 å’Œ cuda:1 ä¸Šç”¨ç›¸åŒçš„ seed åˆ›å»º generatorï¼Œå®ƒä»¬ç”Ÿæˆçš„éšæœºæ•°åºåˆ—ä¹Ÿæ˜¯ä¸åŒçš„ã€‚

è§£å†³æ–¹æ¡ˆæ˜¯ä½¿ç”¨ CPU generatorã€‚CPU çš„ RNG çŠ¶æ€æ˜¯å…¨å±€çš„ï¼Œæ‰€ä»¥æ‰€æœ‰ rank ç”¨ç›¸åŒçš„ seed åˆ›å»º CPU generator æ—¶ï¼Œä¼šç”Ÿæˆå®Œå…¨ç›¸åŒçš„éšæœºæ•°åºåˆ—ã€‚diffusers ä¼šè‡ªåŠ¨æŠŠè¿™äº›éšæœºæ•°ç§»åŠ¨åˆ°æ­£ç¡®çš„ GPU ä¸Šï¼Œæ‰€ä»¥ä¸ç”¨æ‹…å¿ƒæ€§èƒ½é—®é¢˜ã€‚

å¦å¤–ï¼Œå¦‚æœç”¨æˆ·æ²¡æœ‰æä¾› seedï¼Œåœ¨åˆ†å¸ƒå¼æ¨¡å¼ä¸‹ä¼šè‡ªåŠ¨ç”Ÿæˆä¸€ä¸ªå›ºå®šçš„ seedï¼ˆæ¯”å¦‚ 42ï¼‰ã€‚è¿™æ ·å¯ä»¥ç¡®ä¿å³ä½¿ç”¨æˆ·å¿˜è®°è®¾ç½® seedï¼Œä¹Ÿä¸ä¼šå‡ºç°å›¾ç‰‡æ¨¡ç³Šæˆ–ä¹±ç çš„é—®é¢˜ã€‚

### Device æ”¾ç½®çš„å‘

è¿˜æœ‰ä¸€ä¸ªå®¹æ˜“è¸©çš„å‘æ˜¯ device æ”¾ç½®ã€‚åœ¨ CP æ¨¡å¼ä¸‹ï¼Œall-to-all é€šä¿¡éœ€è¦æ‰€æœ‰ tensor éƒ½åœ¨ GPU ä¸Šã€‚ä½†æ˜¯å¦‚æœ pipeline çš„æŸäº›ç»„ä»¶ï¼ˆæ¯”å¦‚ VAEã€text encoderï¼‰è¿˜åœ¨ CPU ä¸Šï¼Œå°±ä¼šæŠ¥é”™è¯´ "No backend type associated with device type cpu"ã€‚

è§£å†³æ–¹æ¡ˆæ˜¯åœ¨æ‰€æœ‰æ¨¡å¼ä¸‹éƒ½è°ƒç”¨ `pipe.to("cuda")`ã€‚è™½ç„¶ TP æ¨¡å¼ä¸‹ transformer å·²ç»è¢«åˆ‡åˆ†åˆ°å¤šä¸ª GPU ä¸Šäº†ï¼Œä½†å…¶ä»–ç»„ä»¶è¿˜éœ€è¦æ‰‹åŠ¨ç§»åŠ¨ã€‚CP æ¨¡å¼ä¸‹æ›´æ˜¯æ‰€æœ‰ç»„ä»¶éƒ½éœ€è¦åœ¨ GPU ä¸Šã€‚

## 0x5. æ€»ç»“è¸©è¿‡çš„å‘

åœ¨å®ç°è¿‡ç¨‹ä¸­è¸©äº†ä¸å°‘å‘ï¼Œè¿™é‡Œæ€»ç»“ä¸€ä¸‹ä¸»è¦çš„å‡ ä¸ªï¼š

ç¬¬ä¸€ä¸ªå‘æ˜¯ TP/CP æ­»é”ã€‚ä¸€å¼€å§‹ä»¥ä¸º CP æ¨¡å¼ä¸éœ€è¦ broadcast æœºåˆ¶ï¼Œå› ä¸ºçœ‹èµ·æ¥ CP çš„ forward pattern å’Œå•å¡æ˜¯ä¸€æ ·çš„ã€‚ç»“æœå¯åŠ¨æœåŠ¡åå‘ç° rank 0 ä¸€ç›´å¡åœ¨æ¨ç†ä¸­ï¼Œrank 1 åœ¨ç¡è§‰ã€‚åæ¥æ‰æ„è¯†åˆ° CP çš„ all-to-all é€šä¿¡ä¹Ÿéœ€è¦æ‰€æœ‰ rank åŒæ—¶å‚ä¸ï¼Œæ‰€ä»¥å¿…é¡»ç”¨ broadcast æ¥åŒæ­¥è¯·æ±‚ã€‚

ç¬¬äºŒä¸ªå‘æ˜¯å›¾ç‰‡æ¨¡ç³Šå’Œä¹±ç ã€‚TP æ¨¡å¼ä¸‹ç”Ÿæˆçš„å›¾ç‰‡æ•´ä½“éƒ½æ˜¯æ¨¡ç³Šçš„ï¼ŒCP æ¨¡å¼ä¸‹å›¾ç‰‡çš„ä¸‹åŠéƒ¨åˆ†æ˜¯ä¹±ç ã€‚debug äº†å¾ˆä¹…æ‰å‘ç°æ˜¯éšæœºæ•°ç”Ÿæˆçš„é—®é¢˜ã€‚ä¸€å¼€å§‹ç”¨çš„æ˜¯ GPU generatorï¼Œåæ¥å‘ç°ä¸åŒ GPU ä¸Šçš„ generator å³ä½¿ç”¨ç›¸åŒçš„ seed ä¹Ÿä¼šç”Ÿæˆä¸åŒçš„éšæœºæ•°ã€‚æ”¹æˆ CPU generator åé—®é¢˜å°±è§£å†³äº†ã€‚

ç¬¬ä¸‰ä¸ªå‘æ˜¯ rank 0 æ²¡æœ‰ä½¿ç”¨ broadcast çš„è¯·æ±‚ã€‚ä¸€å¼€å§‹è®© rank 0 ç›´æ¥ä½¿ç”¨åŸå§‹çš„è¯·æ±‚å¯¹è±¡ï¼Œå…¶ä»– rank ä½¿ç”¨ broadcast æ¥æ”¶åˆ°çš„è¯·æ±‚ã€‚ç»“æœå‘ç° CP æ¨¡å¼ä¸‹å›¾ç‰‡è¿˜æ˜¯æœ‰é—®é¢˜ã€‚åæ¥æ„è¯†åˆ°å¯èƒ½æ˜¯ pickle åºåˆ—åŒ–/ååºåˆ—åŒ–çš„å·®å¼‚å¯¼è‡´çš„ï¼Œæ”¹æˆæ‰€æœ‰ rank éƒ½ä½¿ç”¨ broadcast çš„è¯·æ±‚åå°±æ­£å¸¸äº†ã€‚

ç¬¬å››ä¸ªå‘æ˜¯ä¸åŒ pipeline æ”¯æŒçš„å‚æ•°ä¸ä¸€æ ·ã€‚åœ¨æµ‹è¯• FLUX2 æ¨¡å‹æ—¶å‘ç°ä¼šæŠ¥é”™ `Flux2Pipeline.__call__() got an unexpected keyword argument 'negative_prompt'`ã€‚åŸæ¥ FLUX2 çš„ pipeline ä¸æ”¯æŒ `negative_prompt` å‚æ•°ï¼Œä½†ä»£ç é‡Œé»˜è®¤ä¼ äº†è¿™ä¸ªå‚æ•°ã€‚è§£å†³æ–¹æ¡ˆæ˜¯é€šè¿‡ `inspect.signature` æ£€æŸ¥ pipeline çš„ `__call__` æ–¹æ³•æ”¯æŒå“ªäº›å‚æ•°ï¼Œåªä¼ é€’å®ƒæ”¯æŒçš„å‚æ•°ã€‚è¿™æ ·å°±å¯ä»¥å…¼å®¹ä¸åŒçš„ pipeline äº†ã€‚

## 0x6. ä½¿ç”¨æ–¹æ³•

ä½¿ç”¨èµ·æ¥éå¸¸ç®€å•ï¼Œå’Œ SGLang çš„ä½“éªŒåŸºæœ¬ä¸€è‡´ã€‚

é¦–å…ˆå®‰è£… Cache-DiTï¼š

```bash
git clone https://github.com/vipshop/cache-dit.git
cd cache-dit
pip install -e .
```

ç„¶åå¯åŠ¨æœåŠ¡ã€‚å•å¡æ¨¡å¼ï¼š

```bash
cache-dit-serve \
    --model-path black-forest-labs/FLUX.1-dev \
    --cache \
    --host 0.0.0.0 \
    --port 8000
```

TP æ¨¡å¼ï¼ˆ2å¡ï¼‰ï¼š

```bash
torchrun --nproc_per_node=2 -m cache_dit.serve.serve \
    --model-path black-forest-labs/FLUX.1-dev \
    --cache \
    --parallel-type tp \
    --host 0.0.0.0 \
    --port 8000
```

CP æ¨¡å¼ï¼ˆ2å¡ï¼‰ï¼š

```bash
torchrun --nproc_per_node=2 -m cache_dit.serve.serve \
    --model-path black-forest-labs/FLUX.1-dev \
    --cache \
    --parallel-type ulysses \
    --host 0.0.0.0 \
    --port 8000
```

å¯åŠ¨åå°±å¯ä»¥é€šè¿‡ HTTP API æ¥è°ƒç”¨äº†ã€‚Python å®¢æˆ·ç«¯ç¤ºä¾‹ï¼š

```python
import requests
import base64
from PIL import Image
from io import BytesIO

url = "http://localhost:8000/generate"
data = {
    "prompt": "A beautiful sunset over the ocean",
    "width": 1024,
    "height": 1024,
    "num_inference_steps": 28,
    "guidance_scale": 3.5,
    "seed": 42,
}

response = requests.post(url, json=data)
result = response.json()

# è§£ç å›¾ç‰‡
image_data = base64.b64decode(result["images"][0])
image = Image.open(BytesIO(image_data))
image.save("output.png")

print(f"Time cost: {result['time_cost']:.2f}s")
```

ä¹Ÿå¯ä»¥ç”¨ cURLï¼š

```bash
curl -X POST http://localhost:8000/generate \
  -H "Content-Type: application/json" \
  -d '{
    "prompt": "A beautiful sunset over the ocean",
    "width": 1024,
    "height": 1024,
    "num_inference_steps": 28,
    "guidance_scale": 3.5,
    "seed": 42
  }'
```

å¯åŠ¨æœåŠ¡åï¼Œè®¿é—® `http://localhost:8000/docs` å¯ä»¥çœ‹åˆ°å®Œæ•´çš„ API æ–‡æ¡£ï¼ˆSwagger UIï¼‰ã€‚

å®Œæ•´çš„å‘½ä»¤è¡Œå‚æ•°å¯ä»¥æŸ¥çœ‹ [get_args å‡½æ•°](https://github.com/vipshop/cache-dit/pull/522/files#diff-8d807db087ac7dc3923b8b6c6c4af29c87f8f29882f19ca5a9bf33f9b3d608b6R17)ï¼Œè¿™é‡Œåˆ—ä¸¾å‡ ä¸ªå¸¸ç”¨çš„ï¼š

- `--model-path`: æ¨¡å‹è·¯å¾„æˆ– HuggingFace model ID
- `--cache`: å¯ç”¨ç¼“å­˜åŠ é€Ÿ
- `--parallel-type`: å¹¶è¡Œç±»å‹ (tp/ulysses/ring)
- `--compile`: å¯ç”¨ torch.compile
- `--host`: æœåŠ¡å™¨åœ°å€ (é»˜è®¤ 0.0.0.0)
- `--port`: æœåŠ¡å™¨ç«¯å£ (é»˜è®¤ 8000)

å¦‚æœæƒ³è¦æ›´å¿«çš„æ¨ç†é€Ÿåº¦ï¼Œå¯ä»¥å¯ç”¨ torch.compileï¼š

```bash
cache-dit-serve --model-path FLUX.1-dev --cache --compile
```

é¦–æ¬¡æ¨ç†ä¼šè¿›è¡Œç¼–è¯‘ï¼ˆæ¯”è¾ƒæ…¢ï¼‰ï¼Œä½†åç»­æ¨ç†ä¼šå¿«å¾ˆå¤šã€‚

è´´ä¸€ä¸ªFLUX.1.devçš„ä¾‹å­ã€‚

- serverç«¯log


```markdown
cache-dit-serve --model-path /nas/bbuf/FLUX.1-dev/ --cache --compile
WARNING 12-03 06:50:00 [_attention_dispatch.py:303] Re-registered NATIVE attention backend to enable context parallelism with attn mask. You can disable this behavior by export env: export CACHE_DIT_ENABLE_CUSTOM_CP_NATIVE_ATTN_DISPATCH=0.
INFO 12-03 06:50:00 [_attention_dispatch.py:416] Registered new attention backend: _SDPA_CUDNN, to enable context parallelism with attn mask. You can disable it by: export CACHE_DIT_ENABLE_CUSTOM_CP_NATIVE_ATTN_DISPATCH=0.
INFO 12-03 06:50:01 [serve.py:107] Initializing model manager...
INFO 12-03 06:50:01 [model_manager.py:68] Initializing ModelManager: model_path=/nas/bbuf/FLUX.1-dev/, device=cuda
INFO 12-03 06:50:01 [serve.py:119] Loading model...
INFO 12-03 06:50:01 [model_manager.py:72] Loading model: /nas/bbuf/FLUX.1-dev/
Loading pipeline components...:   0%|                                                                       | 0/7 [00:00<?, ?it/s]`torch_dtype` is deprecated! Use `dtype` instead!
Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:01<00:00,  1.05it/s]
Loading pipeline components...:  29%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                                             | 2/7 [00:02<00:05,  1.08s/it]You set `add_prefix_space`. The tokenizer needs to be converted from the slow tokenizers
Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:04<00:00,  1.49s/it]
Loading pipeline components...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7/7 [00:08<00:00,  1.28s/it]
INFO 12-03 06:50:10 [model_manager.py:81] Enabling DBCache acceleration
INFO 12-03 06:50:10 [cache_adapter.py:49] FluxPipeline is officially supported by cache-dit. Use it's pre-defined BlockAdapter directly!
INFO 12-03 06:50:10 [functor_flux.py:61] Applied FluxPatchFunctor for FluxTransformer2DModel, Patch: False.
INFO 12-03 06:50:10 [block_adapters.py:147] Found transformer from diffusers: diffusers.models.transformers.transformer_flux enable check_forward_pattern by default.
INFO 12-03 06:50:10 [block_adapters.py:494] Match Block Forward Pattern: ['FluxSingleTransformerBlock', 'FluxTransformerBlock'], ForwardPattern.Pattern_1
INFO 12-03 06:50:10 [block_adapters.py:494] IN:('hidden_states', 'encoder_hidden_states'), OUT:('encoder_hidden_states', 'hidden_states'))
INFO 12-03 06:50:10 [cache_adapter.py:148] Use custom 'enable_separate_cfg' from cache context kwargs: True. Pipeline: FluxPipeline.
INFO 12-03 06:50:10 [cache_adapter.py:307] Collected Context Config: DBCache_F8B0_W8I1M0MC0_R0.08, Calibrator Config: None
INFO 12-03 06:50:10 [pattern_base.py:70] Match Blocks: CachedBlocks_Pattern_0_1_2, for transformer_blocks, cache_context: transformer_blocks_139774198646688, context_manager: FluxPipeline_139774199622368.
INFO 12-03 06:50:10 [model_manager.py:100] Moving pipeline to CUDA
INFO 12-03 06:52:33 [model_manager.py:108] Enabling torch.compile
INFO 12-03 06:52:33 [model_manager.py:112] Model loaded successfully
INFO 12-03 06:52:33 [serve.py:121] Model loaded successfully!
INFO 12-03 06:52:33 [serve.py:125] Starting server at http://0.0.0.0:8000
INFO 12-03 06:52:33 [serve.py:126] API docs at http://0.0.0.0:8000/docs
INFO:     Started server process [1928284]
INFO:     Waiting for application startup.
INFO:     Application startup complete.
INFO:     Uvicorn running on http://0.0.0.0:8000 (Press CTRL+C to quit)
INFO 12-03 06:52:45 [model_manager.py:117] Warming up for shape 1024x1024...
  0%|                                                                                                       | 0/4 [00:00<?, ?it/s]/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/functions.py:1547: UserWarning: Dynamo detected a call to a `functools.lru_cache` wrapped function.Dynamo currently ignores `functools.lru_cache` and directly traces the wrapped function.`functools.lru_cache` wrapped functions that read outside state may not be traced soundly.
  warnings.warn(
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/functions.py:1547: UserWarning: Dynamo detected a call to a `functools.lru_cache` wrapped function.Dynamo currently ignores `functools.lru_cache` and directly traces the wrapped function.`functools.lru_cache` wrapped functions that read outside state may not be traced soundly.
  warnings.warn(
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/functions.py:1547: UserWarning: Dynamo detected a call to a `functools.lru_cache` wrapped function.Dynamo currently ignores `functools.lru_cache` and directly traces the wrapped function.`functools.lru_cache` wrapped functions that read outside state may not be traced soundly.
  warnings.warn(
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/functions.py:1547: UserWarning: Dynamo detected a call to a `functools.lru_cache` wrapped function.Dynamo currently ignores `functools.lru_cache` and directly traces the wrapped function.`functools.lru_cache` wrapped functions that read outside state may not be traced soundly.
  warnings.warn(
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/functions.py:1547: UserWarning: Dynamo detected a call to a `functools.lru_cache` wrapped function.Dynamo currently ignores `functools.lru_cache` and directly traces the wrapped function.`functools.lru_cache` wrapped functions that read outside state may not be traced soundly.
  warnings.warn(
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:06<00:00,  1.51s/it]
INFO 12-03 06:52:53 [model_manager.py:127] Warmup completed for 1024x1024
INFO 12-03 06:52:53 [model_manager.py:137] Generating image: prompt='A beautiful sunset over the ocean...'
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 50/50 [00:06<00:00,  7.98it/s]
WARNING 12-03 06:53:00 [summary.py:275] Can't find Context Options for: FluxSingleTransformerBlock
WARNING 12-03 06:53:00 [summary.py:284] Can't find Parallelism Config for: FluxSingleTransformerBlock
WARNING 12-03 06:53:00 [summary.py:275] Can't find Context Options for: FluxTransformerBlock
WARNING 12-03 06:53:00 [summary.py:284] Can't find Parallelism Config for: FluxTransformerBlock

ğŸ¤—Context Options: OptimizedModule

{'cache_config': DBCacheConfig(cache_type=<CacheType.DBCache: 'DBCache'>, Fn_compute_blocks=8, Bn_compute_blocks=0, residual_diff_threshold=0.08, max_accumulated_residual_diff_threshold=None, max_warmup_steps=8, warmup_interval=1, max_cached_steps=-1, max_continuous_cached_steps=-1, enable_separate_cfg=True, cfg_compute_first=False, cfg_diff_compute_separate=True, num_inference_steps=None, steps_computation_mask=None, steps_computation_policy='dynamic'), 'name': 'transformer_blocks_139774198646688'}
WARNING 12-03 06:53:00 [summary.py:284] Can't find Parallelism Config for: OptimizedModule

âš¡ï¸Cache Steps and Residual Diffs Statistics: OptimizedModule

| Cache Steps | Diffs P00 | Diffs P25 | Diffs P50 | Diffs P75 | Diffs P95 | Diffs Min | Diffs Max |
|-------------|-----------|-----------|-----------|-----------|-----------|-----------|-----------|
| 6           | 0.043     | 0.06      | 0.089     | 0.135     | 0.217     | 0.043     | 0.285     |


âš¡ï¸CFG Cache Steps and Residual Diffs Statistics: OptimizedModule

| CFG Cache Steps | Diffs P00 | Diffs P25 | Diffs P50 | Diffs P75 | Diffs P95 | Diffs Min | Diffs Max |
|-----------------|-----------|-----------|-----------|-----------|-----------|-----------|-----------|
| 6               | 0.043     | 0.055     | 0.097     | 0.144     | 0.266     | 0.043     | 0.373     |

INFO 12-03 06:53:00 [model_manager.py:183] Image generation completed in 6.55s
INFO:     127.0.0.1:55144 - "POST /generate HTTP/1.1" 200 OK
```

- clientç«¯log



```markdwon
 python -m cache_dit.serve.client \
    --prompt "A beautiful sunset over the ocean" \
    --width 1024 \
    --height 1024 \
    --steps 50 \
    --output output.png
WARNING 12-03 06:48:43 [_attention_dispatch.py:303] Re-registered NATIVE attention backend to enable context parallelism with attn mask. You can disable this behavior by export env: export CACHE_DIT_ENABLE_CUSTOM_CP_NATIVE_ATTN_DISPATCH=0.
INFO 12-03 06:48:43 [_attention_dispatch.py:416] Registered new attention backend: _SDPA_CUDNN, to enable context parallelism with attn mask. You can disable it by: export CACHE_DIT_ENABLE_CUSTOM_CP_NATIVE_ATTN_DISPATCH=0.
Generating image: A beautiful sunset over the ocean
Image saved to output.png
Cache stats: {'cache_stats': [{'cache_options': "{'cache_config': DBCacheConfig(cache_type=<CacheType.DBCache: 'DBCache'>, Fn_compute_blocks=8, Bn_compute_blocks=0, residual_diff_threshold=0.08, max_accumulated_residual_diff_threshold=None, max_warmup_steps=8, warmup_interval=1, max_cached_steps=-1, max_continuous_cached_steps=-1, enable_separate_cfg=True, cfg_compute_first=False, cfg_diff_compute_separate=True, num_inference_steps=None, steps_computation_mask=None, steps_computation_policy='dynamic'), 'name': 'transformer_blocks_140121591103456'}", 'cached_steps': [8, 10, 12, 14, 16, 18], 'parallelism_config': None}]}
Time cost: 9.22s
root@264a63f2d86e:/nas/bbuf/cache-dit# curl -X POST http://localhost:8000/generate \
  -H "Content-Type: application/json" \
  -d '{
    "prompt": "A beautiful sunset over the ocean",
    "width": 1024,
    "height": 1024,
    "num_inference_steps": 50
  }' | jq -r '.images[0]' | base64 -d > output.png
  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current
                                 Dload  Upload   Total   Spent    Left  Speed
100 1666k  100 1666k  100   125   173k     13  0:00:09  0:00:09 --:--:--  379k
```

- ç»“æœ

![](https://files.mdnice.com/user/59/3b471855-3c19-406b-9ae7-52fa7c141cdc.jpg)

## 0x7. æ€»ç»“

Cache-DiT Serving çš„å®ç°ç›®æ ‡å¾ˆç®€å•ï¼šè®©ç”¨æˆ·å¯ä»¥åƒä½¿ç”¨ SGLang ä¸€æ ·æ–¹ä¾¿åœ°éƒ¨ç½² DiT æ¨¡å‹ã€‚é€šè¿‡å€Ÿé‰´ SGLang çš„è®¾è®¡ï¼Œå¹¶é’ˆå¯¹ DiT æ¨¡å‹çš„ç‰¹ç‚¹è¿›è¡Œç®€åŒ–ï¼ŒåŠªåŠ›å®ç°äº†ä¸€ä¸ªè½»é‡çº§ä½†åŠŸèƒ½è¿˜ç®—å®Œæ•´çš„æ¨ç†æœåŠ¡ã€‚æ•´ä¸ªå®ç°çš„æ ¸å¿ƒåœ¨äºåˆ†å¸ƒå¼æ¨ç†çš„åŒæ­¥æœºåˆ¶ã€‚é€šè¿‡ä½¿ç”¨ NCCL broadcast æ¥åŒæ­¥è¯·æ±‚ï¼Œé¿å…äº†å¤æ‚çš„å¤šè¿›ç¨‹æ¶æ„ï¼ŒåŒæ—¶ä¿è¯äº† TP å’Œ CP æ¨¡å¼ä¸‹çš„æ­£ç¡®æ€§ã€‚æˆ‘åœ¨å®ç°è¿‡ç¨‹ä¸­è¸©äº†ä¸å°‘å‘ï¼Œç‰¹åˆ«æ˜¯éšæœºæ•°ç”Ÿæˆå’Œ device æ”¾ç½®çš„é—®é¢˜ï¼Œæœ€ç»ˆéƒ½èŠ±æ—¶é—´æå®šäº†ã€‚ç„¶åç›®å‰è¿™ä¸ªåŠŸèƒ½å·²ç»å¯ä»¥æ­£å¸¸å·¥ä½œäº†ï¼Œæ”¯æŒå•å¡ã€TP å’Œ CP ä¸‰ç§æ¨¡å¼ã€‚åç»­ä¼šåšæ›´å¤šæµ‹è¯•å’Œæ¨è¿›mergeè¿›ä¸»åˆ†æ”¯ã€‚serverç«¯çš„profilerç­‰ä»¥åä¹Ÿä¼šèŠ±æ—¶é—´æå®šã€‚

æ¬¢è¿å¤§å®¶è¯•ç”¨å¹¶æä¾›åé¦ˆï¼


## å‚è€ƒèµ„æ–™

- Cache-DiT GitHub: https://github.com/vipshop/cache-dit
- SGLang GitHub: https://github.com/sgl-project/sglang
- Cache-DiT å­¦ä¹ ç¬”è®°: https://github.com/BBuf/how-to-optim-algorithm-in-cuda/blob/master/large-language-model-note/Cache-Dit%20%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0.md
