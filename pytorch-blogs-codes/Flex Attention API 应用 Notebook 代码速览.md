> å¯¹FlexAttentionçš„å¸¸è§APIçš„ä½¿ç”¨æ–¹æ³•åšä¸€ä¸ªè§£è¯»ï¼Œåšå®¢æ¥æºï¼šhttps://github.com/pytorch-labs/attention-gym/blob/main/examples/flex_attn.ipynb ï¼Œåœ¨æ­¤åŸºç¡€ä¸Šæˆ‘å¯¹éƒ¨åˆ†ä»£ç æ·»åŠ äº†ä¸€äº›è§£é‡Šï¼Œä¿®å¤äº†å‡ ä¸ªä»£ç ä¸­çš„bugå¹¶ä½¿ç”¨PyTorchçš„nightlyç‰ˆæœ¬è¿è¡Œäº†ç¤ºä¾‹ï¼Œå¾—åˆ°äº†æ¯ä¸ªcustom attentionçš„è¾“å‡ºï¼Œå±•ç¤ºåœ¨äº†ä¸‹é¢çš„æ¯ä¸ªç¤ºä¾‹ä»£ç åé¢ã€‚æœ€åè¿˜è¡¥å……äº†ä¸€ä¸‹torch compile inductoråç«¯ä¸­å®ç°FlexAttentionçš„å…¥å£çš„ä»£ç æµè§ˆã€‚

# FlexAttention API ä½¿ç”¨ NoteBook

æœ¬ç¬”è®°æœ¬æ¼”ç¤ºäº†æ–°çš„ FlexAttention API çš„ä½¿ç”¨æ–¹æ³•ï¼Œè¯¥ API å…è®¸ç”¨æˆ·æŒ‡å®šå¯¹ç¼©æ”¾ç‚¹ç§¯æ³¨æ„åŠ›ï¼ˆSDPAï¼‰ä¸­è®¡ç®—çš„æ³¨æ„åŠ›åˆ†æ•°è¿›è¡Œä¿®æ”¹ã€‚

## ç›®å½•

1. [ä»‹ç»](#ä»‹ç»)
2. [è®¾ç½®](#è®¾ç½®)
3. [åŸºæœ¬ç”¨æ³•](#åŸºæœ¬ç”¨æ³•)
4. [åˆ†æ•°ä¿®æ”¹ vs åˆ†æ•°æ©ç ](#åˆ†æ•°ä¿®æ”¹vsåˆ†æ•°æ©ç )
5. [åˆ†æ•°ä¿®æ”¹ç¤ºä¾‹](#åˆ†æ•°ä¿®æ”¹ç¤ºä¾‹)
   - [å…¨æ³¨æ„åŠ›ï¼ˆæ— æ“ä½œï¼‰](#å…¨æ³¨æ„åŠ›)
   - [æ ‡å‡†å› æœæ©ç ](#æ ‡å‡†å› æœæ©ç )
   - [æ»‘åŠ¨çª—å£æ³¨æ„åŠ›](#æ»‘åŠ¨çª—å£æ³¨æ„åŠ›)
   - [å‰ç¼€ LMï¼ˆåŒå‘ + å› æœï¼‰](#prefix-lm-bidirectional-causal)
   - [æ–‡æ¡£æ©ç ](#æ–‡æ¡£æ©ç )
   - [NATTEN æ©ç ](#natten-masking)
   - [Alibi åç½®](#alibi-bias)
   - [Tanh è½¯ä¸Šé™](#tanh-soft-capping)
   - [åµŒå¥—ä¸è§„åˆ™å¼ é‡](#nested-jagged-tensor)
   - [Flamingo äº¤å‰æ³¨æ„åŠ›](#flamingo-cross-attention)

## ä»‹ç»

FlexAttention API å…è®¸ç”¨æˆ·åœ¨Fused Scaled Dot Product Attention Kernelä¸­æŒ‡å®šå¯¹æ³¨æ„åŠ›åˆ†æ•°çš„è‡ªå®šä¹‰ä¿®æ”¹ã€‚è¿™ä½¿å¾—å„ç§æ³¨æ„åŠ›æ¨¡å¼å’Œåç½®èƒ½å¤Ÿé«˜æ•ˆåœ°å®ç°ï¼Œå¹¶å…·æœ‰æ½œåœ¨çš„è¿è¡Œæ—¶å’Œå†…å­˜èŠ‚çœã€‚API è¿˜å°†æ ¹æ®ç”¨æˆ·å®šä¹‰çš„ä¿®æ”¹ç”Ÿæˆèåˆçš„åå‘kernelã€‚

## è®¾ç½®
é¦–å…ˆï¼Œè®©æˆ‘ä»¬å¯¼å…¥å¿…è¦çš„åº“å¹¶è®¾ç½®æˆ‘ä»¬çš„ç¯å¢ƒã€‚

```python
import random
from functools import lru_cache, partial

import torch
import torch.nn.functional as F

from tabulate import tabulate
from torch.nn.attention.flex_attention import (
    _DEFAULT_SPARSE_BLOCK_SIZE,
    create_block_mask,
    create_mask,
    flex_attention,
)
from triton.testing import do_bench

torch.set_default_device("cuda")
torch.manual_seed(0)

torch._dynamo.config.cache_size_limit = 1000

# Compile the flex_attention function
flex_attention = torch.compile(flex_attention, dynamic=False)

# For better performance, you can use:
# flex_attention = torch.compile(_flex_attention, dynamic=False, mode="max-autotune-no-cudagraphs")

data_type = torch.float16

# The kernels will utilize block sparisty to increase performance
print(f"Using the default sparsity block size: {_DEFAULT_SPARSE_BLOCK_SIZE}")
```

æˆ‘ä»¬å°†å®šä¹‰ä¸€äº›æœ‰ç”¨çš„æµ‹è¯•å·¥å…·ï¼Œè¿™äº›å·¥å…·å°†æ‰“å°score_modå‡½æ•°å’Œmask_fnçš„å—ç¨€ç–è¡¨ç¤ºã€‚

æ­¤å¤–ï¼Œå®ƒå°†æ¯”è¾ƒä»¥ä¸‹å‡ ç§å®ç°çš„æ€§èƒ½ï¼š

- FlexAttention
- ä¸€ç§FlashAttentionV2çš„SOTAå®ç°ï¼Œå¸¦æœ‰å› æœæ©ç ã€‚
- `nn.F.scaled_dot_product_attention` + å®Œå…¨å…·ä½“åŒ–çš„attn_maskã€‚è¿™å°†dispatchåˆ°ä¸€ä¸ªèåˆå®ç°`EFFICIENT_ATTENTION`ï¼Œå…è®¸ä»»æ„æ©ç ã€‚

```python
@lru_cache
def create_block_mask_cached(score_mod, B, H, M, N, device="cuda"):
    """
    åˆ›å»ºå¹¶ç¼“å­˜å—æ©ç ã€‚
    
    å‚æ•°:
    - score_mod: åˆ†æ•°ä¿®æ”¹å‡½æ•°
    - B: æ‰¹æ¬¡å¤§å°
    - H: å¤´æ•°
    - M: æŸ¥è¯¢åºåˆ—é•¿åº¦
    - N: é”®å€¼åºåˆ—é•¿åº¦
    - device: è®¾å¤‡ç±»å‹
    
    è¿”å›:
    - block_mask: åˆ›å»ºçš„å—æ©ç 
    """
    block_mask = create_block_mask(score_mod, B, H, M, N, device=device)
    return block_mask


def calculate_tflops(flops: float, time_ms: float, multiplier: int) -> float:
    """
    è®¡ç®—TFLOPSã€‚
    
    å‚æ•°:
    - flops: æµ®ç‚¹è¿ç®—æ¬¡æ•°
    - time_ms: æ—¶é—´ï¼ˆæ¯«ç§’ï¼‰
    - multiplier: ä¹˜æ•°
    
    è¿”å›:
    - TFLOPSå€¼
    """
    return multiplier * flops * (1e3 / time_ms) / 1e12


def test_mask(
    score_mod=None,
    mask_mod=None,
    B=16,
    H=16,
    S=8192,
    D=64,
    skip_correctness=False,
    print_mask=True,
):
    """
    æµ‹è¯•æ©ç åŠŸèƒ½ã€‚
    
    å‚æ•°:
    - score_mod: åˆ†æ•°ä¿®æ”¹å‡½æ•°
    - mask_mod: æ©ç ä¿®æ”¹å‡½æ•°
    - B: æ‰¹æ¬¡å¤§å°
    - H: å¤´æ•°
    - S: åºåˆ—é•¿åº¦
    - D: åµŒå…¥ç»´åº¦
    - skip_correctness: æ˜¯å¦è·³è¿‡æ­£ç¡®æ€§æ£€æŸ¥
    - print_mask: æ˜¯å¦æ‰“å°æ©ç 
    """
    assert (
        score_mod is not None or mask_mod is not None
    ), "Must provide a score_mod or mask_mod"
    
    # åˆ›å»ºè¾“å…¥å¼ é‡
    query = torch.randn(
        B, H, S, D, device="cuda", dtype=torch.float16, requires_grad=True
    )
    key = torch.randn(
        B, H, S, D, device="cuda", dtype=torch.float16, requires_grad=True
    )
    value = torch.randn(
        B, H, S, D, device="cuda", dtype=torch.float16, requires_grad=True
    )
    gradOut = torch.randn(B, H, S, D, device="cuda", dtype=torch.float16)

    # åˆ›å»ºå—æ©ç 
    if mask_mod is not None:
        block_mask = create_block_mask_cached(mask_mod, 1, 1, S, S, device=query.device)
    else:
        block_mask = None
    
    # ç¡®å®šæ©ç å‡½æ•°
    sdpa_mask_fn = mask_mod if mask_mod is not None else score_mod
    mask = create_mask(sdpa_mask_fn, 1, 1, S, S, device=query.device)

    # å®šä¹‰ä¸åŒçš„æ³¨æ„åŠ›è®¡ç®—å‡½æ•°
    causal_fa2 = lambda: F.scaled_dot_product_attention(
        query, key, value, is_causal=True
    )
    xformers_mask = lambda: F.scaled_dot_product_attention(
        query, key, value, attn_mask=mask
    )
    flex_attention_call = lambda: flex_attention(
        query, key, value, score_mod=score_mod, block_mask=block_mask
    )

    results = []
    
    # è®¡ç®—å¯†åº¦
    if block_mask is not None:
        density = (100 - block_mask.sparsity()) / 100
    else:
        density = 1.0
    
    # è®¡ç®—æµ®ç‚¹è¿ç®—æ¬¡æ•°
    causal_fav2_flops = 0.5 * B * H * D * S * S
    flops = density * B * H * D * S * S

    # å‰å‘ä¼ æ’­æ—¶é—´
    causal_fa2_time = do_bench(causal_fa2)
    xformers_mask_time = do_bench(xformers_mask)
    flex_ms = do_bench(flex_attention_call)

    # åå‘ä¼ æ’­æ—¶é—´
    causal_fa2_out = causal_fa2()
    xformers_out = xformers_mask()
    flex_out = flex_attention_call()

    causal_fa2_bw_time = do_bench(
        lambda: causal_fa2_out.backward(gradOut, retain_graph=True)
    )
    xformers_mask_bw_time = do_bench(
        lambda: xformers_out.backward(gradOut, retain_graph=True)
    )
    flex_bw_ms = do_bench(lambda: flex_out.backward(gradOut, retain_graph=True))

    # æ­£ç¡®æ€§æ£€æŸ¥
    if not skip_correctness:
        xformers_outs = []
        flex_outs = []

        query.grad = None
        key.grad = None
        value.grad = None

        out1 = xformers_mask()
        xformers_outs.append(out1)
        out1.backward(gradOut)
        xformers_outs += [query.grad, key.grad, value.grad]

        query.grad = None
        key.grad = None
        value.grad = None

        out2 = flex_attention_call()
        flex_outs.append(out2)
        out2.backward(gradOut)
        flex_outs += [query.grad, key.grad, value.grad]
        for flex, xformer in zip(flex_outs, xformers_outs):
            torch.testing.assert_close(flex, xformer, atol=1e-1, rtol=1e-2)

        print("Correctness check passed âœ…")
    
    # ç»“æœæ ¼å¼åŒ–
    results = [
        [
            "causal FA2",
            f"{causal_fa2_time:.4f}",
            f"{calculate_tflops(causal_fav2_flops, causal_fa2_time, 4):.2f}",
            f"{causal_fa2_bw_time:.4f}",
            f"{calculate_tflops(causal_fav2_flops, causal_fa2_bw_time, 10):.2f}",
        ],
        [
            "F.sdpa + mask",
            f"{xformers_mask_time:.4f}",
            f"{calculate_tflops(flops, xformers_mask_time, 4):.2f}",
            f"{xformers_mask_bw_time:.4f}",
            f"{calculate_tflops(flops, xformers_mask_bw_time, 10):.2f}",
        ],
        [
            "flexattention",
            f"{flex_ms:.4f}",
            f"{calculate_tflops(flops, flex_ms, 4):.2f}",
            f"{flex_bw_ms:.4f}",
            f"{calculate_tflops(flops, flex_bw_ms, 10):.2f}",
        ],
    ]
    print(
        f"\nResults for {score_mod.__name__ if score_mod is not None else mask_mod.__name__}:"
    )
    print(
        tabulate(
            results,
            headers=[
                "Operation",
                "FW Time (ms)",
                "FW FLOPS (TF/s)",
                "BW Time (ms)",
                "BW FLOPS (TF/s)",
            ],
            tablefmt="grid",
        )
    )
    if print_mask:
        print(f"\nBlock Mask:\n{block_mask}")

    # æ¸…ç†å†…å­˜
    del query, key, value, gradOut, causal_fa2_out, xformers_out, flex_out
    torch.cuda.empty_cache()
```

> è¿™é‡Œçš„multiplierä¸ºä»€ä¹ˆæ˜¯4å’Œ10æ²¡ææ¸…æ¥šã€‚

## åŸºæœ¬ç”¨æ³•

ä»¥ä¸‹æ˜¯å¦‚ä½•ä½¿ç”¨FlexAttention APIçš„åŸºæœ¬ç¤ºä¾‹ï¼š

```python

def checkerboard(score, batch, head, token_q, token_kv):
    score = torch.where(torch.abs(token_kv - token_q) % 1 == 0, score * 0.5, score)
    score = torch.where(torch.abs(token_kv - token_q) % 2 == 0, score * 2.0, score)
    return score


# Create input tensors
query = torch.randn(8, 8, 2048, 64, device="cuda", dtype=torch.float32)
key = torch.randn(8, 8, 2048, 64, device="cuda", dtype=torch.float32)
value = torch.randn(8, 8, 2048, 64, device="cuda", dtype=torch.float32)

# Call flex_attention with the checkerboard score modification
output = flex_attention(query, key, value, score_mod=checkerboard)

# Compile and run
compiled_flex_attention = torch.compile(flex_attention)
out_compiled = compiled_flex_attention(query, key, value, score_mod=checkerboard)

# Check if the results are close
torch.testing.assert_close(output, out_compiled, atol=2e-2, rtol=2e-2)
```

## åˆ†æ•°ä¿®æ”¹vsåˆ†æ•°æ©ç 

æˆ‘ä»¬å°†æš‚æ—¶ç¦»å¼€ä¸»é¢˜ï¼Œæè¿°ä¸¤ä¸ªå…³é”®æ¦‚å¿µï¼Œè¿™äº›æ¦‚å¿µå¯¹äºç†è§£å¦‚ä½•è·å¾—FlexAttentionçš„æœ€å¤§æ€§èƒ½ä¼˜åŠ¿éå¸¸é‡è¦ã€‚flex_attentionçš„å®Œæ•´APIå¦‚ä¸‹ï¼š

```python
flex_attention(
    query: torch.Tensor,
    key: torch.Tensor,
    value: torch.Tensor,
    score_mod: Optional[Callable[[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor], torch.Tensor]] = None,
    block_mask: Optional[torch.nn.attention.flex_attention.BlockMask] = None,
    scale: Optional[float] = None,
)
```

ä½ å¯èƒ½ä¼šå¥½å¥‡ä¸ºä»€ä¹ˆæˆ‘ä»¬éœ€è¦åŒæ—¶ä½¿ç”¨ `score_mod` å’Œ `block_mask`ã€‚

- å½“ä½ æƒ³åœ¨æ³¨æ„åŠ›æƒé‡çŸ©é˜µä¸­ä¿®æ”¹åˆ†æ•°å€¼æ—¶ï¼Œåº”è¯¥ä½¿ç”¨ `score_mod` å‡½æ•°ã€‚
- å½“ä½ æƒ³åœ¨æ³¨æ„åŠ›æƒé‡çŸ©é˜µä¸­æ©ç åˆ†æ•°å€¼æ—¶ï¼Œåº”è¯¥ä½¿ç”¨ `mask_mod` å‡½æ•°ï¼Œè¿™äº›åˆ†æ•°å€¼ç‹¬ç«‹äºåˆ†æ•°å€¼æœ¬èº«ï¼Œä»…ä¾èµ–äºä½ç½®ä¿¡æ¯ã€‚

æ³¨æ„ï¼šä»»ä½• `block_mask` ä¹Ÿå¯ä»¥ç”¨ `score_mod` è¡¨ç¤ºï¼Œä½†kernelçš„æ€§èƒ½å°†ä¸æ˜¯æœ€ä¼˜çš„ã€‚

### è®©æˆ‘ä»¬é€šè¿‡å› æœæ³¨æ„åŠ›æ¥çªå‡ºå·®å¼‚ã€‚

ä½¿ç”¨score_modçš„å®ç°ï¼š

```python
def causal_bias(score, b, h, q_idx, kv_idx):
    return torch.where(q_idx >= kv_idx, score, -float("inf"))
```
æ¯å½“ä½ ç¼–å†™ä¸€ä¸ª `score_mod` å‡½æ•°ï¼Œè¯¥å‡½æ•°å¯¹æŸäº›å…ƒç´ ä¼ é€’åŸå§‹åˆ†æ•°ï¼Œè€Œå¯¹å…¶ä»–å…ƒç´ è®¾ç½®ä¸º -inf æ—¶ï¼Œä½ åº”è¯¥å¯èƒ½ä½¿ç”¨ `mask_mod`ã€‚

ä½¿ç”¨ `mask_mod` çš„å®ç°ï¼š

```python
def casual_mask(b,h,q_idx, kv_idx):
    return q_idx >= kv_idx
```

æ­£å¦‚ä½ æ‰€è§ï¼Œå®ƒä»¬çœ‹èµ·æ¥éå¸¸ç›¸ä¼¼ï¼Œéƒ½è¿”å›æ ‡é‡å¼ é‡ã€‚å…³é”®çš„åŒºåˆ«åœ¨äºï¼š

- `mask_mods` è¿”å›å¸ƒå°”å¼ é‡ï¼Œå…¶ä¸­ `True` è¡¨ç¤ºåº”è¯¥è®¡ç®—è¯¥åˆ†æ•°ï¼Œè€Œ `False` è¡¨ç¤ºæˆ‘ä»¬æƒ³è¦æ©ç è¯¥åˆ†æ•°ã€‚
- `mask_mods` ä¸æ¥å— `score` å‚æ•°ï¼Œå› ä¸ºå®ƒä»¬åœ¨è®¡ç®—è¿‡ç¨‹ä¸­ä¸å…è®¸ä¾èµ–å®é™…å€¼ã€‚

### å½“æˆ‘åŒæ—¶ä½¿ç”¨ score_mod å’Œ mask_mod æ—¶ä¼šå‘ç”Ÿä»€ä¹ˆï¼Ÿ

score_mod å‡½æ•°å°†åº”ç”¨äºæ¯ä¸ªæœªè¢«æ©ç çš„å…ƒç´ ã€‚

### æˆ‘æœ‰ä¸€ä¸ª mask mod å‡½æ•°ï¼Œå¦‚ä½•åˆ›å»ºä¸€ä¸ª BlockMaskï¼Ÿ

é—®å¾—å¥½ï¼Œè¯»è€…ï¼é™¤äº† flex_attentionï¼Œæˆ‘ä»¬è¿˜æä¾›äº†ä¸€ä¸ªä¸»è¦çš„ APIã€‚

```python
create_block_mask(
    mask_mod (Callable): mask_mod function.
    B (int): Batch size.
    H (int): Number of heads.
    Q_LEN (int): Sequence length of query.
    KV_LEN (int): Sequence length of key/value.
    device (str): Device to run the mask creation on.
    KV_BLOCK_SIZE (int): Block size of block mask for each query.
    Q_BLOCK_SIZE (int): Block size of block mask for each key/value.
    _compile (bool): Whether to compile the mask creation.
)
```

å› æ­¤ï¼Œå¯¹äºä¸Šè¿°ç¤ºä¾‹ï¼Œè°ƒç”¨flex_attentionçš„æœ€ä¼˜æ€§èƒ½æ–¹å¼æ˜¯ï¼š

```python
causal_block_mask = create_block_mask(causal_mask, B, H, M, N)
flex_attention(query, key, value, block_mask = causal_block_mask)
```

B,H,Q_LEN,KV_LEN åˆ†åˆ«æ˜¯ batch_sizeã€num_headsã€query_sequence_length å’Œ key_sequence_lengthã€‚

### ä¸ºä»€ä¹ˆä¸¤è€…éƒ½æœ‰ï¼Ÿ

çº¯ç²¹æ˜¯ä¸ºäº†æ€§èƒ½ã€‚å› æœæ©ç å®é™…ä¸Šéå¸¸ç¨€ç–ã€‚åªæœ‰æ³¨æ„åŠ›åˆ†æ•°çš„ä¸‹ä¸‰è§’éƒ¨åˆ†æ˜¯é‡è¦çš„ã€‚å¦‚æœä¸ç”ŸæˆBlockMaskï¼Œæˆ‘ä»¬å°†éœ€è¦åšä¸¤å€çš„å·¥ä½œï¼ä¸‹é¢æˆ‘ä»¬å°†æ¯”è¾ƒè¿™ä¸¤ç§å®ç°çš„æ€§èƒ½å·®å¼‚ã€‚

## åˆ†æ•°ä¿®æ”¹ç¤ºä¾‹
è®©æˆ‘ä»¬æ¢ç´¢å¯ä»¥ä½¿ç”¨FlexAttention APIçš„å„ç§åˆ†æ•°ä¿®æ”¹ç¤ºä¾‹ã€‚

å›¾ä¾‹ï¼šæˆ‘ä»¬å°†æ‰“å°è¿™äº›score_mod + mask_fnsçš„ç¨€ç–æ€§è¡¨ç¤ºã€‚

ä»»ä½•å—çš„ç¼ºå¤±æ„å‘³ç€å®ƒè¢«å®Œå…¨æ©ç ï¼Œå®é™…ä¸Šä¸éœ€è¦è®¡ç®—æœ€ç»ˆçš„æ³¨æ„åŠ›è¾“å‡º
- â–ˆâ–ˆ è¿™ä¸ªå—è®¡ç®—æ‰€æœ‰æŸ¥è¯¢å’Œé”®tokenä¹‹é—´çš„å®Œå…¨æ³¨æ„åŠ›
- â–‘â–‘ è¿™ä¸ªå—éƒ¨åˆ†æ©ç ï¼Œä¸€äº›æŸ¥è¯¢tokenå…³æ³¨ä¸€äº›é”®tokenï¼Œä½†ä¸€äº›è¢«æ©ç ä¸º-inf

### å…¨æ³¨æ„åŠ›

åº”ç”¨ä¸€ä¸ªâ€œæ— æ“ä½œâ€çš„åˆ†æ•°ä¿®æ”¹ã€‚ä¿æŒæ³¨æ„åŠ›åˆ†æ•°ä¸å˜ã€‚

```python
def noop(score, b, h, q_idx, kv_idx):
    return score

test_mask(noop, print_mask=True)
```

æ‰§è¡Œåçš„è¾“å‡ºä¸ºï¼š

```python
Results for noop:
+---------------+----------------+-------------------+----------------+-------------------+
| Operation     |   FW Time (ms) |   FW FLOPS (TF/s) |   BW Time (ms) |   BW FLOPS (TF/s) |
+===============+================+===================+================+===================+
| causal FA2    |        14.6478 |            150.13 |        41.1986 |            133.44 |
+---------------+----------------+-------------------+----------------+-------------------+
| F.sdpa + mask |        58.8032 |             74.79 |       125.07   |             87.91 |
+---------------+----------------+-------------------+----------------+-------------------+
| flexattention |        27.3449 |            160.84 |        94.4015 |            116.47 |
+---------------+----------------+-------------------+----------------+-------------------+

Block Mask:
None
```

### æ ‡å‡†å› æœæ©ç 

æ ‡å‡†å› æœæ©ç æ˜¯è‡ªå›å½’è¯­è¨€æ¨¡å‹ä¸­çš„å…³é”®æŠ€æœ¯ï¼Œç¡®ä¿æ¯ä¸ªtokenåªèƒ½å…³æ³¨åºåˆ—ä¸­è‡ªèº«åŠå…¶ä¹‹å‰çš„tokenã€‚å—ç¨€ç–è¡¨ç¤ºå±•ç¤ºäº†è¿™ç§æ©ç çš„ä¸‹ä¸‰è§’æ€§è´¨ã€‚

æœ‰å…³è¿™äº›å®ç°çš„æ›´å¤šè¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚é˜…ä¸Šé¢çš„ã€Šåˆ†æ•°ä¿®æ”¹vsåˆ†æ•°æ©ç ã€‹

```python
def causal_bias(score, b, h, q_idx, kv_idx):
    return torch.where(q_idx >= kv_idx, score, -float("inf"))

test_mask(score_mod=causal_bias)

def causal_mask(b, h, q_idx, kv_idx):
    return q_idx >= kv_idx

test_mask(mask_mod=causal_mask)
```

![](https://files.mdnice.com/user/59/726eab1c-14c4-42ac-b627-373c2f9e2326.png)

### æ»‘åŠ¨çª—å£æ³¨æ„åŠ›

Mistral è®ºæ–‡ä¸­æœ‰ä¸€ä¸ªéå¸¸å¥½çš„å›¾ç¤ºæè¿°äº†è¿™ç§åç½®ã€‚æœ¬è´¨ä¸Šï¼Œä½ å®šä¹‰ä¸€ä¸ªå›ºå®šå¤§å°çš„â€œæ»‘åŠ¨çª—å£â€ï¼Œåœ¨è‡ªå›å½’è§£ç ä¸­ï¼Œä½ åªå…è®¸ `torch.abs(q_tokens - kv_tokens) < SLIDING_WINDOW` çš„ token ç›¸äº’å…³æ³¨ã€‚é€šå¸¸ï¼Œè¿™ä¹Ÿä¼šä¸å› æœæ³¨æ„åŠ›ç»“åˆä½¿ç”¨ã€‚æˆ‘ä»¬å°†é€šè¿‡ä¸€ä¸ªå¾ˆå¥½çš„æ¨¡å¼æ¥å®ç°è¿™ä¸€ç‚¹ï¼Œå³æ©ç ç»„åˆã€‚é€šå¸¸ï¼Œæ©ç å¯ä»¥æ¦‚å¿µä¸Šåˆ†ä¸ºå‡ ä¸ªéƒ¨åˆ†ï¼Œç„¶åç»„åˆåœ¨ä¸€èµ·ã€‚

æˆ‘ä»¬å°†ç¼–å†™ä¸¤ä¸ªæ©ç å‡½æ•°ï¼Œä¸€ä¸ªç”¨äºæ‰§è¡Œ `å› æœæ©ç `ï¼Œå¦ä¸€ä¸ªç”¨äºæ‰§è¡Œ `çª—å£æ³¨æ„åŠ›`ï¼Œå¹¶å°†å®ƒä»¬ç»„åˆåœ¨ä¸€èµ·ä»¥ç”Ÿæˆæœ€ç»ˆçš„æ©ç å‡½æ•°ã€‚æ­£å¦‚æˆ‘ä»¬ä¹‹å‰æ‰€çŸ¥ï¼Œæ©ç å‡½æ•°è¿”å›å¸ƒå°”å€¼ï¼Œå…¶ä¸­ `True` è¡¨ç¤ºè¯¥å…ƒç´ åº”å‚ä¸æ³¨æ„åŠ›è®¡ç®—ã€‚

```python
SLIDING_WINDOW = 1024


def sliding_window_causal_mask(b, h, q_idx, kv_idx):
    causal_mask = q_idx >= kv_idx
    windowed_mask = (
        q_idx - kv_idx <= SLIDING_WINDOW
    )  # We dont need to check the right side of the sliding window since we are applying the causal mask

    return causal_mask & windowed_mask

test_mask(mask_mod=sliding_window_causal_mask)
```

![](https://files.mdnice.com/user/59/ba95a3d9-7949-4f48-b8bb-8a743173077e.png)

### å‰ç¼€ LMï¼ˆåŒå‘ + å› æœï¼‰

T5 æ¶æ„çš„è®ºæ–‡ï¼ˆhttps://paperswithcode.com/method/t5ï¼‰æè¿°äº†ä¸€ç§æ‰§è¡Œå‰ç¼€æ³¨æ„åŠ›çš„æ³¨æ„åŠ›å˜ä½“ã€‚å…¶ä¸­ï¼Œä¸€å®šæ•°é‡çš„ `å‰ç¼€` tokenå…è®¸å®Œå…¨å‚ä¸ï¼Œç„¶åæ‰€æœ‰åç»­tokenæ‰§è¡Œå› æœæ³¨æ„åŠ›ã€‚æˆ‘ä»¬å†æ¬¡ç»„åˆä¸¤ä¸ªæ©ç å‡½æ•°æ¥å®ç°è¿™ä¸€ç‚¹ï¼Œä¸€ä¸ªç”¨äºå› æœæ©ç ï¼Œå¦ä¸€ä¸ªåŸºäºå‰ç¼€é•¿åº¦ã€‚

```python
PREFIX_LENGTH = 2048

def prefix_lm_causal_mask(b, h, q_idx, kv_idx):
    prefix_mask = kv_idx <= PREFIX_LENGTH
    causal_mask = q_idx >= kv_idx
    return prefix_mask | causal_mask

test_mask(mask_mod=prefix_lm_causal_mask)
```

![](https://files.mdnice.com/user/59/1211c505-855e-4dde-97e1-b80e80b760dc.png)

### æ–‡æ¡£æ©ç 

æƒ³è±¡ä¸€ä¸‹ï¼Œæˆ‘ä»¬æœ‰å¤šä¸ªä¸åŒé•¿åº¦çš„æ–‡æ¡£ã€‚æˆ‘ä»¬å¸Œæœ›æ©ç æ‰æ–‡æ¡£ä¹‹é—´çš„æ³¨æ„åŠ›ï¼Œä½†å…è®¸åŒä¸€æ–‡æ¡£å†…çš„tokenä¹‹é—´çš„æ³¨æ„åŠ›ã€‚æˆ‘ä»¬å¯ä»¥é€šè¿‡ä½¿ç”¨ä¸€ä¸ªdocument_idå¼ é‡æ¥å®ç°è¿™ä¸€ç‚¹ï¼Œè¯¥å¼ é‡ç»™å‡ºäº†æ¯ä¸ªtokenæ‰€å±çš„æ–‡æ¡£ã€‚ç„¶åï¼Œæˆ‘ä»¬å¯ä»¥æ©ç æ‰æ‰€æœ‰document_id[q_idx]ä¸document_id[kv_idx]ä¸åŒçš„æ³¨æ„åŠ›åˆ†æ•°ã€‚

æ³¨æ„ï¼šåªæœ‰å½“`score_mod`æ”¹å˜æ—¶ï¼Œæˆ‘ä»¬æ‰éœ€è¦ç¼–è¯‘ä¸€ä¸ªæ–°çš„kernelï¼ˆå®ƒä¼šä½¿ç”¨torch.compileåŸºç¡€è®¾æ–½è‡ªåŠ¨æ£€æµ‹åˆ°è¿™ä¸€ç‚¹ï¼‰ã€‚è¿™ä¸ªç¤ºä¾‹ä»£ç æ˜¯é€šè¿‡ç¼“å­˜BlockMaskå®ç°çš„ï¼Œä½†ä¸€èˆ¬æ¥è¯´ï¼Œæ”¹å˜BlockMaskä¸éœ€è¦é‡æ–°ç¼–è¯‘ã€‚ä¹Ÿå°±æ˜¯è¯´ï¼Œå¯¹äºæ–‡æ¡£æ©ç ï¼Œæˆ‘ä»¬åªéœ€è¦åœ¨æ–‡æ¡£é•¿åº¦æ”¹å˜æ—¶è®¡ç®—ä¸€ä¸ªæ–°çš„BlockMaskï¼Œè€Œä¸æ˜¯ä¸€ä¸ªæ–°çš„kernelã€‚

```python
document_id = torch.zeros(32768, dtype=torch.int, device="cuda")
document_id[:4096] = 0
document_id[4096:8192] = 1
for i in range(8192, 32768, 8192):
    document_id[i : i + 8192] = i // 8192 + 1

def document_causal_mask(b, h, q_idx, kv_idx):
    causal_mask = q_idx >= kv_idx
    document_mask = document_id[q_idx] == document_id[kv_idx]
    return causal_mask & document_mask

test_mask(mask_mod=document_causal_mask, S=32768)
```

æˆ‘åœ¨4090ä¸Šè·‘ä¼šoomï¼Œè¿™é‡ŒæŠŠé•¿åº¦æ”¹å°ä¸€ç‚¹ï¼š

```python
document_id = torch.zeros(8192, dtype=torch.int, device="cuda")
document_id[:4096] = 0
document_id[4096:8192] = 1
# for i in range(8192, 32768, 8192):
#     document_id[i : i + 8192] = i // 8192 + 1

def document_causal_mask(b, h, q_idx, kv_idx):
    causal_mask = q_idx >= kv_idx
    document_mask = document_id[q_idx] == document_id[kv_idx]
    return causal_mask & document_mask

test_mask(mask_mod=document_causal_mask, S=8192)
```

![](https://files.mdnice.com/user/59/702dc016-23ab-45b1-b587-04f120d8a6a6.png)

### ç‹¬ç«‹è‡ªæ³¨æ„åŠ›æ©ç 

åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œæƒ³è±¡æˆ‘ä»¬æœ‰ä¸€ä¸ªå¤§å°ä¸º (H x W) çš„äºŒç»´å›¾åƒï¼Œè¢«å±•å¹³æˆä¸€ä¸ªtokenåºåˆ—ã€‚æˆ‘ä»¬åªæƒ³å…³æ³¨8ä¸ª`åƒç´ `å†…çš„tokenï¼Œä½†ä»äºŒç»´è§’åº¦æ¥çœ‹ã€‚

æˆ‘ä»¬å¯ä»¥é€šè¿‡é¦–å…ˆå°†ä¸€ç»´ä½ç½®è½¬æ¢ä¸ºäºŒç»´åæ ‡æ¥å®ç°è¿™ä¸ªmask_modã€‚ç„¶åï¼Œæˆ‘ä»¬å¯ä»¥ç®€å•åœ°æ£€æŸ¥ä¸¤ä¸ªåæ ‡çš„è·ç¦»æ˜¯å¦åœ¨çª—å£å†…ã€‚

æ›´å¤šç»†èŠ‚è¯·æŸ¥çœ‹è®ºæ–‡ï¼ŒStand-Alone Self-Attention in Vision Models(https://arxiv.org/abs/1906.05909)

```python
H = 128
W = 128
WINDOW = 8

def get_x_y(idx):
    return idx // W, idx % W

def sasa_mask(b, h, q_idx, kv_idx):
    q_x, q_y = get_x_y(q_idx)
    kv_x, kv_y = get_x_y(kv_idx)
    horizontal_mask = (q_x - kv_x).abs() <= WINDOW
    vertical_mask = (q_y - kv_y).abs() <= WINDOW
    return horizontal_mask & vertical_mask

test_mask(mask_mod=sasa_mask)
```

![](https://files.mdnice.com/user/59/44b5dec7-a666-42df-926e-0ec9e7f219ef.png)


### NATTEN æ©ç 

è€ƒè™‘ä¸€ä¸ªå¤§å°ä¸º (H x W) çš„äºŒç»´å›¾åƒï¼Œè¢«å±•å¹³æˆä¸€ä¸ªtokenåºåˆ—ã€‚æŸ¥è¯¢å…³æ³¨é”®åœ¨ä¸€ä¸ªå›ºå®škernelåŒºåŸŸ (K_H x K_W) å†…ï¼Œå°½å¯èƒ½ä»¥æŸ¥è¯¢ä¸ºä¸­å¿ƒï¼ŒåŒæ—¶ä¿æŒåœ¨ç”»å¸ƒå†…å¹¶å§‹ç»ˆåŒ…æ‹¬æŸ¥è¯¢ã€‚

è¿™ä¸SASAç±»ä¼¼ï¼Œä½†æœ‰é¢å¤–çš„å¤„ç†æ¥ä¿æŒkernelåœ¨ç”»å¸ƒå†…ï¼Œç¡®ä¿æ‰€æœ‰æŸ¥è¯¢å…³æ³¨å›ºå®šæ•°é‡çš„é”®ã€‚é”®å°†å…¶ä½ç½®ä¸kernelä¸­å¿ƒè¿›è¡Œæ¯”è¾ƒï¼Œè€Œä¸æ˜¯æŸ¥è¯¢ã€‚kernelä¸­å¿ƒè¯•å›¾è·ŸéšæŸ¥è¯¢ä½ç½®ï¼Œä½†è¢«é™åˆ¶åœ¨ç”»å¸ƒè¾¹ç¼˜ä¿æŒå›ºå®šè·ç¦»ï¼ˆå…¶åŠé•¿åº¦ï¼‰ã€‚

æ›´å¤šä¿¡æ¯è¯·å‚è§NATTENä»“åº“(https://github.com/SHI-Labs/NATTEN)ã€‚
> æ³¨æ„ï¼šæ›´å®Œæ•´çš„NATTENå®ç°å°†åŒ…æ‹¬å¯¹kernelè†¨èƒ€çš„æ”¯æŒã€‚NATTENæœªèåˆçš„kernelè¿˜å…·æœ‰è¯¸å¦‚èƒ½å¤Ÿäº¤å‰å…³æ³¨å¯„å­˜å™¨tokenç­‰åŠŸèƒ½ã€‚è¿™ç§èƒ½åŠ›å¯ä»¥åœ¨Flex Attentionä¸­è¡¨è¾¾ï¼Œä½†è¿™é‡Œæ²¡æœ‰å°è¯•ã€‚

```python
H = 128
W = 128
K_H = 7
K_W = 7

def get_x_y(idx):
    return idx // W, idx % W

def natten_mask(
    b,
    h,
    q_idx,
    kv_idx,
):
    q_x, q_y = get_x_y(q_idx)
    kv_x, kv_y = get_x_y(kv_idx)
    # kernel nominally attempts to center itself on the query, but kernel center
    # is clamped to a fixed distance (kernel half-length) from the canvas edge
    kernel_x = q_x.clamp(K_W // 2, (W - 1) - K_W // 2)
    kernel_y = q_y.clamp(K_H // 2, (H - 1) - K_H // 2)
    hori_mask = (kernel_x - kv_x).abs() <= K_W // 2
    vert_mask = (kernel_y - kv_y).abs() <= K_H // 2
    return hori_mask & vert_mask

test_mask(mask_mod=natten_mask)
```

![](https://files.mdnice.com/user/59/37c61482-1108-472e-8dc4-01b6d32d3886.png)

### Alibi åç½®

Alibi æ³¨æ„åŠ›åç½®åœ¨ Train Short, Test Long: Attention with Linear Biases Enables Input Length Extrapolation(https://arxiv.org/abs/2108.12409) ä¸­å˜å¾—æµè¡Œï¼Œå¹¶å£°ç§°åœ¨æ¨ç†æ—¶å…·æœ‰é•¿åº¦å¤–æ¨çš„æœ‰ç›Šç‰¹æ€§ã€‚"ALiBi ä¸ä¼šå°†ä½ç½®åµŒå…¥æ·»åŠ åˆ°è¯åµŒå…¥ä¸­ï¼›ç›¸åï¼Œå®ƒé€šè¿‡ä¸å®ƒä»¬è·ç¦»æˆæ¯”ä¾‹çš„æƒ©ç½šæ¥åç½®æŸ¥è¯¢-é”®æ³¨æ„åŠ›åˆ†æ•°ã€‚"

æˆ‘ä»¬å°†ä»¥ä¸¤ç§æ–¹å¼å®ç°è¿™ä¸€ç‚¹ï¼Œä»¥çªå‡ºä¸€ä¸ªæ–°çš„åŠŸèƒ½ï¼Œå³åœ¨åˆ†æ•°ä¿®æ”¹å‡½æ•°ä¸­åˆ©ç”¨å…¶ä»–å¼ é‡çš„èƒ½åŠ›ã€‚å°½ç®¡å‡½æ•°ç­¾åä¸æ¥å—å…¶ä»–å¼ é‡ï¼Œä½†ç”¨æˆ·å¯ä»¥é€šè¿‡ `closure` æ¥å®ç°è¿™ä¸€ç‚¹ã€‚åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬åˆ©ç”¨äº†æˆ‘ä»¬éå¸¸ç†Ÿæ‚‰çš„å› æœæ©ç å‡½æ•°ä»¥åŠå„ä¸ªå¤´çš„åç½®ã€‚

```python
# Alibi Bias
def generate_alibi_bias():
    alibi_bias = []
    for h in range(H):
        alibi_bias.append(-((h + 1) * 8.0 / H))
    alibi_bias = torch.tensor(alibi_bias, device="cuda")
    alibi_bias = torch.exp2(alibi_bias)
    return alibi_bias


alibi_bias = generate_alibi_bias()


# In this case we are going to use a mask_mod and a score_mod
def causal_mask(b, h, q_idx, kv_idx):
    return q_idx >= kv_idx


def alibi_and_causal_closure(score, b, h, q_idx, kv_idx):
    bias = alibi_bias[h] * (q_idx - kv_idx)
    return score + bias


def alibi_and_causal_functional(score, b, h, q_idx, kv_idx):
    scale = torch.exp2(-((h + 1) * 8.0 / H))
    bias = (q_idx - kv_idx) * scale
    return score + bias


# Correctness check here is simple and only works with mask_fns and not actual score_mods

test_mask(
    alibi_and_causal_closure,
    mask_mod=causal_mask,
    skip_correctness=True,
    print_mask=False,
)
test_mask(
    alibi_and_causal_functional,
    mask_mod=causal_mask,
    skip_correctness=True,
    print_mask=False,
)
```

> è¿™é‡Œçš„Hæ²¡æœ‰å®šä¹‰ï¼Œæˆ‘ä»¬å†™ä¸€ä¸ªH=64æ¥çœ‹ä¸‹ç»“æœã€‚å¦å¤–éœ€è¦æŠŠprint_maskæ”¹æˆTrueæ‰èƒ½çœ‹åˆ°maské•¿ä»€ä¹ˆæ ·ã€‚

![](https://files.mdnice.com/user/59/e8f5f35d-fcff-4ddb-a4ad-2958d0aa3046.png)

![](https://files.mdnice.com/user/59/fc2cb332-abab-4370-a824-9a7ba1c85a96.png)



### Tanh è½¯ä¸Šé™
æˆ‘ä»¬ä¹Ÿå¯ä»¥ä½¿ç”¨è¿™ä¸ªAPIå®ç°tanhè½¯ä¸Šé™ã€‚é€šè¿‡tanhè¿›è¡Œlogitè½¯ä¸Šé™åœ¨Gemma 2ä¸­å˜å¾—æµè¡Œã€‚

åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œæœ‰ä¸€äº›ç»†å¾®å·®åˆ«ã€‚ç‰¹åˆ«æ˜¯ï¼ŒPyTorchï¼ˆå’ŒCUDA/Tritonï¼‰ä¸­çš„æ ‡å‡†`tanh`æ“ä½œç¬¦ä¼šé™ä½åˆ°ä¸€ä¸ªæ•°å€¼ä¸Šå‡†ç¡®ä½†ï¼ˆç›¸å¯¹ï¼‰è¾ƒæ…¢çš„SASSå®ç°ã€‚å‚è§https://godbolt.org/z/W8afevWv1äº†è§£SASSçš„æ ·å­ã€‚

å› æ­¤ï¼Œåœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œæˆ‘ä»¬å¸Œæœ›å°†`tanh`é™ä½åˆ°è¿‘ä¼¼tanhå®ç°ã€‚æˆ‘ä»¬å¯ä»¥é€šè¿‡åœ¨PyTorchä¸­æ³¨å†Œä¸€ä¸ªè‡ªå®šä¹‰æ“ä½œç¬¦ï¼Œç„¶åè¿›è¡ŒInductoré™ä½æ¥å®ç°è¿™ä¸€ç‚¹ã€‚

```python
def causal_mask(b, h, q_idx, kv_idx):
    return q_idx >= kv_idx

# Tanh Soft-Capping
@torch.library.custom_op("approx::tanh", mutates_args=())
def tanh_approx(inp: torch.Tensor) -> torch.Tensor:
    return torch.tanh(inp)


@tanh_approx.register_fake
def _(inp: torch.Tensor) -> torch.Tensor:
    return torch.tanh(inp)


from torch._inductor.lowering import make_pointwise, register_lowering

# Some internal torch.compile details
from torch._inductor.virtualized import ops

def tanh_approx_lowering(inp):
    fn = partial(ops.inline_asm_elementwise, asm="tanh.approx.f32 0,1;")
    return make_pointwise(fn)(inp)

register_lowering(torch.ops.approx.tanh)(tanh_approx_lowering)

class TanhApprox(torch.autograd.Function):
    @staticmethod
    def forward(x):
        return torch.ops.approx.tanh(x)

    @staticmethod
    def setup_context(ctx, inputs, output):
        (x,) = inputs
        result = output
        ctx.save_for_backward(result)

    @staticmethod
    def backward(ctx, grad_output):
        (result,) = ctx.saved_tensors
        return grad_output * (1 - result * result)

tanh_approx = TanhApprox.apply

def tanh_soft_cap(score, b, h, q_idx, kv_idx):
    score = score / 2
    score = tanh_approx(score)
    return score * 2

# The baseline (xformers) does not have a way to generate tanh-softcapping so we skip correctness checks
test_mask(tanh_soft_cap, mask_mod=causal_mask, skip_correctness=True)
```

> ä»£ç é‡Œé¢çš„asmä»£ç æœ‰é”™è¯¯ï¼Œè¿™ä¸ªä¾‹å­æ— æ³•è¿è¡Œã€‚æŠ¥é”™ä¿¡æ¯å¦‚ä¸‹ï¼š

```shell
ptxas /tmp/tmpmehxr5i1.ptx, line 3972; error   : Arguments mismatch for instruction 'tanh'
ptxas /tmp/tmpmehxr5i1.ptx, line 3977; error   : Arguments mismatch for instruction 'tanh'
ptxas /tmp/tmpmehxr5i1.ptx, line 3982; error   : Arguments mismatch for instruction 'tanh'
ptxas /tmp/tmpmehxr5i1.ptx, line 3987; error   : Arguments mismatch for instruction 'tanh'
ptxas /tmp/tmpmehxr5i1.ptx, line 3992; error   : Arguments mismatch for instruction 'tanh'
ptxas /tmp/tmpmehxr5i1.ptx, line 3997; error   : Arguments mismatch for instruction 'tanh'
ptxas /tmp/tmpmehxr5i1.ptx, line 4002; error   : Arguments mismatch for instruction 'tanh'
ptxas /tmp/tmpmehxr5i1.ptx, line 4007; error   : Arguments mismatch for instruction 'tanh'
ptxas /tmp/tmpmehxr5i1.ptx, line 4012; error   : Arguments mismatch for instruction 'tanh'
ptxas fatal   : Ptx assembly aborted due to errors

```

### åµŒå¥—ä¸è§„åˆ™å¼ é‡

åµŒå¥—å¼ é‡æ˜¯ä¸€ç§å¼ é‡å­ç±»ï¼Œç”¨äºé«˜æ•ˆåœ°è¡¨ç¤ºå’Œè®¡ç®—ä¸è§„åˆ™æ•°æ®ã€‚å¯ä»¥ä½¿ç”¨FlexAttentionå¤„ç†è¿™ç§æ•°æ®ï¼Œä»¥é«˜æ•ˆåœ°å¯¹ä¸åŒé•¿åº¦çš„åºåˆ—æ‰¹æ¬¡æ‰§è¡Œå› æœæ³¨æ„åŠ›ã€‚

åœ¨åº•å±‚ï¼ŒNJT(åµŒå¥—ä¸è§„åˆ™å¼ é‡)å°†å…¶ä¸è§„åˆ™æ•°æ®å­˜å‚¨ä¸ºè¿ç»­æ•°æ® `[[sequence_0], [sequence_1], ..., [Sequence_B]], sum(*),..`

```python
# è®¾ç½®éšæœºç§å­ä»¥ç¡®ä¿ç»“æœå¯é‡å¤
random.seed(0)
torch.manual_seed(0)

# å®šä¹‰æ‰¹æ¬¡å¤§å°ã€å¤´æ•°å’Œç»´åº¦
batch_size = 16
n_heads = 16
D = 64

# å‡†å¤‡QKVå€¼ï¼Œä½¿å…¶å¯ä»¥è®¡ç®—æ¢¯åº¦
def prepare_qkv_values(tensor):
    return tensor._values.detach().requires_grad_()

# æ„å»ºåºåˆ—ç´¢å¼•è¡¨
def build_seq_idx(tensor: torch.Tensor):
    offsets = tensor.offsets()
    total_length = tensor.offsets()[-1].item()
    # åˆ›å»ºä»0åˆ°total_lengthçš„èŒƒå›´å¼ é‡
    range_tensor = torch.arange(total_length, device="cuda", dtype=torch.int32)

    # ä½¿ç”¨searchsortedæŸ¥æ‰¾æ¯ä¸ªä½ç½®çš„ç´¢å¼•
    seq_idx = torch.searchsorted(offsets, range_tensor, right=True) - 1

    return seq_idx

# åˆ›å»ºNJTåŒ…è£…å™¨ï¼Œå°†å¯†é›†æ©ç å‡½æ•°è½¬æ¢ä¸ºNJTæ©ç å‡½æ•°
def create_njt_wrapper(orig_mask_mod, offsets, seq_idx):
    """é€šç”¨åŒ…è£…å™¨ï¼Œå°†å¯†é›†æ©ç å‡½æ•°è½¬æ¢ä¸ºNJTæ©ç å‡½æ•°"""

    def njt_score_mod(b, h, q_idx, kv_idx):
        q_nested = q_idx - offsets[seq_idx[q_idx]]
        kv_nested = kv_idx - offsets[seq_idx[kv_idx]]
        is_same_sequence = seq_idx[q_idx] == seq_idx[kv_idx]
        return orig_mask_mod(b, h, q_nested, kv_nested) & is_same_sequence

    return njt_score_mod

# å¯†é›†å¾—åˆ†æ©ç å‡½æ•°
def causal_mask(b, h, q_idx, kv_idx):
    return q_idx >= kv_idx
    # return torch.where(q_idx >= kv_idx, score, -float("inf"))

# å½“å‰é™åˆ¶ï¼šæ€»åºåˆ—é•¿åº¦å¿…é¡»èƒ½è¢«128æ•´é™¤
sentence_lengths = [random.randint(1, 1024) for _ in range(batch_size - 1)]
total = sum(sentence_lengths)
sentence_lengths.append(128 - total % 128)
total = sum(sentence_lengths)

# åˆ›å»ºä¸è§„åˆ™å¼ é‡
ragged_tensors = [torch.randn(l, n_heads, D, device="cuda") for l in sentence_lengths]
query = torch.nested.nested_tensor(
    ragged_tensors, layout=torch.jagged, requires_grad=True
)
key = torch.nested.nested_tensor(
    ragged_tensors, layout=torch.jagged, requires_grad=True
)
value = torch.nested.nested_tensor(
    ragged_tensors, layout=torch.jagged, requires_grad=True
)

# æ„å»ºseq_idxæŸ¥æ‰¾è¡¨
offsets = query.offsets()
seq_idx = build_seq_idx(query)

# åˆ›å»ºNJTå› æœå¾—åˆ†æ©ç å‡½æ•°
causal_score_mod_njt = create_njt_wrapper(causal_mask, offsets, seq_idx)

# å‡†å¤‡QKVå€¼
query_values = prepare_qkv_values(query)
key_values = prepare_qkv_values(key)
value_values = prepare_qkv_values(value)

# åˆ›å»ºå—æ©ç 
block_mask = create_block_mask_cached(
    causal_score_mod_njt, 1, 1, total, total, device=query_values.device
)
# ä½¿ç”¨FlexAttentionè®¡ç®—è¾“å‡º
out_flex = flex_attention(
    query_values.view(1, -1, n_heads, D).transpose(1, 2),
    key_values.view(1, -1, n_heads, D).transpose(1, 2),
    value_values.view(1, -1, n_heads, D).transpose(1, 2),
    block_mask=block_mask,
)
# ä½¿ç”¨Scaled Dot-Product Attentionè®¡ç®—è¾“å‡º
out_sdpa = F.scaled_dot_product_attention(
    query.transpose(1, 2),
    key.transpose(1, 2),
    value.transpose(1, 2),
    is_causal=True,
)

# å­˜å‚¨è¾“å‡ºç»“æœ
sdpa_outs = []
flex_outs = []

# åˆ›å»ºæ¢¯åº¦è¾“å‡º
gradOut = torch.randn_like(out_sdpa)

# è®¡ç®—å¹¶å­˜å‚¨SDPAçš„è¾“å‡ºå’Œæ¢¯åº¦
sdpa_outs.append(out_sdpa)
out_sdpa.backward(gradOut)
sdpa_outs += [query.grad, key.grad, value.grad]

# è®¡ç®—å¹¶å­˜å‚¨FlexAttentionçš„è¾“å‡ºå’Œæ¢¯åº¦
flex_outs.append(out_flex)
out_flex.backward(gradOut._values.unsqueeze(0))
flex_outs += [query_values.grad, key_values.grad, value_values.grad]

# æ¯”è¾ƒä¸¤ç§æ–¹æ³•çš„è¾“å‡ºå’Œæ¢¯åº¦
for flex, sdpa in zip(flex_outs, sdpa_outs):
    flex = flex.squeeze(0)
    torch.testing.assert_close(flex, sdpa._values, atol=1e-2, rtol=1e-2)

# æ‰“å°æ­£ç¡®æ€§æ£€æŸ¥ç»“æœ
print("Correctness check passed âœ…")
print(block_mask)
```

![](https://files.mdnice.com/user/59/8ca85ee2-b2fc-4ce4-80b3-b7f84f5b1917.png)

### Flamingo Cross Attention

ğŸ¦© Flamingo è®ºæ–‡ï¼ˆhttps://arxiv.org/pdf/2204.14198ï¼‰ä»‹ç»äº†ä¸€ç§â€œè§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰å®¶æ—ï¼Œå®ƒä»¬ä»¥äº¤é”™çš„è§†è§‰æ•°æ®å’Œæ–‡æœ¬ä½œä¸ºè¾“å…¥ï¼Œå¹¶ç”Ÿæˆè‡ªç”±å½¢å¼çš„æ–‡æœ¬ä½œä¸ºè¾“å‡ºã€‚â€

å®ƒåˆ©ç”¨ `VisionCrossAttentionMask` æ¥ç¡®ä¿æ–‡æœ¬åªå…³æ³¨ç›¸å…³çš„å›¾åƒã€‚TorchTune å¯¹è¿™ç§æ©ç ç±»å‹æœ‰å¾ˆå¥½çš„æè¿°ï¼šVisionCrossAttentionMaskï¼ˆhttps://github.com/pytorch/torchtune/blob/bbc48e089b072c7cbaea175bc70501b2193ba482/torchtune/modules/transforms/_transforms.py#L22-L43ï¼‰

è¿™ç§æ³¨æ„åŠ›æœºåˆ¶ç¡®ä¿æ–‡æœ¬åºåˆ—å®Œå…¨å…³æ³¨å‰é¢çš„å›¾åƒï¼Œè€Œä¸å…³æ³¨å…¶ä»–æœªæ¥çš„æˆ–ä¸ç›¸å…³çš„å›¾åƒã€‚

```python
Example:
    >>> text = "<img1><img2>These are two dogs. <img3>This is a cat."
    >>> image_token_id = 1
    >>> tokens = [1, 1, 9673, 527, 1403, 12875, 13, 1, 1115, 374, 264, 8415]
    >>> transform = VisionCrossAttentionMask(tile_size=400, patch_size=40, image_token_id=1)
    >>> intervals = transform._get_image_attention_intervals(tokens)
    >>> print(intervals)
    [[0, 7], [1, 7], [7, 12]]
```

åœ¨ä¸Šé¢çš„ä¾‹å­ä¸­ï¼Œæˆ‘ä»¬å°†ç”Ÿæˆä¸€ä¸ª12 x sum(image_tokens_1 + image_tokens_2 + image_tokens_3)çš„æ©ç 

å‡è®¾image_tokensçš„å¤§å°ä¸º3

![](https://files.mdnice.com/user/59/b82c1996-a282-42a2-9b3e-11d90969caf1.png)

```python
# Given information
num_tokens = 12
num_images = 3
image_token_length = 3
num_image_tokens = num_images * image_token_length
intervals = [[0, 7], [1, 7], [7, 12]]
# This is only needed if your images have different number of tokens per image
# If they are all the same number of tokens you can use image_idx = kv_idx // image_token_length
image_boundaries = [image_token_length * i for i in range(num_images)]
image_boundaries = (
    [0] * image_token_length + [1] * image_token_length + [2] * image_token_length
)

image_boundaries = torch.tensor(image_boundaries, dtype=torch.long, device="cuda")
intervals = torch.tensor(intervals, dtype=torch.long, device="cuda")


def vision_x_attention_mask(b, h, q_idx, kv_idx):
    image_idx = image_boundaries[kv_idx]
    interval = intervals[image_idx]
    return (q_idx >= interval[0]) & (q_idx < interval[1])


mask = create_mask(vision_x_attention_mask, 1, 1, num_tokens, num_image_tokens, "cuda")

print(mask)
```


# FlexAttentionæ˜¯å¦‚ä½•å®ç°çš„

FlexAttentionæ˜¯é€šè¿‡PyTorchç¼–è¯‘å™¨æ¥å®ç°çš„ï¼Œé€šè¿‡inductoråç«¯ç”ŸæˆFlexAttentionçš„å„ç§å˜ä½“å¯¹åº”çš„Tritonä»£ç ã€‚å…·ä½“å®ç°è§ï¼šhttps://github.com/pytorch/pytorch/blob/ee09d066d35d7e17cf7e9479c0b8bfc70cffc264/torch/_inductor/kernel/flex_attention.py#L317 ï¼Œä¸‹é¢å¯¹ flex_attention çš„æ ¸å¿ƒå…¥å£ç®€å•æµè§ˆä¸€ä¸‹ï¼š

```python
# TODO: We probably also need a layout constraint?
@register_lowering(torch.ops.higher_order.flex_attention, type_promotion_kind=None)
def flex_attention(
    query,
    key,
    value,
    subgraph,
    block_mask,
    scale,
    score_mod_other_buffers,
    mask_mod_other_buffers,
):
    # è¿™è¡Œä»£ç å®é™…ä¸Šå°±è·å–äº†æˆ‘ä»¬åœ¨APIåº”ç”¨ä¸­å®šä¹‰çš„score_modå’Œmask_modä¹‹åçœŸæ­£è¦è®¡ç®—çš„Q,K,V
    (
        kv_num_blocks,
        kv_indices,
        full_kv_num_blocks,
        full_kv_indices,
        q_num_blocks,
        q_indices,
        full_q_num_blocks,
        full_q_indices,
        SPARSE_KV_BLOCK_SIZE,
        SPARSE_Q_BLOCK_SIZE,
        mask_graph,
    ) = block_mask
    // åˆ›å»ºå ä½ç¬¦è¾“å…¥åˆ—è¡¨ï¼ŒåŒ…å«scoreã€bã€hã€mã€näº”ä¸ªå ä½ç¬¦ï¼Œç±»å‹åˆ†åˆ«ä¸ºqueryçš„ç±»å‹å’Œint32
    placeholder_inps = [
        create_placeholder(name, dtype, query.get_device())
        for name, dtype in [
            ("score", query.get_dtype()),
            ("b", torch.int32),
            ("h", torch.int32),
            ("m", torch.int32),
            ("n", torch.int32),
        ]
    ]
    // æ„å»ºå­å›¾ç¼“å†²åŒºï¼ŒåŒ…å«å ä½ç¬¦è¾“å…¥å’Œå…¶ä»–åˆ†æ•°ä¿®æ”¹ç¼“å†²åŒº
    subgraph_buffer = build_subgraph_buffer(
        placeholder_inps + list(score_mod_other_buffers), subgraph
    )
    // åˆ›å»ºæ©ç å›¾çš„å ä½ç¬¦è¾“å…¥åˆ—è¡¨ï¼ŒåŒ…å«bã€hã€mã€nå››ä¸ªå ä½ç¬¦ï¼Œç±»å‹å‡ä¸ºint32
    mask_graph_placeholder_inps = [
        create_placeholder(name, dtype, query.get_device())
        for name, dtype in [
            ("b", torch.int32),
            ("h", torch.int32),
            ("m", torch.int32),
            ("n", torch.int32),
        ]
    ]
    // æ„å»ºæ©ç å›¾ç¼“å†²åŒºï¼ŒåŒ…å«æ©ç å›¾çš„å ä½ç¬¦è¾“å…¥å’Œå…¶ä»–æ©ç ä¿®æ”¹ç¼“å†²åŒº
    mask_graph_buffer = build_subgraph_buffer(
        mask_graph_placeholder_inps + list(mask_mod_other_buffers), mask_graph
    )
    // å¦‚æœä½¿ç”¨Flexè§£ç ï¼Œåˆ™è¿”å›åˆ›å»ºçš„Flexè§£ç å†…æ ¸
    if _use_flex_decoding(query):
        return create_flex_decoding_kernel(
            query,
            key,
            value,
            block_mask,
            scale,
            subgraph_buffer,
            mask_graph_buffer,
            score_mod_other_buffers,
            mask_mod_other_buffers,
        )
    // å¯¹æ‰€æœ‰ç¼“å†²åŒºè¿›è¡Œrealizeæ“ä½œï¼Œç¡®ä¿å®ƒä»¬è¢«å®ä¾‹åŒ–
    for buf in [
        query,
        key,
        value,
        kv_num_blocks,
        kv_indices,
        q_num_blocks,
        q_indices,
        full_kv_num_blocks,
        full_kv_indices,
        full_q_num_blocks,
        full_q_indices,
    ]:
        if buf is not None:
            buf.realize()

    // åˆ›å»ºå¸ƒå±€å¯¹è±¡ï¼ŒåŒ…å«è®¾å¤‡ã€æ•°æ®ç±»å‹ã€å¤§å°å’Œæ­¥å¹…ä¿¡æ¯
    layout = FixedLayout(
        query.get_device(),
        query.get_dtype(),
        query.get_size(),
        query.get_stride(),
    )
    // è®¡ç®—logsumexpçš„å½¢çŠ¶ï¼Œå³queryçš„å½¢çŠ¶å»æ‰æœ€åä¸€ä¸ªç»´åº¦
    logsumexp_shape = query.get_size()[:-1]  # [B, H, M]
    // åˆ›å»ºlogsumexpå¼ é‡ï¼Œç±»å‹ä¸ºfloat32ï¼Œè®¾å¤‡ä¸queryç›¸åŒ
    logsumexp = empty_strided(
        logsumexp_shape,
        None,
        dtype=torch.float32,  # The logsumexp is always stored in fp32 regardless of the input dtype
        device=query.get_device(),
    )
    // åˆ¤æ–­æ˜¯å¦å­˜åœ¨å®Œæ•´å—ï¼Œå¦‚æœfull_kv_num_blocksä¸ºNoneï¼Œåˆ™ä¸å­˜åœ¨å®Œæ•´å—
    has_full_blocks = full_kv_num_blocks is not None
    if full_kv_num_blocks is None:
        full_kv_num_blocks, full_kv_indices = (
            empty(0, device=query.get_device()) for _ in range(2)
        )
    // åˆå§‹åŒ–é€‰æ‹©åˆ—è¡¨å’Œé…ç½®åˆ—è¡¨
    choices: List[Any] = []
    configs: List[Tuple[int, int, int, int]] = []
    // æ·»åŠ é»˜è®¤é…ç½®
    configs.append(_get_default_config_fwd(query))
    // å¦‚æœå¯ç”¨äº†æœ€å¤§è‡ªåŠ¨è°ƒä¼˜ï¼Œåˆ™æ·»åŠ å…¶ä»–é…ç½®
    if config.max_autotune:
        configs += [
            (128, 64, 4, 3),
            (128, 128, 4, 3),
            (128, 128, 8, 2),
            (64, 128, 4, 3),
            (64, 64, 4, 3),
        ]

    // éå†æ‰€æœ‰é…ç½®ï¼Œå¦‚æœå—å¤§å°ä¸åŒ¹é…æˆ–é…ç½®ä¸º2é˜¶æ®µï¼Œåˆ™è·³è¿‡
    for BLOCK_M, BLOCK_N, num_warps, num_stages in configs:
        if SPARSE_KV_BLOCK_SIZE % BLOCK_N != 0 or SPARSE_Q_BLOCK_SIZE % BLOCK_M != 0:
            continue
        if num_stages == 2:
            continue

        // å°†å½“å‰é…ç½®æ·»åŠ åˆ°é€‰æ‹©åˆ—è¡¨ä¸­
        flex_attention_template.maybe_append_choice(
            choices=choices,
            input_nodes=[
                query,
                key,
                value,
                logsumexp,
                kv_num_blocks,
                kv_indices,
                full_kv_num_blocks,
                full_kv_indices,
            ],
            layout=layout,
            subgraphs=[
                subgraph_buffer,
                mask_graph_buffer,
            ],
            mutated_inputs=[
                logsumexp,
            ],
            num_stages=num_stages,
            num_warps=num_warps,
            call_sizes=query.get_size(),
            OUTPUT_LOGSUMEXP=True,
            SM_SCALE=scale,
            BLOCK_DMODEL=query.get_size()[-1],
            BLOCK_M=BLOCK_M,
            BLOCK_N=BLOCK_N,
            SPARSE_Q_BLOCK_SIZE=SPARSE_Q_BLOCK_SIZE,
            SPARSE_KV_BLOCK_SIZE=SPARSE_KV_BLOCK_SIZE,
            ROWS_GUARANTEED_SAFE=False,
            PRESCALE_QK=False,
            HAS_FULL_BLOCKS=has_full_blocks,
        )
    // åˆ›å»ºç”¨äºè‡ªåŠ¨è°ƒä¼˜çš„è¾“å…¥åˆ—è¡¨
    inputs_for_autotuning = (
        [
            query,
            key,
            value,
            logsumexp,
            kv_num_blocks,
            kv_indices,
            full_kv_num_blocks,
            full_kv_indices,
        ]
        + list(score_mod_other_buffers)
        + list(mask_mod_other_buffers)
    )
    // åˆ›å»ºè¾“å…¥ç”Ÿæˆå‡½æ•°æ˜ å°„
    input_gen_fns = {
        4: create_num_blocks_fake_generator(full_kv_indices),
        5: create_indices_fake,
    }
    // è¿”å›è‡ªåŠ¨è°ƒä¼˜é€‰æ‹©ç®—æ³•çš„ç»“æœå’Œlogsumexp
    return (
        autotune_select_algorithm(
            "flex_attention",
            choices,
            inputs_for_autotuning,
            layout,
            input_gen_fns=input_gen_fns,
        ),
        logsumexp,
    )

```


è¿™ä¸ª`flex_attention`å‡½æ•°çš„`block_mask`å‚æ•°æ˜¯é€šè¿‡ä¸Šé¢APIåº”ç”¨ä¸­æåˆ°çš„`create_block_mask`å‡½æ•°æ¥åˆ›å»ºçš„ã€‚ç„¶åè¿™ä¸ªå‡½æ•°æ¥å—æŸ¥è¯¢ï¼ˆqueryï¼‰ã€é”®ï¼ˆkeyï¼‰ã€å€¼ï¼ˆvalueï¼‰ã€å­å›¾ï¼ˆsubgraphï¼‰ã€å—æ©ç ï¼ˆblock_maskï¼‰ã€ç¼©æ”¾å› å­ï¼ˆscaleï¼‰ä»¥åŠåˆ†æ•°å’Œæ©ç ä¿®æ”¹ç¼“å†²åŒºä½œä¸ºè¾“å…¥ã€‚å‡½æ•°å†…éƒ¨é€šè¿‡åˆ›å»ºå ä½ç¬¦è¾“å…¥ã€æ„å»ºå­å›¾å’Œæ©ç å›¾ç¼“å†²åŒºï¼Œå¹¶æ ¹æ®é…ç½®é€‰æ‹©åˆé€‚çš„kernelæ¥å®ç° FlexAttention è®¡ç®—ã€‚æœ€ç»ˆè¿”å›è‡ªåŠ¨è°ƒä¼˜é€‰æ‹©ç®—æ³•çš„ç»“æœå’Œ logsumexp å¼ é‡ã€‚æ„Ÿå…´è¶£çš„æœ‹å‹ä¹Ÿå¯ä»¥çœ‹ä¸‹è¿™é‡Œçš„triton kernelçš„å…·ä½“å®ç°ã€‚











