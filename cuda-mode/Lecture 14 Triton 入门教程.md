<h1><b></v>Triton å®è·µæŒ‡å—</b></h1>

ä½œè€…ï¼šUmerHA (https://x.com/UmerHAdil // https://github.com/UmerHA/)ï¼Œä¸º cuda-mode å°ç»„ç¼–å†™ â¤ï¸ May our brrrr level reach over ã€‚

# ä¸ºä»€ä¹ˆä»¥åŠä½•æ—¶ä½¿ç”¨Triton

**ä»€ä¹ˆæ˜¯Triton**
ç®€è€Œè¨€ä¹‹ï¼šTritonæ˜¯ä¸€ç§æ›´æ–¹ä¾¿åœ°ç¼–ç¨‹GPUçš„è¯­è¨€ã€‚ä½ ç¼–å†™ç±»ä¼¼Pythonçš„ä»£ç ï¼Œç„¶åè¿™äº›ä»£ç è¢«ç¼–è¯‘æˆptxä»£ç ï¼ˆä¸cudaä»£ç ç¼–è¯‘æˆçš„ä»£ç ç›¸åŒï¼‰ã€‚

åœ¨ç¼–è¯‘è¿‡ç¨‹ä¸­ï¼ŒTritonç¼–è¯‘å™¨å°è¯•ä½¿ç”¨å·§å¦™çš„æŠ€å·§æ¥é‡æ–°æ’åˆ—ç¨‹åºçš„éƒ¨åˆ†å†…å®¹ï¼ˆä¸æ”¹å˜ç¨‹åºçš„æ„ä¹‰ï¼ï¼‰ä»¥ä½¿å…¶è¿è¡Œå¾—æ›´å¿«ã€‚

**Triton vs Cuda**

![](https://files.mdnice.com/user/59/dda26356-ad90-415d-8cbe-5b5b4bad930d.png)
source: https://zhuanlan.zhihu.com/p/672086654

CUDA æ˜¯ä¸€ä¸ªé«˜ç«¯å·¥å…·ï¼Œæœ‰è®¸å¤šè®¾ç½®ä¾›ä¸“ä¸šäººå£«ä½¿ç”¨ã€‚
- å¯¹æ‰€æœ‰å†…å®¹æœ‰å®Œå…¨æ§åˆ¶ï¼Œå› æ­¤å¯ä»¥å®ç°ç»å¯¹æœ€å¤§æ€§èƒ½
- æ›´éš¾è·å¾—è‰¯å¥½çš„æ€§èƒ½
- ç¼–å†™å’Œè°ƒè¯•æ›´åŠ ç¹ç
- æ›´å¤æ‚ï¼Œå› æ­¤æ›´éš¾å­¦ä¹ 

Triton æ˜¯ä¸€ä¸ªéå¸¸é€‚åˆå¤§å¤šæ•°ç”¨æˆ·çš„å·¥å…·
- ä½ ä¸èƒ½æ§åˆ¶æ‰€æœ‰å†…å®¹ï¼Œå› ä¸ºæœ‰äº›äº‹æƒ…ç•™ç»™è‡ªåŠ¨ä¼˜åŒ–ï¼›æ‰€ä»¥ä½ å¯èƒ½ä¸ä¼šè·å¾—ç»å¯¹æœ€å¤§æ€§èƒ½
- æ›´å®¹æ˜“è·å¾—è‰¯å¥½çš„æ€§èƒ½
- æ›´å®¹æ˜“ç¼–å†™å’Œè°ƒè¯•
- æ›´å®¹æ˜“å­¦ä¹ ï¼Œå› ä¸ºå®ƒå…·æœ‰ç±»ä¼¼ Python çš„è¯­æ³•

**Triton vs torch.compile**

`torch.compile` é€šè¿‡å°è¯•æ›´æœ‰æ•ˆåœ°ä½¿ç”¨ç°æœ‰kernelå¹¶åˆ›å»ºç®€å•çš„æ–°kernelæ¥ä½¿ä½ çš„æ¨¡å‹æ›´å¿«ã€‚è¿™å¯èƒ½ä¼šä½¿ä½ çš„æ¨¡å‹è¶³å¤Ÿå¿«ã€‚å¦‚æœæ²¡æœ‰ï¼Œä½ å¯ä»¥å†³å®šæŠ•å…¥æ—¶é—´ç¼–å†™æ›´å¿«çš„ Triton kernelã€‚

ï¼ˆ`torch.compile` åˆ›å»ºçš„è¿™äº›ç®€å•æ–°kernelå®é™…ä¸Šæ˜¯ Triton kernelã€‚å› æ­¤ï¼Œå®ƒä»¬æ˜¯è‡ªå®šä¹‰kernelçš„è‰¯å¥½èµ·ç‚¹ã€‚å‚è§ [Mark Saroufim](https://twitter.com/marksaroufim) çš„ [cuda mode ç¬¬ä¸€è®²](https://www.youtube.com/watch?v=LuhJEEJQgUM&t=2200s) äº†è§£å¦‚ä½•æ“ä½œã€‚ï¼‰

**ä½•æ—¶ä½¿ç”¨ Triton**

ä½ ä»ä½ çš„ AI æ¨¡å‹å¼€å§‹ã€‚
1. å¦‚æœå®ƒä¸å¤Ÿå¿«ï¼Œä½¿ç”¨ `torch.compile`ã€‚
2. å¦‚æœå®ƒä¸å¤Ÿå¿«ï¼Œæ£€æŸ¥ä½ æ˜¯å¦å¯ä»¥é‡å†™ä»£ç ä»¥ä½¿å…¶æ›´é€‚åˆ `torch.compile`ã€‚
3. å¦‚æœå®ƒä¸å¤Ÿå¿«ï¼Œæ£€æŸ¥å“ªäº›éƒ¨åˆ†æ…¢å¹¶ä¸ºå…¶ç¼–å†™è‡ªå®šä¹‰ Triton kernelã€‚
4. å¦‚æœå®ƒä¸å¤Ÿå¿«ï¼Œæ£€æŸ¥å“ªäº›éƒ¨åˆ†æ…¢å¹¶ä¸ºå…¶ç¼–å†™è‡ªå®šä¹‰ CUDA kernelã€‚

ï¼ˆåœ¨ä¸å¤ªå¯èƒ½çš„æƒ…å†µä¸‹ï¼Œå¦‚æœä½ äº‹å…ˆçŸ¥é“ä½ éœ€è¦ç»å¯¹æœ€å¤§æ€§èƒ½ï¼Œä½ å¯ä»¥å†³å®šç›´æ¥ä» CUDA å¼€å§‹ã€‚ï¼‰

**å…³äºç²—ç³™è¾¹ç¼˜çš„è¯´æ˜**

ç”±äº Triton æ˜¯ä¸€ä¸ªè¾ƒæ–°çš„é¡¹ç›®ï¼Œäººä»¬å‘ç°å®ƒæœ‰ä¸€äº›ç²—ç³™çš„è¾¹ç¼˜ã€‚æˆ‘å·²ç»è®°å½•äº†æˆ‘é‡åˆ°çš„æ‰€æœ‰ç²—ç³™è¾¹ç¼˜ï¼Œå¹¶åœ¨è¯„è®ºä¸­æ³¨æ˜äº†â€œWeirdness: <æˆ‘å¯¹å¥‡æ€ªä¹‹å¤„çš„æè¿°>â€ã€‚

æˆ‘é¢„è®¡å®ƒä¼šåœ¨æœªæ¥å˜å¾—æ›´åŠ å®Œå–„ã€‚

# å¦‚ä½•ç¼–å†™Triton kernel

ä¸CUDAä¸åŒï¼Œå¦‚æœæˆ‘ä»¬è®¾ç½®ç¯å¢ƒå˜é‡ `TRITON_INTERPRET = 1`ï¼Œæˆ‘ä»¬å¯ä»¥åƒè°ƒè¯•ä»»ä½•CPUç¨‹åºä¸€æ ·è°ƒè¯•Triton kernelã€‚ç„¶åTritonåœ¨CPUä¸Šè¿è¡Œï¼Œä½†æ¨¡æ‹Ÿå®ƒåœ¨GPUä¸Šè¿è¡Œã€‚

æˆ‘å»ºè®®é¦–å…ˆåœ¨æ¨¡æ‹Ÿå™¨ä¸­ç¼–å†™æ‰€æœ‰ç¨‹åºï¼Œå¹¶æ£€æŸ¥å…¶æ­£ç¡®æ€§ã€‚å¦‚æœæ­£ç¡®ï¼Œç„¶åä½ å¯ä»¥ä½¿å…¶å¿«é€Ÿè¿è¡Œã€‚

ä»¥ä¸‹æ˜¯ä¸€äº›ç”¨äºè°ƒè¯•çš„å®ç”¨å‡½æ•°ï¼š
- `check_tensors_gpu_ready`ï¼š(i) æ–­è¨€æ‰€æœ‰å¼ é‡åœ¨å†…å­˜ä¸­æ˜¯è¿ç»­çš„ï¼Œ(ii) ä»…åœ¨éæ¨¡æ‹Ÿæƒ…å†µä¸‹ï¼Œæ–­è¨€æ‰€æœ‰å¼ é‡åœ¨GPUä¸Š
- `breakpoint_if`ï¼šæ ¹æ®pidsçš„æ¡ä»¶è®¾ç½®æ–­ç‚¹
- `print_if`ï¼šæ ¹æ®pidsçš„æ¡ä»¶æ‰“å°å†…å®¹

```python
import os
from IPython.core.debugger import set_trace

os.environ['TRITON_INTERPRET'] = '1' # needs to be set *before* triton is imported

def check_tensors_gpu_ready(*tensors):
    """æ£€æŸ¥æ‰€æœ‰å¼ é‡æ˜¯å¦åœ¨GPUä¸Šå¹¶ä¸”æ˜¯è¿ç»­çš„"""
    for t in tensors:
        assert t.is_contiguous, "A tensor is not contiguous"  # æ–­è¨€å¼ é‡æ˜¯è¿ç»­çš„
        if not os.environ.get('TRITON_INTERPRET') == '1': assert t.is_cuda, "A tensor is not on cuda"  # å¦‚æœä¸æ˜¯æ¨¡æ‹Ÿæ¨¡å¼ï¼Œæ–­è¨€å¼ é‡åœ¨GPUä¸Š

def test_pid_conds(conds, pid_0=[0], pid_1=[0], pid_2=[0]):
    """æµ‹è¯•pidæ¡ä»¶æ˜¯å¦æ»¡è¶³
    ä¾‹å¦‚:
        '=0'  æ£€æŸ¥pid_0 == 0
        ',>1' æ£€æŸ¥pid_1 > 1
        '>1,=0' æ£€æŸ¥pid_0 > 1 ä¸” pid_1 == 0
    """
    pids = pid_0[0], pid_1[0], pid_2[0]  # è·å–pidå€¼
    conds = conds.replace(' ','').split(',')  # å»é™¤ç©ºæ ¼å¹¶åˆ†å‰²æ¡ä»¶
    for i, (cond, pid) in enumerate(zip(conds, pids)):
        if cond=='': continue  # å¦‚æœæ¡ä»¶ä¸ºç©ºï¼Œè·³è¿‡
        op, threshold = cond[0], int(cond[1:])  # è·å–æ“ä½œç¬¦å’Œé˜ˆå€¼
        if op not in ['<','>','>=','<=','=', '!=']: raise ValueError(f"Rules may only use these ops: '<','>','>=','<=','=', '!='. Invalid rule: '{condition}'.")  # æ£€æŸ¥æ“ä½œç¬¦æ˜¯å¦åˆæ³•
        op = '==' if op == '=' else op  # å°†'='æ›¿æ¢ä¸º'=='
        if not eval(f'{pid} {op} {threshold}'): return False  # è¯„ä¼°æ¡ä»¶æ˜¯å¦æ»¡è¶³
    return True

assert test_pid_conds('')  # æµ‹è¯•ç©ºæ¡ä»¶
assert test_pid_conds('>0', [1], [1])  # æµ‹è¯•pid_0 > 0
assert not test_pid_conds('>0', [0], [1])  # æµ‹è¯•pid_0 > 0ä¸æ»¡è¶³
assert test_pid_conds('=0,=1', [0], [1], [0])  # æµ‹è¯•pid_0 = 0 ä¸” pid_1 = 1

def breakpoint_if(conds, pid_0=[0], pid_1=[0], pid_2=[0]):
    """å¦‚æœä»»ä½•pidæ¡ä»¶æ»¡è¶³ï¼Œåœæ­¢kernel"""
    if test_pid_conds(conds, pid_0, pid_1, pid_2): set_trace()  # å¦‚æœæ¡ä»¶æ»¡è¶³ï¼Œè®¾ç½®æ–­ç‚¹

def print_if(txt, conds, pid_0=[0], pid_1=[0], pid_2=[0]):
    """å¦‚æœä»»ä½•pidæ¡ä»¶æ»¡è¶³ï¼Œæ‰“å°txt"""
    if test_pid_conds(conds, pid_0, pid_1, pid_2): print(txt)  # å¦‚æœæ¡ä»¶æ»¡è¶³ï¼Œæ‰“å°æ–‡æœ¬

def cdiv(a,b): 
    """è®¡ç®—aé™¤ä»¥bçš„ä¸Šé™å€¼"""
    return (a + b - 1) // b  # è®¡ç®—aé™¤ä»¥bçš„ä¸Šé™å€¼
assert cdiv(10,2)==5  # æµ‹è¯•cdivå‡½æ•°
assert cdiv(10,3)==4  # æµ‹è¯•cdivå‡½æ•°
```

```python
import torch
import triton
import triton.language as tl
```

# ç¼–ç¨‹æ¨¡å‹

åœ¨CUDAä¸­ï¼Œæˆ‘ä»¬å°†è®¡ç®—åˆ†è§£ä¸ºä¸¤ä¸ªå±‚æ¬¡ï¼šé¦–å…ˆæ˜¯å—ï¼Œç„¶åæ¯ä¸ªå—è¿›ä¸€æ­¥åˆ†è§£ä¸ºçº¿ç¨‹ã€‚ä¸€ä¸ªå—ä¸­çš„æ‰€æœ‰çº¿ç¨‹è¿è¡Œåœ¨åŒä¸€ä¸ªSMä¸Šï¼Œå¹¶å…±äº«ç›¸åŒçš„å…±äº«å†…å­˜ã€‚æ¯ä¸ªçº¿ç¨‹è®¡ç®—**æ ‡é‡**ã€‚

åœ¨Tritonä¸­ï¼Œæˆ‘ä»¬åªå°†è®¡ç®—åˆ†è§£ä¸ºä¸€ä¸ªå±‚æ¬¡ï¼šå—ã€‚æ²¡æœ‰è¿›ä¸€æ­¥çš„çº¿ç¨‹åˆ†è§£ã€‚**Tritonè¦æ±‚æˆ‘ä»¬å¯¹å‘é‡è¿›è¡Œæ“ä½œ**ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬ä¸éœ€è¦ä¹Ÿä¸èƒ½ç®¡ç†å…±äº«å†…å­˜ã€‚Tritonä¼šè‡ªåŠ¨å¤„ç†è¿™äº›ã€‚

ç¤ºä¾‹ï¼š

å‡è®¾æˆ‘ä»¬è¦å°†å¤§å°ä¸º8çš„å‘é‡`x`å’Œ`y`ç›¸åŠ ï¼Œå¹¶å°†è¾“å‡ºä¿å­˜åˆ°å¤§å°ä¹Ÿä¸º8çš„å‘é‡`z`ä¸­ã€‚æˆ‘ä»¬ä½¿ç”¨å¤§å°ä¸º4çš„å—ï¼Œå› æ­¤æˆ‘ä»¬æœ‰`8 / 4 = 2`ä¸ªå—ã€‚
- CUDAè¿è¡Œ2ä¸ªå—ï¼Œæ¯ä¸ªå—æœ‰4ä¸ªçº¿ç¨‹ã€‚8ä¸ªçº¿ç¨‹ä¸­çš„æ¯ä¸€ä¸ªè®¡ç®—ä¸€ä¸ªå•ç‹¬çš„ä½ç½®ï¼Œä¾‹å¦‚`z[0] = x[0] + y[0]`
- Tritonä¹Ÿè¿è¡Œ2ä¸ªå—ï¼Œæ¯ä¸ªå—æ‰§è¡Œå‘é‡åŒ–åŠ æ³•ã€‚å‘é‡çš„å¤§å°æ˜¯å—çš„å¤§å°ï¼Œå³4ã€‚ä¾‹å¦‚`z[0:3] = x[0:3] + y[0:3]`

**æ‰€æœ‰**Triton kernelä¸­çš„æ“ä½œéƒ½æ˜¯å‘é‡åŒ–çš„ï¼šåŠ è½½æ•°æ®ã€æ“ä½œæ•°æ®ã€å­˜å‚¨æ•°æ®å’Œåˆ›å»ºæ©ç ã€‚

è®©æˆ‘ä»¬è€ƒè™‘å¦ä¸€ä¸ªç®€å•çš„ä¾‹å­ï¼š

åŒæ ·ï¼Œæˆ‘ä»¬è¦å°†å¤§å°ä¸º**6**çš„å‘é‡`x`å’Œ`y`ç›¸åŠ ï¼Œå¹¶å°†è¾“å‡ºä¿å­˜åˆ°å¤§å°ä¹Ÿä¸º6çš„å‘é‡`z`ä¸­ã€‚æˆ‘ä»¬ä½¿ç”¨å¤§å°ä¸º4çš„å—ï¼Œå› æ­¤æˆ‘ä»¬æœ‰`cdiv(6, 4) = 2`ä¸ªå—ã€‚

```python
x = torch.tensor([1,2,3,4,5,6])
y = torch.tensor([0,1,0,1,0,1])

x, y, x+y
```

CUDA kernelå°†ç±»ä¼¼äºä»¥ä¸‹Cä»£ç ï¼š

```python
# x,y = è¾“å…¥å¼ é‡, z = è¾“å‡ºå¼ é‡, n = xçš„å¤§å°, bs = å—å¤§å°
def add_cuda_k(x, y, z, n, bs):
    # å®šä½æ­¤ç‰¹å®škernelæ­£åœ¨æ‰§è¡Œçš„æ•´ä½“è®¡ç®—çš„å“ªä¸€éƒ¨åˆ†
    block_id = ... # åœ¨æˆ‘ä»¬çš„ä¾‹å­ä¸­: æ˜¯[0,1]ä¸­çš„ä¸€ä¸ª
    thread_id = ... # åœ¨æˆ‘ä»¬çš„ä¾‹å­ä¸­: æ˜¯[0,1,2,3]ä¸­çš„ä¸€ä¸ª

    # è¯†åˆ«æ­¤ç‰¹å®škerneléœ€è¦çš„æ•°æ®ä½ç½®
    offs = block_id * bs + thread_id
    
    # ä¿æŠ¤å­å¥, ç¡®ä¿æˆ‘ä»¬ä¸ä¼šè¶Šç•Œ
    if offs < n:

        # è¯»å–æ•°æ®
        x_value = x[offs]
        y_value = y[offs]
        
        # æ‰§è¡Œæ“ä½œ
        z_value = x_value + y_value
        
        # å†™å…¥æ•°æ®
        z[offs] = z_value

    # é‡è¦: offs, x_value, y_value, x_value éƒ½æ˜¯æ ‡é‡!
    # ä¿æŠ¤æ¡ä»¶ä¹Ÿæ˜¯ä¸€ç§æ ‡é‡, å› ä¸ºå®ƒæ£€æŸ¥ä¸€ä¸ªå€¼ä¸Šçš„ä¸€ä¸ªæ¡ä»¶ã€‚
```

ä¸ºäº†è¯´æ˜ï¼Œè¿™é‡Œæ˜¯æ¯ä¸ªkernelçš„å˜é‡ï¼š

![](https://files.mdnice.com/user/59/29863a80-c092-4d8d-a7de-0ecdb2a9213c.png)

ç°åœ¨è®©æˆ‘ä»¬çœ‹ä¸€ä¸‹ç›¸åº”çš„Triton kernelï¼Œå¤§è‡´å¦‚ä¸‹æ‰€ç¤ºï¼š

```python
# æ³¨æ„ï¼šè¿™æ˜¯ä¸ºäº†è¯´æ˜ï¼Œè¯­æ³•ä¸å®Œå…¨æ­£ç¡®ã€‚è¯·å‚è§ä¸‹æ–‡ä»¥è·å–æ­£ç¡®çš„Tritonè¯­æ³•

def add_triton_k(x, y, z, n, bs):
    # å®šä½æ­¤ç‰¹å®škernelæ­£åœ¨æ‰§è¡Œçš„æ•´ä½“è®¡ç®—çš„å“ªä¸€éƒ¨åˆ†
    block_id = tl.program_id(0)  # åœ¨æˆ‘ä»¬çš„ä¾‹å­ä¸­: æ˜¯[0,1]ä¸­çš„ä¸€ä¸ª
    
    # è¯†åˆ«æ­¤ç‰¹å®škerneléœ€è¦çš„æ•°æ®ä½ç½®
    offs = block_id * bs + tl.arange(0, bs) # <- è¿™æ˜¯ä¸€ä¸ªå‘é‡!
    
    # ä¿æŠ¤å­å¥å˜æˆä¸€ä¸ªæ©ç ï¼Œè¿™æ˜¯ä¸€ä¸ªå¸ƒå°”å‘é‡
    mask = offs < n # <- è¿™æ˜¯ä¸€ä¸ªå¸ƒå°”å‘é‡!
    
    # è¯»å–æ•°æ®
    x_values = x[offs] # <- è¯»å–ä¸€ä¸ªå‘é‡!
    y_values = y[offs] # <- è¯»å–ä¸€ä¸ªå‘é‡!
    
    # æ‰§è¡Œæ“ä½œ
    z_value = x_value + y_value  # <- å‘é‡ç›¸åŠ !
    
    # å†™å…¥æ•°æ®
    z[offs] = z_value  # <- å†™å…¥ä¸€ä¸ªå‘é‡!
```

å†æ¬¡è¯´æ˜ï¼Œè¿™é‡Œæ˜¯æ¯ä¸ªkernelçš„å˜é‡ï¼š

![](https://files.mdnice.com/user/59/8dd01a33-89cc-4587-8376-b2471600c63a.png)

æœ¯è¯­è¯´æ˜ï¼šåœ¨Tritonæœ¯è¯­ä¸­ï¼Œæ¯ä¸ªå¤„ç†å—çš„kernelè¢«ç§°ä¸ºâ€œprogramâ€ã€‚ä¹Ÿå°±æ˜¯è¯´ï¼Œæˆ‘ä»¬ä¸Šé¢çš„ä¾‹å­è¿è¡Œäº†2ä¸ªprogramã€‚å› æ­¤ï¼Œâ€œblock_idâ€é€šå¸¸è¢«ç§°ä¸ºâ€œpidâ€ï¼ˆâ€œprogram idâ€çš„ç¼©å†™ï¼‰ï¼Œä½†å®ƒä»¬æ˜¯ç›¸åŒçš„ã€‚

# Example 1: Copying a tensor

Let's looks at some examples. To keeps things simple, we'll use very small block sizes.

Goal: Given a tensor `x` of shape (n), copy it into another tensor `z`.

```python
# # This is a normal python function, which launches the triton kernels
def copy(x, bs, kernel_fn):
    z = torch.zeros_like(x)
    check_tensors_gpu_ready(x, z)
    n = x.numel()
    n_blocks = cdiv(n, bs)
    grid = (n_blocks,)  # how many blocks do we have? can be 1d/2d/3d-tuple or function returning 1d/2d/3d-tuple

    # launch grid!
    # - kernel_fn is the triton kernel, which we write below
    # - grid is the grid we constructed above
    # - x,z,n,bs are paramters that are passed into each kernel function
    kernel_fn[grid](x,z,n,bs)

    return z
```

**Note:** For educational purposes, the kernel below has a logic bug (but the syntax is correct). Can you spot it?

```python
# # This is the triton kernel:

# The triton.jit decorator takes a python function and turns it into a triton kernel, which is run on the GPU.
# Inside this function only a subset of all python ops are allowed.
# E.g., when NOT simulating, we can't print or use breakpoints, as these don't exist on the GPU. 
@triton.jit
# When we pass torch tensors, they are automatically converted into a pointer to their first value
# E.g., above we passed x, but here we receive x_ptr
def copy_k(x_ptr, z_ptr, n, bs: tl.constexpr):
    pid = tl.program_id(0)
    offs = tl.arange(0, bs)  # compute the offsets from the pid 
    mask = offs < n
    x = tl.load(x_ptr + offs, mask) # load a vector of values, think of `x_ptr + offs` as `x_ptr[offs]`
    tl.store(z_ptr + offs, x, mask) # store a vector of values

    print_if(f'pid = {pid} | offs = {offs}, mask = {mask}, x = {x}', '')

    # Question: What is wrong with this kernel?
```

```python
z = copy(x, bs=2, kernel_fn=copy_k)
```

```
z
```

```shell
tensor([1, 2, 0, 0, 0, 0])
```

We were not shifting the offets correcltly. We always used offsets = [0,1], but they should change with the pid.

```python
@triton.jit
def copy_k(x_ptr, z_ptr, n, bs: tl.constexpr):
    pid = tl.program_id(0)
    offs = pid * n + tl.arange(0, bs)
    mask = offs < n
    x = tl.load(x_ptr + offs, mask)
    tl.store(z_ptr + offs, x, mask)
    print_if(f'pid = {pid} | offs = {offs}, mask = {mask}, x = {x}', '')
```

```
z = copy(x, bs=2, kernel_fn=copy_k)
```

Not quite correct. We added `pid * n`, but want to add `pid * bs`

```python
@triton.jit
def copy_k(x_ptr, z_ptr, n, bs: tl.constexpr):
    pid = tl.program_id(0)
    offs = pid * bs + tl.arange(0, bs)
    mask = offs < n
    x = tl.load(x_ptr + offs, mask)
    tl.store(z_ptr + offs, x, mask)
    print_if(f'pid = {pid} | offs = {offs}, mask = {mask}, x = {x}', '')
```

```python
z = copy(x, bs=2, kernel_fn=copy_k)
```

```shell
pid = [0] | offs = [0 1], mask = [ True  True], x = [1 2]
pid = [1] | offs = [2 3], mask = [ True  True], x = [3 4]
pid = [2] | offs = [4 5], mask = [ True  True], x = [5 6]
```

Yes!

```python
x, z
```

```shell
(tensor([1, 2, 3, 4, 5, 6]), tensor([1, 2, 3, 4, 5, 6]))
```

As we saw, writing GPU programs involves many indices, which we can easily mess up. So I highly recommend writing and debugging the kernel in simuation mode, and testing with tiny examples first!


# Example 2: Greyscaling an image

_Restart kernel here_
In this example, we'll grayscale an image of a puppy. We'll see how we can work on 2d data.

This works analogously for 3D data.

We've adapted Jeremy Howard's example from this [colab](https://colab.research.google.com/drive/180uk6frvMBeT4tywhhYXmz3PJaCIA_uk?usp=sharing) / [youtube](https://www.youtube.com/watch?v=4sgKnKbR-WE&feature=youtu.be). So, h/t for the example and selection of puppy image.
_Side note: Two weird things happen in this example, if we don't restart the kernel:_
1. _torchvision can't be imported, probably due to a circular dependency. -> I currently don't know why, need to dig deeper._
2. _the simulated triton kernel below fails, because a float can't be mutliplied to a uint vector -> Works on GPU w/o simulation, so seems to be a `TRITON_INTERPRET` bug._
import os

import matplotlib.pyplot as plt
from urllib.request import urlretrieve
from pathlib import Path

import torch
from torch import tensor
import torchvision as tv
import torchvision.transforms.functional as tvf
from torchvision import io

import triton
import triton.language as tl
def cdiv(a,b): return (a + b - 1) // b
url = 'https://upload.wikimedia.org/wikipedia/commons/thumb/4/43/Cute_dog.jpg/1600px-Cute_dog.jpg?20140729055059'
path_img = Path('puppy.jpg')
if not path_img.exists(): urlretrieve(url, path_img)
img = io.read_image('puppy.jpg')
print(img.shape)
img[:2,:3,:4]
def show_img(x, figsize=(4,3), **kwargs):
    plt.figure(figsize=figsize)
    plt.axis('off')
    if len(x.shape)==3: x = x.permute(1,2,0)  # CHW -> HWC
    plt.imshow(x.cpu(), **kwargs)
img = tvf.resize(img, 150, antialias=True)
ch,h,w = img.shape
ch,h,w,h*w
show_img(img)
To work with 2d data, we'll build 2d offsets and masks. Here's an illustration how it works, e.g. for an `4x7` matrix and block sizes of `2` for each dimensions.
<img src='images/4_offset_2d.png'>
And in code, it looks like this:
@triton.jit
def rgb2grey_k(x_ptr, out_ptr, h, w, bs0: tl.constexpr, bs1: tl.constexpr):
    pid_0 = tl.program_id(0)
    pid_1 = tl.program_id(1)
    
    offs_0 = pid_0 * bs0 + tl.arange(0,bs0)  # 1d vector
    offs_1 = pid_1 * bs1 + tl.arange(0,bs1)  # 1d vector

    # Weirdness: None-slicing currently doesn't work when simulating on cpu. Use tl.expand_dim instead.
    # offs = w * tl.expand_dims(offs_0, 1) + tl.expand_dims(offs_1, 0)
    offs = w * offs_0[:,None] + offs_1[None, :]  # 2d matrix! - we multiply first offset by width, see image above

    mask_0 = offs_0 < h  # 1d vector
    mask_1 = offs_1 < w  # 1d vector

    # mask = tl.expand_dims(mask_0, 1) & tl.expand_dims(mask_1, 0)
    mask = mask_0[:,None] & mask_1[None,:]  # 2d matrix! - data musn't go out of bounds along either axis, therefore `logical and` of the individual masks
    
    r = tl.load(x_ptr + 0*h*w+offs, mask=mask)
    g = tl.load(x_ptr + 1*h*w+offs, mask=mask)
    b = tl.load(x_ptr + 2*h*w+offs, mask=mask)

    # Weirdness: multiplying float with uint vectors fails when simulating on cpu
    out = 0.2989*r + 0.5870*g + 0.1140*b  # don't worry why it's these 3 numbers we're multiplying with

    tl.store(out_ptr + offs, out, mask=mask)
Let's use the kernel!
def rgb2grey(x, bs):
    c,h,w = x.shape
    out = torch.empty((h,w), dtype=x.dtype, device=x.device)

    # grid can be a function returning a 1d/2d/3d-tuple
    # (having a grid function is not more useful than a grid tuple in this case, but will be below when benchmarking & auto-tuning)
    grid = lambda meta: (cdiv(h, meta['bs0']), cdiv(w,  meta['bs1']))
    
    rgb2grey_k[grid](x, out, h, w, bs0=bs[0], bs1=bs[1]) # all kwargs are passed into grid function
    return out.view(h,w)
grey_img = rgb2grey(img.to('cuda'), bs=(32, 32)).to('cpu')
show_img(grey_img, cmap='gray')
Very cool


# Example 3: Matmul
_For simplicity, restart kernel here_
import os
# os.environ['TRITON_INTERPRET'] = '1'

import torch
import triton
import triton.language as tl

# moved util functions into separate file for better readability
from triton_util import cdiv, breakpoint_if, print_if, check_tensors_gpu_ready
Now, let's implement a naive matmul in Triton. We'll learn:
- A method to split computation 
- Calling functions from our kernel 
- Using pre-implemented vector/matrix ops within an block

This is adapted from the [OpenAI blog post announcing Triton](https://openai.com/research/triton).

We want to multiply the `m x k`-matrix `A` and the `k x n`-matrix `B` into the `m x n`-matrix `C`.

We split the computation along each of the three axes:
- along the m axis - we'll use block dimension 0 to represent this
- along the n axis - we'll use block dimension 1 to represent this
- along the shared k axis - this will not be represented by a block. All chunks of computation will be done in same block.
<img src='images/5_matmul_split.png'>
Because we frequently create 1d- or 2d-offets and -masks, let's put that functionality into utility functions. As long as these functions are `triton.jit`-ed, they can be used in the kernel.
@triton.jit
def get_1d_offset(size, n_prev_chunks):
    return n_prev_chunks * size + tl.arange(0, size)

@triton.jit
def get_2d_offset(offs_0, offs_1, stride_0, stride_1=1): 
    return tl.expand_dims(offs_0, 1)*stride_0 + tl.expand_dims(offs_1, 0)*stride_1

@triton.jit
def get_1d_mask(offs, max):
    return offs < max

@triton.jit
def get_2d_mask(offs_0, offs_1, max_0, max_1):
    return (tl.expand_dims(offs_0, 1) < max_0) & (tl.expand_dims(offs_1, 0) < max_1)
Here's the naive matmul kernel:
@triton.jit
def naive_matmul_k(
    a_ptr, b_ptr, c_ptr,
    m, n, k,
    stride_am, stride_ak, 
    stride_bk, stride_bn,
    stride_cm, stride_cn,
    bm: tl.constexpr, bn: tl.constexpr, bk: tl.constexpr
):
    pid_m, pid_n = tl.program_id(0), tl.program_id(1)
    # chunks along m/n/k dimensions
    rm = get_1d_offset(size=bm, n_prev_chunks=pid_m)
    rn = get_1d_offset(size=bn, n_prev_chunks=pid_n)
    rk = get_1d_offset(size=bk, n_prev_chunks=0)
    # relevant offsets of a, b
    offs_a = a_ptr + get_2d_offset(rm, rk, stride_am, stride_ak)
    offs_b = b_ptr + get_2d_offset(rk, rn, stride_bk, stride_bn)
    # initialize and iteratively update accumulator
    acc = tl.zeros((bm, bn), dtype=tl.float32)
    for _ in range(0, k, bk):
        # todo umer: don't we need mask when loading a & b?
        a = tl.load(offs_a)
        b = tl.load(offs_b)
        acc += tl.dot(a, b, allow_tf32=False) # matmul in block ; Weirdness: allow_tf32 must be set to False for older GPUs, otherwise won't compile
        # increase offets, so next iteration loads next chunks
        offs_a += bk * stride_ak
        offs_b += bk * stride_bk
    c = c_ptr + get_2d_offset(rm, rn, stride_cm, stride_cn)
    mask = get_2d_mask(rm, rn, m, n)
    tl.store(c, acc, mask=mask)
from functools import partial

def matmul(a, b, matmul_k_fn, bs=16, group_sz=None):
    assert a.shape[1] == b.shape[0], "matrix dims not compatible for matmul"
    check_tensors_gpu_ready(a, b)
    (m, k), (_, n) = a.shape, b.shape
    c = torch.empty((m, n), device=a.device, dtype=torch.float16)
    grid = lambda meta: (triton.cdiv(m, meta['bm']),  triton.cdiv(n, meta['bn']))
    group_sz = {} if group_sz is None else {"group_sz":group_sz} # not used in naive_matmul, but will be in grouped_matmul further below 
    matmul_k_fn[grid](
        a, b, c,
        m, n, k,
        a.stride(0), a.stride(1),
        b.stride(0), b.stride(1),
        c.stride(0), c.stride(1),
        bm=bs, bn=bs, bk=bs, # Weirdness: allow_tf32 must be set to False for older GPUs, otherwise won't compile
        **group_sz
    )
    return c

naive_matmul = partial(matmul, matmul_k_fn=naive_matmul_k)
a = torch.ones((3, 4), dtype=torch.float32, device='cuda')
b = torch.ones((4, 5), dtype=torch.float32, device='cuda')
naive_matmul(a,b)
Let's unit test this against PyTorch's implementation
torch.manual_seed(0)
a = torch.randn((512, 512), device='cuda', dtype=torch.float16)
b = torch.randn((512, 512), device='cuda', dtype=torch.float16)
triton_output = naive_matmul(a, b)
torch_output = torch.matmul(a, b)
if torch.allclose(triton_output, torch_output, atol=5e-2, rtol=0):
    print("âœ… Triton and Torch match")
else:
    print("âŒ Triton and Torch differ")


# Example 4: Faster Matmul
_Note: Needs code from example 3, so run that before_
Triton handles the order of memory access **within** blocks, but not **across** blocks. So this is a knob we can use to make our kernels faster.

In fact, cleverly reordering blocks can increase L2-cache hit rate, which makes our kernels faster. This example is taken from the [triton docs](https://triton-lang.org/main/getting-started/tutorials/03-matrix-multiplication.html).
Now, to make better use of the L2 cache, we want to reuse data that's was recently loaded, and is therefore likely still in the L2 cache. How? By reducing the number of _different_ data loads that a bunch of "consecutive" kernels need. By "consecutive" we mean kernels that are executed approximately at the same time.

This picture (adapter from the [triton docs](https://triton-lang.org/main/getting-started/tutorials/03-matrix-multiplication.html)) shows how we can do that. If we order naively, the first row of the output matrix is computed "consecutively", which needs 90 different block reads (9 from matrix A, 81 from matrix B). If we use "group ordering", a 3x3 square of blocks of the output matrix is computed "consecutively", which needs 54 different block reads (27 from matrix A, 27 from matrix B).
<img src='images/6_matmul_order.png'>
_Note: In the docs, grouping is called "super-grouping"_
Okay, how can we tell Triton in which order to process blocks? The answer is: We take the pids, change them, and use them as if they were the original pids.

Let's do a minimal example to illustrate this principle:
def process_item(id): print(f"I'm processing item {id}")

for i in range(5): process_item(i)
def change_id(old_id): return 5-old_id

for i in range(5): process_item(change_id(i))
Et voilÃ , the items were processed in a different order.

So how should the pid-change-function for faster matmul look like? It should change the left matrix into the right matrix.
<img src='images/7_swizzling_exmple.png'>
On the left, the default ordering is shown (called "row-major"). Remember, we deal with blocks. We can't arrange how the individual cells are processed, only the blocks. In the picture, our output matrix C has `5x7 = 35` cells, but only `cdiv(5,1) x cdiv(7,2) = 5x4 = 20` blocks.

On the right, notice how the first 9 processed blocks are the `3x3` grid we want! We process 3 blocks in a column. Then advance a column, again process 3, advance, and so on. The orange lines show where advance. This operation is called **"swizzling"**.

By the way, you can of course change the number 3. It's called the `group_size`.

You don't need to write swizzling yourself, as  there is a `triton.language.swizzle2d` function.

To really understand `swizzle2d`, let's quickly verifiy it works as expected. We'll then continue to use it in our faster matmul kernel.
_Side-Goal:_ Use `swizzle2d` on a `5x4` matrix with elements `0 ... 19` in row-major ordering. We should then get a matrix with elements in grouped ordering.
@triton.jit
def swizzle_k(x_ptr, z_ptr, group_sz: tl.constexpr):
    pid_m, pid_n = tl.program_id(0), tl.program_id(1)
    num_pid_m, num_pid_n = tl.num_programs(0), tl.num_programs(1)

    pid_m_, pid_n_ = tl.swizzle2d(pid_m, pid_n, num_pid_m, num_pid_n, group_sz)  # Weirdness: tl.swizzle2d doesn't work when simulating on CPU
    
    offs_m = get_1d_offset(1, n_prev_chunks=pid_m)
    offs_n = get_1d_offset(1, n_prev_chunks=pid_n)
    
    offs = get_2d_offset(offs_m, offs_n, stride_0=num_pid_n)
    mask = get_2d_mask(offs_m, offs_n, max_0=num_pid_m, max_1=num_pid_n )

    offs_sw_m = get_1d_offset(1, n_prev_chunks=pid_m_)
    offs_sw_n = get_1d_offset(1, n_prev_chunks=pid_n_)
    
    offs_sw = get_2d_offset(offs_sw_m, offs_sw_n, stride_0=num_pid_n)
    mask_sw = get_2d_mask(offs_sw_m, offs_sw_n, max_0=num_pid_m, max_1=num_pid_n)
    
    x = tl.load(x_ptr + offs, mask=mask)
    tl.store(z_ptr + offs_sw, x, mask=mask_sw)
blocks_m, blocks_n = 5,4

x = torch.arange(blocks_m*blocks_n, device='cuda').view(blocks_m,blocks_n)
x
z = -torch.ones_like(x) # empty matrix, with -1 denoting empty
z
# swizzle x into z
swizzle_k[(blocks_m,blocks_n)](x,z, group_sz=3);
z
Looks good!
___
Let's now implement the grouped matmul kernel, which will be faster than the regular matmul.
@triton.jit
def grouped_matmul_k(
    a_ptr, b_ptr, c_ptr,
    m, n, k,
    stride_am, stride_ak, 
    stride_bk, stride_bn,
    stride_cm, stride_cn,
    bm: tl.constexpr, bn: tl.constexpr, bk: tl.constexpr, group_sz: tl.constexpr
):
    pid_m, pid_n = tl.program_id(0), tl.program_id(1)
    num_pid_m, num_pid_n = tl.num_programs(0), tl.num_programs(1)
    # determine location of block in grouped ordering - swizzle! 
    pid_m, pid_n = tl.swizzle2d(pid_m, pid_n, num_pid_m, num_pid_n, group_sz)  # Weirdness: tl.swizzle2d doesn't work when simulating on CPU
    # chunks along m/n/k dimensions
    rm = get_1d_offset(size=bm, n_prev_chunks=pid_m)
    rn = get_1d_offset(size=bn, n_prev_chunks=pid_n)
    rk = get_1d_offset(size=bk, n_prev_chunks=0)
    # relevant offsets of a, b
    offs_a = a_ptr + get_2d_offset(rm, rk, stride_am, stride_ak)
    offs_b = b_ptr + get_2d_offset(rk, rn, stride_bk, stride_bn)
    # initialize and iteratively update accumulator
    acc = tl.zeros((bm, bn), dtype=tl.float32)
    for _ in range(0, k, bk):
        # todo umer: don't we need mask when loading a & b?
        a = tl.load(offs_a)
        b = tl.load(offs_b)
        acc += tl.dot(a, b, allow_tf32=False) # block level matrix multiplication ; Weirdness: allow_tf32 must be set to False for older GPUs, otherwise won't compile
        # increase offets, so next iteration loads next chunks
        offs_a += bk * stride_ak
        offs_b += bk * stride_bk
    c = c_ptr + get_2d_offset(rm, rn, stride_cm, stride_cn)
    mask = get_2d_mask(rm, rn, m, n)
    tl.store(c, acc, mask=mask)
grouped_matmul = partial(matmul, matmul_k_fn=grouped_matmul_k)
a = torch.ones((3, 4), dtype=torch.float32, device='cuda')
b = torch.ones((4, 5), dtype=torch.float32, device='cuda')
grouped_matmul(a,b, group_sz=4)
Let's unit test this against PyTorch's implementation
torch.manual_seed(0)
a = torch.randn((512, 512), device='cuda', dtype=torch.float16)
b = torch.randn((512, 512), device='cuda', dtype=torch.float16)
triton_output = grouped_matmul(a, b, group_sz=32)
torch_output = torch.matmul(a, b)
if torch.allclose(triton_output, torch_output, atol=5e-2, rtol=0):
    print("âœ… Triton and Torch match")
else:
    print("âŒ Triton and Torch differ")


# Benchmarking
Triton brings built-in benchmarking tools with it. Here's an example how to use it.
# adapted from https://triton-lang.org/main/getting-started/tutorials/01-vector-add.html
@triton.testing.perf_report(
    triton.testing.Benchmark(
        x_names=['square_matrix_size'],  # Argument names to use as an x-axis for the plot.
        x_vals=[2**i for i in range(5, 12, 1)],  # Different possible values for `x_name`.
        x_log=True,  # x axis is logarithmic.
        line_arg='provider',  # Argument name whose value corresponds to a different line in the plot.
        line_vals=['naive', 'grouped', 'torch'],  # Possible values for `line_arg`.
        line_names=['Naive', 'Grouped', 'Torch'],  # Label name for the lines.
        styles=[('blue', '-'), ('green', '-'), ('orange','-')],  # Line styles.
        ylabel='GB/s',  # Label name for the y-axis.
        plot_name='matmul-performance',  # Name for the plot. Used also as a file name for saving the plot.
        args={},  # Values for function arguments not in `x_names` and `y_name`.
    ))
def benchmark(square_matrix_size, provider):
    sz = square_matrix_size
    a = torch.rand((sz, sz), device='cuda', dtype=torch.float32)
    b = torch.rand((sz, sz), device='cuda', dtype=torch.float32)
    quantiles = [0.5, 0.2, 0.8]
    if provider == 'naive':   ms, min_ms, max_ms = triton.testing.do_bench(lambda: naive_matmul(a, b), quantiles=quantiles)
    if provider == 'grouped': ms, min_ms, max_ms = triton.testing.do_bench(lambda: grouped_matmul(a, b, group_sz=8), quantiles=quantiles)
    if provider == 'torch':   ms, min_ms, max_ms = triton.testing.do_bench(lambda: torch.matmul(a,b), quantiles=quantiles)
    gbps = lambda ms: 12 * sz / ms * 1e-6
    return gbps(ms), gbps(max_ms), gbps(min_ms)

benchmark.run(print_data=True, show_plots=True)
_Note Umer: I would've expected the GB/s to increase as the matrix sizes get larger. Why don't they? Maybe because share memory is full, so kernel spends more and more time reloading stuff_

Let's try different block sizes:
@triton.testing.perf_report(
    triton.testing.Benchmark(
        x_names=['batch_size'], x_vals=[2**i for i in range(4, 7, 1)], x_log=True,
        line_arg='provider', line_vals=['naive', 'grouped', 'torch'], line_names=['Naive', 'Grouped', 'Torch'],
        styles=[('blue', '-'), ('green', '-'), ('orange','-')],
        ylabel='GB/s', plot_name='matmul-performance', args={}
    ))
def benchmark(batch_size, provider):
    sz = 512
    a = torch.rand((sz, sz), device='cuda', dtype=torch.float32)
    b = torch.rand((sz, sz), device='cuda', dtype=torch.float32)
    quantiles = [0.5, 0.2, 0.8]
    if provider == 'naive':   ms, min_ms, max_ms = triton.testing.do_bench(lambda: naive_matmul(a, b, bs=batch_size), quantiles=quantiles)
    if provider == 'grouped': ms, min_ms, max_ms = triton.testing.do_bench(lambda: grouped_matmul(a, b, bs=batch_size, group_sz=8), quantiles=quantiles)
    if provider == 'torch':   ms, min_ms, max_ms = triton.testing.do_bench(lambda: torch.matmul(a,b), quantiles=quantiles)
    gbps = lambda ms: 12 * sz / ms * 1e-6
    return gbps(ms), gbps(max_ms), gbps(min_ms)

benchmark.run(print_data=True, show_plots=True)
Larger block sizes seem to be better. Let's compare with pytorch again, using larger block sizes.
@triton.testing.perf_report(
    triton.testing.Benchmark(
        x_names=['square_matrix_size'], x_vals=[2**i for i in range(5, 12, 1)], x_log=True,
        line_arg='provider', line_vals=['naive', 'grouped', 'torch'], line_names=['Naive', 'Grouped', 'Torch'],
        styles=[('blue', '-'), ('green', '-'), ('orange','-')],
        ylabel='GB/s', plot_name='matmul-performance', args={}
    ))
def benchmark(square_matrix_size, provider):
    sz = square_matrix_size
    a = torch.rand((sz, sz), device='cuda', dtype=torch.float32)
    b = torch.rand((sz, sz), device='cuda', dtype=torch.float32)
    quantiles = [0.5, 0.2, 0.8]
    if provider == 'naive':   ms, min_ms, max_ms = triton.testing.do_bench(lambda: naive_matmul(a, b, bs=64), quantiles=quantiles)
    if provider == 'grouped': ms, min_ms, max_ms = triton.testing.do_bench(lambda: grouped_matmul(a, b, group_sz=8, bs=64), quantiles=quantiles)
    if provider == 'torch':   ms, min_ms, max_ms = triton.testing.do_bench(lambda: torch.matmul(a,b), quantiles=quantiles)
    gbps = lambda ms: 12 * sz / ms * 1e-6
    return gbps(ms), gbps(max_ms), gbps(min_ms)

benchmark.run(print_data=True, show_plots=True)
This reduces the performance difference to pytorch for larger matrix sizes, but pytorch is still better.

Tip: For profiling, we can use Nsight Compute to profile our kernels:
`ncu --target-processes all your_python_file.py`

# Auto-Tuning
Adapted from https://triton-lang.org/main/getting-started/tutorials/03-matrix-multiplication.html
The choice of meta-parameters (e.g. block sizes) and compilation options (e.g. `num_warps`) impacts the kernel speed. Triton allows you to pass a list of possible choices, runs them all, and then compiles the kernel for the fastest choice. This is called `Auto-Tuning`.

If the size of your problem changes (e.g. when matrix changes size), a new auto-tune will be done for the new problem size.
@triton.autotune(
    # Choices of configs to auto-tune over
    configs=[
        triton.Config({'bm': 128, 'bn': 256, 'bk': 64, 'group_sz': 8}, num_stages=3, num_warps=8),
        triton.Config({'bm': 64, 'bn': 256, 'bk': 32, 'group_sz': 8}, num_stages=4, num_warps=4),
        triton.Config({'bm': 128, 'bn': 128, 'bk': 32, 'group_sz': 8}, num_stages=4, num_warps=4),
        triton.Config({'bm': 128, 'bn': 64, 'bk': 32, 'group_sz': 8}, num_stages=4, num_warps=4),
        triton.Config({'bm': 64, 'bn': 128, 'bk': 32, 'group_sz': 8}, num_stages=4, num_warps=4),
        triton.Config({'bm': 128, 'bn': 32, 'bk': 32, 'group_sz': 8}, num_stages=4, num_warps=4),
        triton.Config({'bm': 64, 'bn': 32, 'bk': 32, 'group_sz': 8}, num_stages=5, num_warps=2),
        triton.Config({'bm': 32, 'bn': 64, 'bk': 32, 'group_sz': 8}, num_stages=5, num_warps=2),
    ],
    # Definition of problem size. If it changes, a new auto-tune is run for the new problem size.
    key=['m', 'n', 'k'],
)
@triton.jit
def grouped_autotuned_matmul_k(
    a_ptr, b_ptr, c_ptr,
    m, n, k,
    stride_am, stride_ak, 
    stride_bk, stride_bn,
    stride_cm, stride_cn,
    bm: tl.constexpr, bn: tl.constexpr, bk: tl.constexpr, group_sz: tl.constexpr
):
    pid_m = tl.program_id(0)
    pid_n = tl.program_id(1)
    num_pid_m = tl.num_programs(0)
    num_pid_n = tl.num_programs(1)
    # determine location of block in grouped ordering
    pid_m, pid_n = tl.swizzle2d(pid_m, pid_n, num_pid_m, num_pid_n, group_sz)  # Weirdness: tl.swizzle2d doesn't work when simulating on CPU
    # chunks along m/n/k dimensions
    rm = get_1d_offset(size=bm, n_prev_chunks=pid_m)
    rn = get_1d_offset(size=bn, n_prev_chunks=pid_n)
    rk = get_1d_offset(size=bk, n_prev_chunks=0)
    # relevant offsets of a, b
    offs_a = a_ptr + get_2d_offset(rm, rk, stride_am, stride_ak)
    offs_b = b_ptr + get_2d_offset(rk, rn, stride_bk, stride_bn)
    # initialize and iteratively update accumulator
    acc = tl.zeros((bm, bn), dtype=tl.float32)
    for _ in range(0, k, bk):
        # todo umer: don't we need mask when loading a & b?
        a = tl.load(offs_a)
        b = tl.load(offs_b)
        acc += tl.dot(a, b, allow_tf32=False) # block level matrix multiplication ; Weirdness: allow_tf32 must be set to False for older GPUs, otherwise won't compile
        # increase offets, so next iteration loads next chunks
        offs_a += bk * stride_ak
        offs_b += bk * stride_bk
    c = c_ptr + get_2d_offset(rm, rn, stride_cm, stride_cn)
    mask = get_2d_mask(rm, rn, m, n)
    tl.store(c, acc, mask=mask)
def grouped_autotuned_matmul(a, b):
    matmul_k_fn = grouped_autotuned_matmul_k
    
    assert a.shape[1] == b.shape[0], "matrix dims not compatible for matmul"
    check_tensors_gpu_ready(a, b)
    (m, k), (_, n) = a.shape, b.shape
    c = torch.empty((m, n), device=a.device, dtype=torch.float16)
    grid = lambda meta: (triton.cdiv(m, meta['bm']),  triton.cdiv(n, meta['bn']))
    matmul_k_fn[grid](
        a, b, c,
        m, n, k,
        a.stride(0), a.stride(1),
        b.stride(0), b.stride(1),
        c.stride(0), c.stride(1),
        # bm=bs, bn=bs, bk=bs, <- will be autotuned
        # **group_sz <- will be autotuned
    )
    return c

a,b = torch.ones(3,4, device='cuda'), torch.ones(4,5, device='cuda')
a@b
_Note: sometimes the following line returns wrong results, and I can't reliably reproduce it. If you can, please tell me via Twitter (@UmerHAdil) ! ğŸ™ğŸ½_
grouped_autotuned_matmul(a,b)

For tips, tricks and heuristics which configs to try for auto-tuning, see [Mark Saroufim's talk "CUDA Performance Checklist"](https://www.youtube.com/watch?v=SGhfUhlowB4). Much of it should apply to Triton as well.

Let's run the benchmark once again. This will take a lot of time, as we auto-tune for each benchmarking paramater choice (i.e., 12-5=7 times for us).
@triton.testing.perf_report(
    triton.testing.Benchmark(
        x_names=['square_matrix_size'], x_vals=[2**i for i in range(5, 12, 1)], x_log=True,
        line_arg='provider', line_vals=['naive', 'grouped', 'grouped-autotuned', 'torch'], line_names=['Naive', 'Grouped', 'Grouped & Auto-Tuned','Torch'],
        styles=[('blue', '-'), ('green', '-'), ('green', '--'), ('orange','-')],
        ylabel='GB/s', plot_name='matmul-performance', args={}
    ))
def benchmark(square_matrix_size, provider):
    sz = square_matrix_size
    a = torch.rand((sz, sz), device='cuda', dtype=torch.float32)
    b = torch.rand((sz, sz), device='cuda', dtype=torch.float32)
    quantiles = [0.5, 0.2, 0.8]
    if provider == 'naive':   ms, min_ms, max_ms = triton.testing.do_bench(lambda: naive_matmul(a, b, bs=64), quantiles=quantiles)
    if provider == 'grouped': ms, min_ms, max_ms = triton.testing.do_bench(lambda: grouped_matmul(a, b, group_sz=8, bs=64), quantiles=quantiles)
    if provider == 'grouped-autotuned': ms, min_ms, max_ms = triton.testing.do_bench(lambda: grouped_autotuned_matmul(a, b), quantiles=quantiles)
    if provider == 'torch':   ms, min_ms, max_ms = triton.testing.do_bench(lambda: torch.matmul(a,b), quantiles=quantiles)
    gbps = lambda ms: 12 * sz / ms * 1e-6
    return gbps(ms), gbps(max_ms), gbps(min_ms)

benchmark.run(print_data=True, show_plots=True)



___
<h1>That's it! Congrats on making it through the tutorial - good work! ğŸ¥³</h1>

I highly encourage you to write a few triton kernels yourself. You can e.g. try these triton puzzles: https://github.com/srush/Triton-Puzzles by [Sasha Rush](https://twitter.com/srush_nlp), Tejas Ramesh and [Keren Zhou](https://twitter.com/ZhouKeren).

Here is other intermediate and advanced material:
- The official documentation: https://triton-lang.org/
- The LightLLM repo has a ton of real-world triton kernels: https://github.com/ModelTC/lightllm/tree/main/lightllm/common/basemodel/triton_kernel
- So does the Unsloth repo: https://github.com/unslothai/unsloth/tree/main/unsloth/kernels
If you're generally interested in GPU programming and performance, the [cuda mode Discord](https://discord.gg/cudamode) may be interesting to you. This tutorial was written as part of their amazing [lecture series](https://www.youtube.com/@CUDAMODE).

___
**About the author:**
Hey ğŸ‘‹ğŸ½ I'm Umer from Germany. Thanks for reading this tutorial. I hope you got learned a lot from it. If you have any questions, feel free to shoot me a message on Twitter ([@UmerHAdil](https://x.com/UmerHAdil)).

As I currently do Open-Source AI work as an independent ML engineer, I have set up a ko-fi page for tips & donations: https://ko-fi.com/umerha.
Apart from this guide, I've contributed to HuggingFace diffusers (e.g. [shoutouts by HF](https://x.com/RisingSayak/status/1773739194474463629)), LangChain [shoutouts by the team](https://twitter.com/search?lang=de&q=(from%3ALangChainAI)%20(%40UmerHAdil)%20lang%3Aen&src=typed_query)), and gpt-engineer (e.g. [this](https://x.com/UmerHAdil/status/1715447656527339668)).
If you're a company in need of Triton and/or CUDA consulting, also shoot me a message on Twitter ([@UmerHAdil](https://x.com/UmerHAdil)).
