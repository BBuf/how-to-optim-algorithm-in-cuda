> æˆ‘çš„è¯¾ç¨‹ç¬”è®°ï¼Œæ¬¢è¿å…³æ³¨ï¼šhttps://github.com/BBuf/how-to-optim-algorithm-in-cuda/tree/master/cuda-mode 

> CUDA-MODE Lecture 15æ˜¯è®²cutlassçš„cute LayoutæŠ½è±¡çš„ï¼Œæ„Ÿè§‰è®²çš„æ¯”è¾ƒå·®ï¼Œå»ºè®®å¤§å®¶ç›´æ¥çœ‹reedå¤§ä½¬çš„cutlassç³»åˆ—åšå®¢ä»‹ç»ï¼Œæ¥ä¸‹æ¥ä¼šå¿½ç•¥æ‰è¿™èŠ‚è¯¾çš„ç¬”è®°ã€‚CUDA-MODE Lecture 16: On Hands profilingæ˜¯ä¸€ä¸ªå…³äºPyTorch Lightingçš„å·¥ç¨‹å¸ˆæ ¹æ®ä¸€ä¸ªå®é™…çš„gemmaæ¨¡å‹å¾®è°ƒçš„ç¨‹åºæ¥è¿›è¡Œprofileå’Œæ”¹è¿›æ€§èƒ½çš„è¯¾ç¨‹ï¼Œè¿™èŠ‚è¯¾æ²¡æœ‰Slidesæ›´è´´è¿‘AI Infraå·¥ç¨‹å¸ˆçš„ç”Ÿæ´»ï¼Œprofileå·¥å…·ä½¿ç”¨äº†Nsight Systemå’ŒPyTorch Profilerï¼Œå¯¹è¿™èŠ‚è¯¾æ„Ÿå…´è¶£çš„å°ä¼™ä¼´å¯ä»¥è‡ªè¡ŒæŸ¥é˜…è¿™ä¸ªè¯¾ç¨‹ï¼Œç”±äºæ²¡æœ‰Slideså¹¶ä¸”è®²å¾—å¾ˆéšæ„æ‰€ä»¥ç¬”è€…ä¹Ÿä¸æ‰“ç®—è®°å½•è¿™èŠ‚è¯¾çš„ç¬”è®°ã€‚ä½†å¦‚æœä½ å¹³æ—¶æœ‰åšProfileçš„éœ€æ±‚ï¼Œæˆ‘è¿˜æ˜¯å»ºè®®çœ‹ä¸€ä¸‹è¿™èŠ‚è¯¾ã€‚

> ä¸‹é¢çš„è¯¾ç¨‹ç¬”è®°çš„å†…å®¹ä¸»è¦æ¥æºæ˜¯ Lecture 14 Triton å®è·µæŒ‡å—ä¸­çš„ https://github.com/gpu-mode/lectures/blob/main/lecture_014/A_Practitioners_Guide_to_Triton.ipynb 

# ç¬¬14è¯¾ï¼ŒTriton å®è·µæŒ‡å—

<h1><b></v>Triton å®è·µæŒ‡å—</b></h1>

ä½œè€…ï¼šUmerHA (https://x.com/UmerHAdil // https://github.com/UmerHA/)ï¼Œä¸º cuda-mode å°ç»„ç¼–å†™ â¤ï¸ May our brrrr level reach over ã€‚

# ä¸ºä»€ä¹ˆä»¥åŠä½•æ—¶ä½¿ç”¨Triton

**ä»€ä¹ˆæ˜¯Triton**
ç®€è€Œè¨€ä¹‹ï¼šTritonæ˜¯ä¸€ç§æ›´æ–¹ä¾¿åœ°ç¼–ç¨‹GPUçš„è¯­è¨€ã€‚ä½ ç¼–å†™ç±»ä¼¼Pythonçš„ä»£ç ï¼Œç„¶åè¿™äº›ä»£ç è¢«ç¼–è¯‘æˆptxä»£ç ï¼ˆä¸cudaä»£ç ç¼–è¯‘æˆçš„ä»£ç ç›¸åŒï¼‰ã€‚

åœ¨ç¼–è¯‘è¿‡ç¨‹ä¸­ï¼ŒTritonç¼–è¯‘å™¨å°è¯•ä½¿ç”¨å·§å¦™çš„æŠ€å·§æ¥é‡æ–°æ’åˆ—ç¨‹åºçš„éƒ¨åˆ†å†…å®¹ï¼ˆä¸æ”¹å˜ç¨‹åºçš„æ„ä¹‰ï¼ï¼‰ä»¥ä½¿å…¶è¿è¡Œå¾—æ›´å¿«ã€‚

**Triton vs Cuda**

![](https://files.mdnice.com/user/59/dda26356-ad90-415d-8cbe-5b5b4bad930d.png)
source: https://zhuanlan.zhihu.com/p/672086654

CUDA æ˜¯ä¸€ä¸ªé«˜ç«¯å·¥å…·ï¼Œæœ‰è®¸å¤šè®¾ç½®ä¾›ä¸“ä¸šäººå£«ä½¿ç”¨ã€‚
- å¯¹æ‰€æœ‰å†…å®¹æœ‰å®Œå…¨æ§åˆ¶ï¼Œå› æ­¤å¯ä»¥å®ç°ç»å¯¹æœ€å¤§æ€§èƒ½
- æ›´éš¾è·å¾—è‰¯å¥½çš„æ€§èƒ½
- ç¼–å†™å’Œè°ƒè¯•æ›´åŠ ç¹ç
- æ›´å¤æ‚ï¼Œå› æ­¤æ›´éš¾å­¦ä¹ 

Triton æ˜¯ä¸€ä¸ªéå¸¸é€‚åˆå¤§å¤šæ•°ç”¨æˆ·çš„å·¥å…·
- ä½ ä¸èƒ½æ§åˆ¶æ‰€æœ‰å†…å®¹ï¼Œå› ä¸ºæœ‰äº›äº‹æƒ…ç•™ç»™è‡ªåŠ¨ä¼˜åŒ–ï¼›æ‰€ä»¥ä½ å¯èƒ½ä¸ä¼šè·å¾—ç»å¯¹æœ€å¤§æ€§èƒ½
- æ›´å®¹æ˜“è·å¾—è‰¯å¥½çš„æ€§èƒ½
- æ›´å®¹æ˜“ç¼–å†™å’Œè°ƒè¯•
- æ›´å®¹æ˜“å­¦ä¹ ï¼Œå› ä¸ºå®ƒå…·æœ‰ç±»ä¼¼ Python çš„è¯­æ³•

**Triton vs torch.compile**

`torch.compile` é€šè¿‡å°è¯•æ›´æœ‰æ•ˆåœ°ä½¿ç”¨ç°æœ‰kernelå¹¶åˆ›å»ºç®€å•çš„æ–°kernelæ¥ä½¿ä½ çš„æ¨¡å‹æ›´å¿«ã€‚è¿™å¯èƒ½ä¼šä½¿ä½ çš„æ¨¡å‹è¶³å¤Ÿå¿«ã€‚å¦‚æœæ²¡æœ‰ï¼Œä½ å¯ä»¥å†³å®šæŠ•å…¥æ—¶é—´ç¼–å†™æ›´å¿«çš„ Triton kernelã€‚

ï¼ˆ`torch.compile` åˆ›å»ºçš„è¿™äº›ç®€å•æ–°kernelå®é™…ä¸Šæ˜¯ Triton kernelã€‚å› æ­¤ï¼Œå®ƒä»¬æ˜¯è‡ªå®šä¹‰kernelçš„è‰¯å¥½èµ·ç‚¹ã€‚å‚è§ [Mark Saroufim](https://twitter.com/marksaroufim) çš„ [cuda mode ç¬¬ä¸€è®²](https://www.youtube.com/watch?v=LuhJEEJQgUM&t=2200s) äº†è§£å¦‚ä½•æ“ä½œã€‚ï¼‰

**ä½•æ—¶ä½¿ç”¨ Triton**

ä½ ä»ä½ çš„ AI æ¨¡å‹å¼€å§‹ã€‚
1. å¦‚æœå®ƒä¸å¤Ÿå¿«ï¼Œä½¿ç”¨ `torch.compile`ã€‚
2. å¦‚æœå®ƒä¸å¤Ÿå¿«ï¼Œæ£€æŸ¥ä½ æ˜¯å¦å¯ä»¥é‡å†™ä»£ç ä»¥ä½¿å…¶æ›´é€‚åˆ `torch.compile`ã€‚
3. å¦‚æœå®ƒä¸å¤Ÿå¿«ï¼Œæ£€æŸ¥å“ªäº›éƒ¨åˆ†æ…¢å¹¶ä¸ºå…¶ç¼–å†™è‡ªå®šä¹‰ Triton kernelã€‚
4. å¦‚æœå®ƒä¸å¤Ÿå¿«ï¼Œæ£€æŸ¥å“ªäº›éƒ¨åˆ†æ…¢å¹¶ä¸ºå…¶ç¼–å†™è‡ªå®šä¹‰ CUDA kernelã€‚

ï¼ˆåœ¨ä¸å¤ªå¯èƒ½çš„æƒ…å†µä¸‹ï¼Œå¦‚æœä½ äº‹å…ˆçŸ¥é“ä½ éœ€è¦ç»å¯¹æœ€å¤§æ€§èƒ½ï¼Œä½ å¯ä»¥å†³å®šç›´æ¥ä» CUDA å¼€å§‹ã€‚ï¼‰

**å…³äºç²—ç³™è¾¹ç¼˜çš„è¯´æ˜**

ç”±äº Triton æ˜¯ä¸€ä¸ªè¾ƒæ–°çš„é¡¹ç›®ï¼Œäººä»¬å‘ç°å®ƒæœ‰ä¸€äº›ç²—ç³™çš„è¾¹ç¼˜ã€‚æˆ‘å·²ç»è®°å½•äº†æˆ‘é‡åˆ°çš„æ‰€æœ‰ç²—ç³™è¾¹ç¼˜ï¼Œå¹¶åœ¨è¯„è®ºä¸­æ³¨æ˜äº†â€œWeirdness: <æˆ‘å¯¹å¥‡æ€ªä¹‹å¤„çš„æè¿°>â€ã€‚

æˆ‘é¢„è®¡å®ƒä¼šåœ¨æœªæ¥å˜å¾—æ›´åŠ å®Œå–„ã€‚

# å¦‚ä½•ç¼–å†™Triton kernel

ä¸CUDAä¸åŒï¼Œå¦‚æœæˆ‘ä»¬è®¾ç½®ç¯å¢ƒå˜é‡ `TRITON_INTERPRET = 1`ï¼Œæˆ‘ä»¬å¯ä»¥åƒè°ƒè¯•ä»»ä½•CPUç¨‹åºä¸€æ ·è°ƒè¯•Triton kernelã€‚ç„¶åTritonåœ¨CPUä¸Šè¿è¡Œï¼Œä½†æ¨¡æ‹Ÿå®ƒåœ¨GPUä¸Šè¿è¡Œã€‚

æˆ‘å»ºè®®é¦–å…ˆåœ¨æ¨¡æ‹Ÿå™¨ä¸­ç¼–å†™æ‰€æœ‰ç¨‹åºï¼Œå¹¶æ£€æŸ¥å…¶æ­£ç¡®æ€§ã€‚å¦‚æœæ­£ç¡®ï¼Œç„¶åä½ å¯ä»¥ä½¿å…¶å¿«é€Ÿè¿è¡Œã€‚

ä»¥ä¸‹æ˜¯ä¸€äº›ç”¨äºè°ƒè¯•çš„å®ç”¨å‡½æ•°ï¼š
- `check_tensors_gpu_ready`ï¼š(i) æ–­è¨€æ‰€æœ‰å¼ é‡åœ¨å†…å­˜ä¸­æ˜¯è¿ç»­çš„ï¼Œ(ii) ä»…åœ¨éæ¨¡æ‹Ÿæƒ…å†µä¸‹ï¼Œæ–­è¨€æ‰€æœ‰å¼ é‡åœ¨GPUä¸Š
- `breakpoint_if`ï¼šæ ¹æ®pidsçš„æ¡ä»¶è®¾ç½®æ–­ç‚¹
- `print_if`ï¼šæ ¹æ®pidsçš„æ¡ä»¶æ‰“å°å†…å®¹

```python
import os
from IPython.core.debugger import set_trace

os.environ['TRITON_INTERPRET'] = '1' # needs to be set *before* triton is imported

def check_tensors_gpu_ready(*tensors):
    """æ£€æŸ¥æ‰€æœ‰å¼ é‡æ˜¯å¦åœ¨GPUä¸Šå¹¶ä¸”æ˜¯è¿ç»­çš„"""
    for t in tensors:
        assert t.is_contiguous, "A tensor is not contiguous"  # æ–­è¨€å¼ é‡æ˜¯è¿ç»­çš„
        if not os.environ.get('TRITON_INTERPRET') == '1': assert t.is_cuda, "A tensor is not on cuda"  # å¦‚æœä¸æ˜¯æ¨¡æ‹Ÿæ¨¡å¼ï¼Œæ–­è¨€å¼ é‡åœ¨GPUä¸Š

def test_pid_conds(conds, pid_0=[0], pid_1=[0], pid_2=[0]):
    """æµ‹è¯•pidæ¡ä»¶æ˜¯å¦æ»¡è¶³
    ä¾‹å¦‚:
        '=0'  æ£€æŸ¥pid_0 == 0
        ',>1' æ£€æŸ¥pid_1 > 1
        '>1,=0' æ£€æŸ¥pid_0 > 1 ä¸” pid_1 == 0
    """
    pids = pid_0[0], pid_1[0], pid_2[0]  # è·å–pidå€¼
    conds = conds.replace(' ','').split(',')  # å»é™¤ç©ºæ ¼å¹¶åˆ†å‰²æ¡ä»¶
    for i, (cond, pid) in enumerate(zip(conds, pids)):
        if cond=='': continue  # å¦‚æœæ¡ä»¶ä¸ºç©ºï¼Œè·³è¿‡
        op, threshold = cond[0], int(cond[1:])  # è·å–æ“ä½œç¬¦å’Œé˜ˆå€¼
        if op not in ['<','>','>=','<=','=', '!=']: raise ValueError(f"Rules may only use these ops: '<','>','>=','<=','=', '!='. Invalid rule: '{condition}'.")  # æ£€æŸ¥æ“ä½œç¬¦æ˜¯å¦åˆæ³•
        op = '==' if op == '=' else op  # å°†'='æ›¿æ¢ä¸º'=='
        if not eval(f'{pid} {op} {threshold}'): return False  # è¯„ä¼°æ¡ä»¶æ˜¯å¦æ»¡è¶³
    return True

assert test_pid_conds('')  # æµ‹è¯•ç©ºæ¡ä»¶
assert test_pid_conds('>0', [1], [1])  # æµ‹è¯•pid_0 > 0
assert not test_pid_conds('>0', [0], [1])  # æµ‹è¯•pid_0 > 0ä¸æ»¡è¶³
assert test_pid_conds('=0,=1', [0], [1], [0])  # æµ‹è¯•pid_0 = 0 ä¸” pid_1 = 1

def breakpoint_if(conds, pid_0=[0], pid_1=[0], pid_2=[0]):
    """å¦‚æœä»»ä½•pidæ¡ä»¶æ»¡è¶³ï¼Œåœæ­¢kernel"""
    if test_pid_conds(conds, pid_0, pid_1, pid_2): set_trace()  # å¦‚æœæ¡ä»¶æ»¡è¶³ï¼Œè®¾ç½®æ–­ç‚¹

def print_if(txt, conds, pid_0=[0], pid_1=[0], pid_2=[0]):
    """å¦‚æœä»»ä½•pidæ¡ä»¶æ»¡è¶³ï¼Œæ‰“å°txt"""
    if test_pid_conds(conds, pid_0, pid_1, pid_2): print(txt)  # å¦‚æœæ¡ä»¶æ»¡è¶³ï¼Œæ‰“å°æ–‡æœ¬

def cdiv(a,b): 
    """è®¡ç®—aé™¤ä»¥bçš„ä¸Šé™å€¼"""
    return (a + b - 1) // b  # è®¡ç®—aé™¤ä»¥bçš„ä¸Šé™å€¼
assert cdiv(10,2)==5  # æµ‹è¯•cdivå‡½æ•°
assert cdiv(10,3)==4  # æµ‹è¯•cdivå‡½æ•°
```

```python
import torch
import triton
import triton.language as tl
```

# ç¼–ç¨‹æ¨¡å‹

åœ¨CUDAä¸­ï¼Œæˆ‘ä»¬å°†è®¡ç®—åˆ†è§£ä¸ºä¸¤ä¸ªå±‚æ¬¡ï¼šé¦–å…ˆæ˜¯å—ï¼Œç„¶åæ¯ä¸ªå—è¿›ä¸€æ­¥åˆ†è§£ä¸ºçº¿ç¨‹ã€‚ä¸€ä¸ªå—ä¸­çš„æ‰€æœ‰çº¿ç¨‹è¿è¡Œåœ¨åŒä¸€ä¸ªSMä¸Šï¼Œå¹¶å…±äº«ç›¸åŒçš„å…±äº«å†…å­˜ã€‚æ¯ä¸ªçº¿ç¨‹è®¡ç®—**æ ‡é‡**ã€‚

åœ¨Tritonä¸­ï¼Œæˆ‘ä»¬åªå°†è®¡ç®—åˆ†è§£ä¸ºä¸€ä¸ªå±‚æ¬¡ï¼šå—ã€‚æ²¡æœ‰è¿›ä¸€æ­¥çš„çº¿ç¨‹åˆ†è§£ã€‚**Tritonè¦æ±‚æˆ‘ä»¬å¯¹å‘é‡è¿›è¡Œæ“ä½œ**ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬ä¸éœ€è¦ä¹Ÿä¸èƒ½ç®¡ç†å…±äº«å†…å­˜ã€‚Tritonä¼šè‡ªåŠ¨å¤„ç†è¿™äº›ã€‚

ç¤ºä¾‹ï¼š

å‡è®¾æˆ‘ä»¬è¦å°†å¤§å°ä¸º8çš„å‘é‡`x`å’Œ`y`ç›¸åŠ ï¼Œå¹¶å°†è¾“å‡ºä¿å­˜åˆ°å¤§å°ä¹Ÿä¸º8çš„å‘é‡`z`ä¸­ã€‚æˆ‘ä»¬ä½¿ç”¨å¤§å°ä¸º4çš„å—ï¼Œå› æ­¤æˆ‘ä»¬æœ‰`8 / 4 = 2`ä¸ªå—ã€‚
- CUDAè¿è¡Œ2ä¸ªå—ï¼Œæ¯ä¸ªå—æœ‰4ä¸ªçº¿ç¨‹ã€‚8ä¸ªçº¿ç¨‹ä¸­çš„æ¯ä¸€ä¸ªè®¡ç®—ä¸€ä¸ªå•ç‹¬çš„ä½ç½®ï¼Œä¾‹å¦‚`z[0] = x[0] + y[0]`
- Tritonä¹Ÿè¿è¡Œ2ä¸ªå—ï¼Œæ¯ä¸ªå—æ‰§è¡Œå‘é‡åŒ–åŠ æ³•ã€‚å‘é‡çš„å¤§å°æ˜¯å—çš„å¤§å°ï¼Œå³4ã€‚ä¾‹å¦‚`z[0:3] = x[0:3] + y[0:3]`

**æ‰€æœ‰**Triton kernelä¸­çš„æ“ä½œéƒ½æ˜¯å‘é‡åŒ–çš„ï¼šåŠ è½½æ•°æ®ã€æ“ä½œæ•°æ®ã€å­˜å‚¨æ•°æ®å’Œåˆ›å»ºæ©ç ã€‚

è®©æˆ‘ä»¬è€ƒè™‘å¦ä¸€ä¸ªç®€å•çš„ä¾‹å­ï¼š

åŒæ ·ï¼Œæˆ‘ä»¬è¦å°†å¤§å°ä¸º**6**çš„å‘é‡`x`å’Œ`y`ç›¸åŠ ï¼Œå¹¶å°†è¾“å‡ºä¿å­˜åˆ°å¤§å°ä¹Ÿä¸º6çš„å‘é‡`z`ä¸­ã€‚æˆ‘ä»¬ä½¿ç”¨å¤§å°ä¸º4çš„å—ï¼Œå› æ­¤æˆ‘ä»¬æœ‰`cdiv(6, 4) = 2`ä¸ªå—ã€‚

```python
x = torch.tensor([1,2,3,4,5,6])
y = torch.tensor([0,1,0,1,0,1])

x, y, x+y
```

CUDA kernelå°†ç±»ä¼¼äºä»¥ä¸‹Cä»£ç ï¼š

```python
# x,y = è¾“å…¥å¼ é‡, z = è¾“å‡ºå¼ é‡, n = xçš„å¤§å°, bs = å—å¤§å°
def add_cuda_k(x, y, z, n, bs):
    # å®šä½æ­¤ç‰¹å®škernelæ­£åœ¨æ‰§è¡Œçš„æ•´ä½“è®¡ç®—çš„å“ªä¸€éƒ¨åˆ†
    block_id = ... # åœ¨æˆ‘ä»¬çš„ä¾‹å­ä¸­: æ˜¯[0,1]ä¸­çš„ä¸€ä¸ª
    thread_id = ... # åœ¨æˆ‘ä»¬çš„ä¾‹å­ä¸­: æ˜¯[0,1,2,3]ä¸­çš„ä¸€ä¸ª

    # è¯†åˆ«æ­¤ç‰¹å®škerneléœ€è¦çš„æ•°æ®ä½ç½®
    offs = block_id * bs + thread_id
    
    # ä¿æŠ¤å­å¥, ç¡®ä¿æˆ‘ä»¬ä¸ä¼šè¶Šç•Œ
    if offs < n:

        # è¯»å–æ•°æ®
        x_value = x[offs]
        y_value = y[offs]
        
        # æ‰§è¡Œæ“ä½œ
        z_value = x_value + y_value
        
        # å†™å…¥æ•°æ®
        z[offs] = z_value

    # é‡è¦: offs, x_value, y_value, x_value éƒ½æ˜¯æ ‡é‡!
    # ä¿æŠ¤æ¡ä»¶ä¹Ÿæ˜¯ä¸€ç§æ ‡é‡, å› ä¸ºå®ƒæ£€æŸ¥ä¸€ä¸ªå€¼ä¸Šçš„ä¸€ä¸ªæ¡ä»¶ã€‚
```

ä¸ºäº†è¯´æ˜ï¼Œè¿™é‡Œæ˜¯æ¯ä¸ªkernelçš„å˜é‡ï¼š

![](https://files.mdnice.com/user/59/29863a80-c092-4d8d-a7de-0ecdb2a9213c.png)

ç°åœ¨è®©æˆ‘ä»¬çœ‹ä¸€ä¸‹ç›¸åº”çš„Triton kernelï¼Œå¤§è‡´å¦‚ä¸‹æ‰€ç¤ºï¼š

```python
# æ³¨æ„ï¼šè¿™æ˜¯ä¸ºäº†è¯´æ˜ï¼Œè¯­æ³•ä¸å®Œå…¨æ­£ç¡®ã€‚è¯·å‚è§ä¸‹æ–‡ä»¥è·å–æ­£ç¡®çš„Tritonè¯­æ³•

def add_triton_k(x, y, z, n, bs):
    # å®šä½æ­¤ç‰¹å®škernelæ­£åœ¨æ‰§è¡Œçš„æ•´ä½“è®¡ç®—çš„å“ªä¸€éƒ¨åˆ†
    block_id = tl.program_id(0)  # åœ¨æˆ‘ä»¬çš„ä¾‹å­ä¸­: æ˜¯[0,1]ä¸­çš„ä¸€ä¸ª
    
    # è¯†åˆ«æ­¤ç‰¹å®škerneléœ€è¦çš„æ•°æ®ä½ç½®
    offs = block_id * bs + tl.arange(0, bs) # <- è¿™æ˜¯ä¸€ä¸ªå‘é‡!
    
    # ä¿æŠ¤å­å¥å˜æˆä¸€ä¸ªæ©ç ï¼Œè¿™æ˜¯ä¸€ä¸ªå¸ƒå°”å‘é‡
    mask = offs < n # <- è¿™æ˜¯ä¸€ä¸ªå¸ƒå°”å‘é‡!
    
    # è¯»å–æ•°æ®
    x_values = x[offs] # <- è¯»å–ä¸€ä¸ªå‘é‡!
    y_values = y[offs] # <- è¯»å–ä¸€ä¸ªå‘é‡!
    
    # æ‰§è¡Œæ“ä½œ
    z_value = x_value + y_value  # <- å‘é‡ç›¸åŠ !
    
    # å†™å…¥æ•°æ®
    z[offs] = z_value  # <- å†™å…¥ä¸€ä¸ªå‘é‡!
```

å†æ¬¡è¯´æ˜ï¼Œè¿™é‡Œæ˜¯æ¯ä¸ªkernelçš„å˜é‡ï¼š

![](https://files.mdnice.com/user/59/8dd01a33-89cc-4587-8376-b2471600c63a.png)

æœ¯è¯­è¯´æ˜ï¼šåœ¨Tritonæœ¯è¯­ä¸­ï¼Œæ¯ä¸ªå¤„ç†å—çš„kernelè¢«ç§°ä¸ºâ€œprogramâ€ã€‚ä¹Ÿå°±æ˜¯è¯´ï¼Œæˆ‘ä»¬ä¸Šé¢çš„ä¾‹å­è¿è¡Œäº†2ä¸ªprogramã€‚å› æ­¤ï¼Œâ€œblock_idâ€é€šå¸¸è¢«ç§°ä¸ºâ€œpidâ€ï¼ˆâ€œprogram idâ€çš„ç¼©å†™ï¼‰ï¼Œä½†å®ƒä»¬æ˜¯ç›¸åŒçš„ã€‚

# ç¤ºä¾‹1: å¤åˆ¶å¼ é‡

è®©æˆ‘ä»¬çœ‹ä¸€äº›ä¾‹å­ã€‚ä¸ºäº†ä¿æŒç®€å•ï¼Œæˆ‘ä»¬å°†ä½¿ç”¨éå¸¸å°çš„å—å¤§å°ã€‚

ç›®æ ‡: ç»™å®šä¸€ä¸ªå½¢çŠ¶ä¸º (n) çš„å¼ é‡ `x`ï¼Œå°†å…¶å¤åˆ¶åˆ°å¦ä¸€ä¸ªå¼ é‡ `z` ä¸­ã€‚

```python
# # è¿™æ˜¯ä¸€ä¸ªæ™®é€šçš„Pythonå‡½æ•°ï¼Œç”¨äºå¯åŠ¨Triton kernel
def copy(x, bs, kernel_fn):
    z = torch.zeros_like(x)
    check_tensors_gpu_ready(x, z)
    n = x.numel()
    n_blocks = cdiv(n, bs)
    grid = (n_blocks,)  # æˆ‘ä»¬æœ‰å¤šå°‘ä¸ªå—ï¼Ÿå¯ä»¥æ˜¯1d/2d/3då…ƒç»„æˆ–è¿”å›1d/2d/3då…ƒç»„çš„å‡½æ•°

    # å¯åŠ¨ç½‘æ ¼ï¼
    # - kernel_fnæ˜¯æˆ‘ä»¬ä¸‹é¢ç¼–å†™çš„Triton kernel
    # - gridæ˜¯æˆ‘ä»¬ä¸Šé¢æ„å»ºçš„ç½‘æ ¼
    # - x,z,n,bsæ˜¯ä¼ é€’ç»™æ¯ä¸ªkernelå‡½æ•°çš„å‚æ•°
    kernel_fn[grid](x,z,n,bs)

    return z
```

**æ³¨æ„:** å‡ºäºæ•™è‚²ç›®çš„ï¼Œä¸‹é¢çš„kernelæœ‰ä¸€ä¸ªé€»è¾‘é”™è¯¯ï¼ˆä½†è¯­æ³•æ˜¯æ­£ç¡®çš„ï¼‰ã€‚ä½ èƒ½å‘ç°å®ƒå—ï¼Ÿ

```python
# # è¿™æ˜¯Triton kernel:

# triton.jitè£…é¥°å™¨å°†ä¸€ä¸ªPythonå‡½æ•°è½¬æ¢ä¸ºTriton kernelï¼Œè¯¥kernelåœ¨GPUä¸Šè¿è¡Œã€‚
# åœ¨è¿™ä¸ªå‡½æ•°å†…éƒ¨ï¼Œåªå…è®¸ä½¿ç”¨éƒ¨åˆ†Pythonæ“ä½œã€‚
# ä¾‹å¦‚ï¼Œå½“ä¸è¿›è¡Œæ¨¡æ‹Ÿæ—¶ï¼Œæˆ‘ä»¬ä¸èƒ½æ‰“å°æˆ–ä½¿ç”¨æ–­ç‚¹ï¼Œå› ä¸ºè¿™äº›åœ¨GPUä¸Šä¸å­˜åœ¨ã€‚
@triton.jit
# å½“æˆ‘ä»¬ä¼ é€’torchå¼ é‡æ—¶ï¼Œå®ƒä»¬ä¼šè‡ªåŠ¨è½¬æ¢ä¸ºæŒ‡å‘å…¶ç¬¬ä¸€ä¸ªå€¼çš„æŒ‡é’ˆ
# ä¾‹å¦‚ï¼Œä¸Šé¢æˆ‘ä»¬ä¼ é€’äº†xï¼Œä½†åœ¨è¿™é‡Œæˆ‘ä»¬æ¥æ”¶åˆ°x_ptr
def copy_k(x_ptr, z_ptr, n, bs: tl.constexpr):
    pid = tl.program_id(0)
    offs = tl.arange(0, bs)  # ä»pidè®¡ç®—åç§»é‡
    mask = offs < n
    x = tl.load(x_ptr + offs, mask) # åŠ è½½ä¸€ä¸ªå€¼å‘é‡ï¼Œå°†`x_ptr + offs`è§†ä¸º`x_ptr[offs]`
    tl.store(z_ptr + offs, x, mask) # å­˜å‚¨ä¸€ä¸ªå€¼å‘é‡

    print_if(f'pid = {pid} | offs = {offs}, mask = {mask}, x = {x}', '')

    # é—®é¢˜: è¿™ä¸ªkernelæœ‰ä»€ä¹ˆé—®é¢˜?
```

```python
z = copy(x, bs=2, kernel_fn=copy_k)
```

```python
pid = [0] | offs = [0 1], mask = [ True  True], x = [1 2]
pid = [1] | offs = [0 1], mask = [ True  True], x = [1 2]
pid = [2] | offs = [0 1], mask = [ True  True], x = [1 2]
```

```
z
```

```shell
tensor([1, 2, 0, 0, 0, 0])
```

æˆ‘ä»¬æ²¡æœ‰æ­£ç¡®åœ°ç§»åŠ¨åç§»é‡ã€‚æˆ‘ä»¬æ€»æ˜¯ä½¿ç”¨ offsets = [0,1]ï¼Œä½†å®ƒä»¬åº”è¯¥éšç€ pid å˜åŒ–ã€‚

```python
@triton.jit
def copy_k(x_ptr, z_ptr, n, bs: tl.constexpr):
    pid = tl.program_id(0)
    offs = pid * n + tl.arange(0, bs)
    mask = offs < n
    x = tl.load(x_ptr + offs, mask)
    tl.store(z_ptr + offs, x, mask)
    print_if(f'pid = {pid} | offs = {offs}, mask = {mask}, x = {x}', '')
```

```
z = copy(x, bs=2, kernel_fn=copy_k)
```

```python
pid = [0] | offs = [0 1], mask = [ True  True], x = [1 2]
pid = [1] | offs = [6 7], mask = [False False], x = [1 1]
pid = [2] | offs = [12 13], mask = [False False], x = [1 1]
```

ä¸å®Œå…¨æ­£ç¡®ã€‚æˆ‘ä»¬æ·»åŠ äº† `pid * n`ï¼Œä½†æƒ³è¦æ·»åŠ  `pid * bs`

```python
@triton.jit
def copy_k(x_ptr, z_ptr, n, bs: tl.constexpr):
    pid = tl.program_id(0)
    offs = pid * bs + tl.arange(0, bs)
    mask = offs < n
    x = tl.load(x_ptr + offs, mask)
    tl.store(z_ptr + offs, x, mask)
    print_if(f'pid = {pid} | offs = {offs}, mask = {mask}, x = {x}', '')
```

```python
z = copy(x, bs=2, kernel_fn=copy_k)
```

```shell
pid = [0] | offs = [0 1], mask = [ True  True], x = [1 2]
pid = [1] | offs = [2 3], mask = [ True  True], x = [3 4]
pid = [2] | offs = [4 5], mask = [ True  True], x = [5 6]
```

Yes!

```python
x, z
```

```shell
(tensor([1, 2, 3, 4, 5, 6]), tensor([1, 2, 3, 4, 5, 6]))
```

æ­£å¦‚æˆ‘ä»¬æ‰€è§ï¼Œç¼–å†™GPUç¨‹åºæ¶‰åŠè®¸å¤šç´¢å¼•ï¼Œæˆ‘ä»¬å¾ˆå®¹æ˜“ææ··ã€‚å› æ­¤ï¼Œæˆ‘å¼ºçƒˆå»ºè®®å…ˆåœ¨æ¨¡æ‹Ÿæ¨¡å¼ä¸‹ç¼–å†™å’Œè°ƒè¯•kernelï¼Œå¹¶é¦–å…ˆä½¿ç”¨å°ç¤ºä¾‹è¿›è¡Œæµ‹è¯•ï¼


# ç¤ºä¾‹2ï¼šç°åº¦åŒ–å›¾åƒ

åœ¨è¿™ä¸ªç¤ºä¾‹ä¸­ï¼Œæˆ‘ä»¬å°†ç°åº¦åŒ–ä¸€å¼ å°ç‹—çš„å›¾åƒã€‚æˆ‘ä»¬å°†çœ‹åˆ°å¦‚ä½•å¤„ç†äºŒç»´æ•°æ®ã€‚

è¿™åŒæ ·é€‚ç”¨äºä¸‰ç»´æ•°æ®ã€‚

æˆ‘ä»¬æ”¹ç¼–äº†Jeremy Howardçš„ç¤ºä¾‹ï¼Œæ¥è‡ªè¿™ä¸ª[colab](https://colab.research.google.com/drive/180uk6frvMBeT4tywhhYXmz3PJaCIA_uk?usp=sharing) / [youtube](https://www.youtube.com/watch?v=4sgKnKbR-WE&feature=youtu.be)ã€‚å› æ­¤ï¼Œæ„Ÿè°¢ä»–çš„ç¤ºä¾‹å’Œé€‰æ‹©çš„å°ç‹—å›¾åƒã€‚
> æ³¨ï¼šåœ¨è¿™ä¸ªç¤ºä¾‹ä¸­ï¼Œå¦‚æœä¸é‡å¯jupyterå†…æ ¸ï¼Œä¼šå‘ç”Ÿä¸¤ä»¶å¥‡æ€ªçš„äº‹æƒ…ï¼š

1. æ— æ³•å¯¼å…¥torchvisionï¼Œå¯èƒ½æ˜¯ç”±äºå¾ªç¯ä¾èµ–ã€‚-> ç›®å‰ä¸çŸ¥é“ä¸ºä»€ä¹ˆï¼Œéœ€è¦æ·±å…¥æŒ–æ˜ã€‚
2. ä¸‹é¢çš„æ¨¡æ‹Ÿtriton kernelå¤±è´¥ï¼Œå› ä¸ºæµ®ç‚¹æ•°ä¸èƒ½ä¹˜ä»¥uintå‘é‡ -> åœ¨GPUä¸Šä¸è¿›è¡Œæ¨¡æ‹Ÿæ—¶å¯ä»¥å·¥ä½œï¼Œæ‰€ä»¥ä¼¼ä¹æ˜¯`TRITON_INTERPRET`çš„bugã€‚

```python
import os

import matplotlib.pyplot as plt
from urllib.request import urlretrieve
from pathlib import Path

import torch
from torch import tensor
import torchvision as tv
import torchvision.transforms.functional as tvf
from torchvision import io

import triton
import triton.language as tl
def cdiv(a,b): return (a + b - 1) // b
url = 'https://upload.wikimedia.org/wikipedia/commons/thumb/4/43/Cute_dog.jpg/1600px-Cute_dog.jpg?20140729055059'
path_img = Path('puppy.jpg')
if not path_img.exists(): urlretrieve(url, path_img)
img = io.read_image('puppy.jpg')
print(img.shape)
img[:2,:3,:4]
```

```shell
torch.Size([3, 1066, 1600])
tensor([[[117, 119, 117, 113],
         [119, 129, 129, 113],
         [130, 126, 122, 115]],

        [[ 83,  85,  85,  80],
         [ 85,  97,  97,  82],
         [ 98,  93,  89,  83]]], dtype=torch.uint8)
```

```python
def show_img(x, figsize=(4,3), **kwargs):
    plt.figure(figsize=figsize)
    plt.axis('off')
    if len(x.shape)==3: x = x.permute(1,2,0)  # CHW -> HWC
    plt.imshow(x.cpu(), **kwargs)
img = tvf.resize(img, 150, antialias=True)
ch,h,w = img.shape
ch,h,w,h*w
```

```shell
(3, 150, 225, 33750)
```

```python
show_img(img)
```

![](https://files.mdnice.com/user/59/63ac6c00-a993-40aa-aa01-afe0a887e153.png)

è¦å¤„ç†äºŒç»´æ•°æ®ï¼Œæˆ‘ä»¬å°†æ„å»ºäºŒç»´åç§»é‡å’Œæ©ç ã€‚ä»¥ä¸‹æ˜¯å¦‚ä½•å·¥ä½œçš„ç¤ºä¾‹ï¼Œä¾‹å¦‚å¯¹äºä¸€ä¸ª `4x7` çŸ©é˜µå’Œæ¯ä¸ªç»´åº¦çš„å¤§å°ä¸º `2` çš„å—ã€‚

![](https://files.mdnice.com/user/59/d5f75553-8b7f-4308-a46e-a9af30a0a70e.png)

åœ¨ä»£ç ä¸­ï¼Œé•¿è¿™æ ·:

```python
@triton.jit
def rgb2grey_k(x_ptr, out_ptr, h, w, bs0: tl.constexpr, bs1: tl.constexpr):
    pid_0 = tl.program_id(0)
    pid_1 = tl.program_id(1)
    
    offs_0 = pid_0 * bs0 + tl.arange(0,bs0)  # 1d å‘é‡
    offs_1 = pid_1 * bs1 + tl.arange(0,bs1)  # 1d å‘é‡

    # å¥‡æ€ªçš„åœ°æ–¹: åœ¨CPUæ¨¡æ‹Ÿæ—¶ï¼ŒNoneåˆ‡ç‰‡ç›®å‰ä¸èµ·ä½œç”¨ã€‚ä½¿ç”¨tl.expand_dimä»£æ›¿ã€‚
    # offs = w * tl.expand_dims(offs_0, 1) + tl.expand_dims(offs_1, 0)
    offs = w * offs_0[:,None] + offs_1[None, :]  # 2d çŸ©é˜µ! - æˆ‘ä»¬å°†ç¬¬ä¸€ä¸ªåç§»é‡ä¹˜ä»¥å®½åº¦ï¼Œè§ä¸Šå›¾

    mask_0 = offs_0 < h  # 1d å‘é‡
    mask_1 = offs_1 < w  # 1d å‘é‡

    # mask = tl.expand_dims(mask_0, 1) & tl.expand_dims(mask_1, 0)
    mask = mask_0[:,None] & mask_1[None,:]  # 2d çŸ©é˜µ! - æ•°æ®ä¸èƒ½è¶…å‡ºä»»ä¸€è½´çš„èŒƒå›´ï¼Œå› æ­¤ä½¿ç”¨`é€»è¾‘ä¸`æ¥ç»„åˆå•ç‹¬çš„æ©ç 
    
    r = tl.load(x_ptr + 0*h*w+offs, mask=mask)
    g = tl.load(x_ptr + 1*h*w+offs, mask=mask)
    b = tl.load(x_ptr + 2*h*w+offs, mask=mask)

    # å¥‡æ€ªçš„åœ°æ–¹: åœ¨CPUæ¨¡æ‹Ÿæ—¶ï¼Œæµ®ç‚¹æ•°ä¸uintå‘é‡ç›¸ä¹˜ä¼šå¤±è´¥
    out = 0.2989*r + 0.5870*g + 0.1140*b  # ä¸ç”¨æ‹…å¿ƒä¸ºä»€ä¹ˆæ˜¯è¿™3ä¸ªæ•°å­—ç›¸ä¹˜

    tl.store(out_ptr + offs, out, mask=mask)
```

è®©æˆ‘ä»¬ä½¿ç”¨è¿™ä¸ªkernel!

```python
def rgb2grey(x, bs):
    c,h,w = x.shape
    out = torch.empty((h,w), dtype=x.dtype, device=x.device)

    # grid å¯ä»¥æ˜¯ä¸€ä¸ªè¿”å› 1d/2d/3d å…ƒç»„çš„å‡½æ•°
    # (åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œæ‹¥æœ‰ä¸€ä¸ª grid å‡½æ•°å¹¶ä¸æ¯” grid å…ƒç»„æ›´æœ‰ç”¨ï¼Œä½†åœ¨ä¸‹é¢çš„åŸºå‡†æµ‹è¯•å’Œè‡ªåŠ¨è°ƒä¼˜ä¸­ä¼šæ›´æœ‰ç”¨)
    grid = lambda meta: (cdiv(h, meta['bs0']), cdiv(w,  meta['bs1']))
    
    rgb2grey_k[grid](x, out, h, w, bs0=bs[0], bs1=bs[1]) # æ‰€æœ‰å…³é”®å­—å‚æ•°éƒ½ä¼ é€’åˆ° grid å‡½æ•°ä¸­
    return out.view(h,w)
grey_img = rgb2grey(img.to('cuda'), bs=(32, 32)).to('cpu')
show_img(grey_img, cmap='gray')
```

![](https://files.mdnice.com/user/59/8f39e5a8-7222-4011-94e2-d0f91d214dd4.png)


# ç¤ºä¾‹ 3: çŸ©é˜µä¹˜æ³•

```python
import os
# os.environ['TRITON_INTERPRET'] = '1'

import torch
import triton
import triton.language as tl

# å°†å®ç”¨å‡½æ•°ç§»åˆ°å•ç‹¬çš„æ–‡ä»¶ä¸­ä»¥æé«˜å¯è¯»æ€§
from triton_util import cdiv, breakpoint_if, print_if, check_tensors_gpu_ready
```

ç°åœ¨ï¼Œè®©æˆ‘ä»¬åœ¨ Triton ä¸­å®ç°ä¸€ä¸ªç®€å•çš„çŸ©é˜µä¹˜æ³•ã€‚æˆ‘ä»¬å°†å­¦ä¹ ï¼š
- ä¸€ç§åˆ†å‰²è®¡ç®—çš„æ–¹æ³•
- ä»kernelä¸­è°ƒç”¨å‡½æ•°
- åœ¨å—å†…ä½¿ç”¨é¢„å®ç°çš„å‘é‡/çŸ©é˜µæ“ä½œ

è¿™æ˜¯ä» [OpenAI å®£å¸ƒ Triton çš„åšå®¢æ–‡ç« ](https://openai.com/research/triton)æ”¹ç¼–è€Œæ¥çš„ã€‚

æˆ‘ä»¬å¸Œæœ›å°† `m x k` çŸ©é˜µ `A` å’Œ `k x n` çŸ©é˜µ `B` ä¹˜ä»¥å¾—åˆ° `m x n` çŸ©é˜µ `C`ã€‚

æˆ‘ä»¬æ²¿ç€ä¸‰ä¸ªè½´åˆ†å‰²è®¡ç®—ï¼š
- æ²¿ç€ m è½´ - æˆ‘ä»¬å°†ä½¿ç”¨å—ç»´åº¦ 0 æ¥è¡¨ç¤ºè¿™ä¸€ç‚¹
- æ²¿ç€ n è½´ - æˆ‘ä»¬å°†ä½¿ç”¨å—ç»´åº¦ 1 æ¥è¡¨ç¤ºè¿™ä¸€ç‚¹
- æ²¿ç€å…±äº«çš„ k è½´ - è¿™å°†ä¸ä¼šç”±å—è¡¨ç¤ºã€‚æ‰€æœ‰è®¡ç®—å—å°†åœ¨åŒä¸€ä¸ªå—ä¸­å®Œæˆã€‚

![](https://files.mdnice.com/user/59/f5a44c41-0d5f-49dc-b1c3-e0770cf61884.png)

ç”±äºæˆ‘ä»¬ç»å¸¸åˆ›å»ºä¸€ç»´æˆ–äºŒç»´åç§»é‡å’Œæ©ç ï¼Œè®©æˆ‘ä»¬å°†è¿™äº›åŠŸèƒ½æ”¾å…¥å®ç”¨å‡½æ•°ä¸­ã€‚åªè¦è¿™äº›å‡½æ•°è¢« `triton.jit` ç¼–è¯‘ï¼Œå®ƒä»¬å°±å¯ä»¥åœ¨kernelä¸­ä½¿ç”¨ã€‚

```python
@triton.jit
def get_1d_offset(size, n_prev_chunks):
    return n_prev_chunks * size + tl.arange(0, size)

@triton.jit
def get_2d_offset(offs_0, offs_1, stride_0, stride_1=1): 
    # ä½¿ç”¨ tl.expand_dims å°† offs_0 å’Œ offs_1 è½¬æ¢ä¸ºäºŒç»´å¼ é‡
    # tl.expand_dims(offs_0, 1) å°† offs_0 è½¬æ¢ä¸º (offs_0, 1) å½¢çŠ¶çš„å¼ é‡
    # tl.expand_dims(offs_1, 0) å°† offs_1 è½¬æ¢ä¸º (1, offs_1) å½¢çŠ¶çš„å¼ é‡
    return tl.expand_dims(offs_0, 1)*stride_0 + tl.expand_dims(offs_1, 0)*stride_1

@triton.jit
def get_1d_mask(offs, max):
    return offs < max

@triton.jit
def get_2d_mask(offs_0, offs_1, max_0, max_1):
    # ä½¿ç”¨ tl.expand_dims å°† offs_0 å’Œ offs_1 è½¬æ¢ä¸ºäºŒç»´å¼ é‡
    # tl.expand_dims(offs_0, 1) å°† offs_0 è½¬æ¢ä¸º (offs_0, 1) å½¢çŠ¶çš„å¼ é‡
    # tl.expand_dims(offs_1, 0) å°† offs_1 è½¬æ¢ä¸º (1, offs_1) å½¢çŠ¶çš„å¼ é‡
    return (tl.expand_dims(offs_0, 1) < max_0) & (tl.expand_dims(offs_1, 0) < max_1)
```

è¿™æ˜¯æœ´ç´ çš„çŸ©é˜µä¹˜æ³•å†…æ ¸ï¼š

```python
@triton.jit
def naive_matmul_k(
    a_ptr, b_ptr, c_ptr,
    m, n, k,
    stride_am, stride_ak, 
    stride_bk, stride_bn,
    stride_cm, stride_cn,
    bm: tl.constexpr, bn: tl.constexpr, bk: tl.constexpr
):
    # è·å–å½“å‰çº¿ç¨‹å—çš„ ID
    pid_m, pid_n = tl.program_id(0), tl.program_id(1)
    # æ²¿ m/n/k ç»´åº¦åˆ†å‰²è®¡ç®—
    rm = get_1d_offset(size=bm, n_prev_chunks=pid_m)  # è®¡ç®— m ç»´åº¦çš„åç§»é‡
    rn = get_1d_offset(size=bn, n_prev_chunks=pid_n)  # è®¡ç®— n ç»´åº¦çš„åç§»é‡
    rk = get_1d_offset(size=bk, n_prev_chunks=0)  # è®¡ç®— k ç»´åº¦çš„åç§»é‡
    # è®¡ç®— a å’Œ b çš„ç›¸å…³åç§»é‡
    offs_a = a_ptr + get_2d_offset(rm, rk, stride_am, stride_ak)  # è®¡ç®— a çš„åç§»é‡
    offs_b = b_ptr + get_2d_offset(rk, rn, stride_bk, stride_bn)  # è®¡ç®— b çš„åç§»é‡
    # åˆå§‹åŒ–å¹¶è¿­ä»£æ›´æ–°ç´¯åŠ å™¨
    acc = tl.zeros((bm, bn), dtype=tl.float32)  # åˆå§‹åŒ–ç´¯åŠ å™¨
    for _ in range(0, k, bk):
        # todo umer: åŠ è½½ a å’Œ b æ—¶æ˜¯å¦éœ€è¦æ©ç ï¼Ÿ
        a = tl.load(offs_a)  # åŠ è½½ a çš„æ•°æ®
        b = tl.load(offs_b)  # åŠ è½½ b çš„æ•°æ®
        acc += tl.dot(a, b, allow_tf32=False)  # åœ¨å—å†…è¿›è¡ŒçŸ©é˜µä¹˜æ³•ï¼›æ³¨æ„ï¼šå¯¹äºè¾ƒæ—§çš„ GPUï¼Œallow_tf32 å¿…é¡»è®¾ç½®ä¸º Falseï¼Œå¦åˆ™æ— æ³•ç¼–è¯‘
        # å¢åŠ åç§»é‡ï¼Œä»¥ä¾¿ä¸‹ä¸€æ¬¡è¿­ä»£åŠ è½½ä¸‹ä¸€ä¸ªå—
        offs_a += bk * stride_ak
        offs_b += bk * stride_bk
    c = c_ptr + get_2d_offset(rm, rn, stride_cm, stride_cn)  # è®¡ç®— c çš„åç§»é‡
    mask = get_2d_mask(rm, rn, m, n)  # è®¡ç®—æ©ç 
    tl.store(c, acc, mask=mask)  # å°†ç»“æœå­˜å‚¨åˆ° c ä¸­
```

```python
from functools import partial

def matmul(a, b, matmul_k_fn, bs=16, group_sz=None):
    # æ£€æŸ¥çŸ©é˜µç»´åº¦æ˜¯å¦å…¼å®¹
    assert a.shape[1] == b.shape[0], "çŸ©é˜µç»´åº¦ä¸å…¼å®¹ï¼Œæ— æ³•è¿›è¡ŒçŸ©é˜µä¹˜æ³•"
    # æ£€æŸ¥å¼ é‡æ˜¯å¦å‡†å¤‡å¥½åœ¨ GPU ä¸Šè¿è¡Œ
    check_tensors_gpu_ready(a, b)
    # è·å–çŸ©é˜µ a å’Œ b çš„å½¢çŠ¶
    (m, k), (_, n) = a.shape, b.shape
    # åˆ›å»ºä¸€ä¸ªç©ºçš„è¾“å‡ºå¼ é‡ c
    c = torch.empty((m, n), device=a.device, dtype=torch.float16)
    # å®šä¹‰ç½‘æ ¼å‡½æ•°ï¼Œç”¨äºè®¡ç®—çº¿ç¨‹å—çš„æ•°é‡
    grid = lambda meta: (triton.cdiv(m, meta['bm']),  triton.cdiv(n, meta['bn']))
    # å¤„ç† group_sz å‚æ•°ï¼Œå¦‚æœä¸º Noneï¼Œåˆ™ä½¿ç”¨ç©ºå­—å…¸
    group_sz = {} if group_sz is None else {"group_sz":group_sz} # åœ¨ naive_matmul ä¸­æœªä½¿ç”¨ï¼Œä½†åœ¨åç»­çš„ grouped_matmul ä¸­ä¼šç”¨åˆ°
    # è°ƒç”¨ matmul_k_fn å‡½æ•°ï¼Œä¼ å…¥å¿…è¦çš„å‚æ•°
    matmul_k_fn[grid](
        a, b, c,
        m, n, k,
        a.stride(0), a.stride(1),
        b.stride(0), b.stride(1),
        c.stride(0), c.stride(1),
        bm=bs, bn=bs, bk=bs, # æ³¨æ„ï¼šå¯¹äºè¾ƒæ—§çš„ GPUï¼Œallow_tf32 å¿…é¡»è®¾ç½®ä¸º Falseï¼Œå¦åˆ™æ— æ³•ç¼–è¯‘
        **group_sz
    )
    # è¿”å›è®¡ç®—ç»“æœ
    return c

# ä½¿ç”¨ partial åˆ›å»ºä¸€ä¸ªéƒ¨åˆ†åº”ç”¨çš„å‡½æ•° naive_matmul
naive_matmul = partial(matmul, matmul_k_fn=naive_matmul_k)
```

```python
a = torch.ones((3, 4), dtype=torch.float32, device='cuda')
b = torch.ones((4, 5), dtype=torch.float32, device='cuda')
```

```python
naive_matmul(a,b)
```

```shell
tensor([[4., 4., 4., 4., 4.],
        [4., 4., 4., 4., 4.],
        [4., 4., 4., 4., 4.]], device='cuda:0', dtype=torch.float16)
```

è®©æˆ‘ä»¬å¯¹ PyTorch çš„å®ç°è¿›è¡Œå•å…ƒæµ‹è¯•

```python
torch.manual_seed(0)
a = torch.randn((512, 512), device='cuda', dtype=torch.float16)
b = torch.randn((512, 512), device='cuda', dtype=torch.float16)
triton_output = naive_matmul(a, b)
torch_output = torch.matmul(a, b)
if torch.allclose(triton_output, torch_output, atol=5e-2, rtol=0):
    print("âœ… Triton and Torch match")
else:
    print("âŒ Triton and Torch differ")
```

âœ… Triton and Torch match

# ç¤ºä¾‹ 4ï¼šæ›´å¿«çš„çŸ©é˜µä¹˜æ³•

Triton å¤„ç†å—å†…çš„å†…å­˜è®¿é—®é¡ºåºï¼Œä½†ä¸å¤„ç†è·¨å—çš„å†…å­˜è®¿é—®é¡ºåºã€‚å› æ­¤ï¼Œè¿™æ˜¯ä¸€ä¸ªæˆ‘ä»¬å¯ä»¥ç”¨æ¥åŠ é€Ÿå†…æ ¸çš„è°ƒèŠ‚ç‚¹ã€‚

äº‹å®ä¸Šï¼Œå·§å¦™åœ°é‡æ–°æ’åºå—å¯ä»¥æé«˜ L2 ç¼“å­˜çš„å‘½ä¸­ç‡ï¼Œä»è€Œä½¿æˆ‘ä»¬çš„å†…æ ¸æ›´å¿«ã€‚è¿™ä¸ªç¤ºä¾‹æ¥è‡ª [Triton æ–‡æ¡£](https://triton-lang.org/main/getting-started/tutorials/03-matrix-multiplication.html)ã€‚
ç°åœ¨ï¼Œä¸ºäº†æ›´å¥½åœ°åˆ©ç”¨ L2 ç¼“å­˜ï¼Œæˆ‘ä»¬å¸Œæœ›é‡ç”¨æœ€è¿‘åŠ è½½çš„æ•°æ®ï¼Œè¿™äº›æ•°æ®å¾ˆå¯èƒ½ä»ç„¶åœ¨ L2 ç¼“å­˜ä¸­ã€‚å¦‚ä½•å®ç°ï¼Ÿé€šè¿‡å‡å°‘ä¸€æ‰¹â€œè¿ç»­â€å†…æ ¸éœ€è¦çš„ä¸åŒæ•°æ®åŠ è½½æ¬¡æ•°ã€‚æˆ‘ä»¬æ‰€è¯´çš„â€œè¿ç»­â€æ˜¯æŒ‡å¤§çº¦åœ¨åŒä¸€æ—¶é—´æ‰§è¡Œçš„å†…æ ¸ã€‚

è¿™å¼ å›¾ï¼ˆæ”¹ç¼–è‡ª [Triton æ–‡æ¡£](https://triton-lang.org/main/getting-started/tutorials/03-matrix-multiplication.html)ï¼‰å±•ç¤ºäº†æˆ‘ä»¬å¦‚ä½•åšåˆ°è¿™ä¸€ç‚¹ã€‚å¦‚æœæŒ‰æœ´ç´ é¡ºåºæ’åˆ—ï¼Œè¾“å‡ºçŸ©é˜µçš„ç¬¬ä¸€è¡Œå°†â€œè¿ç»­â€è®¡ç®—ï¼Œè¿™éœ€è¦ 90 æ¬¡ä¸åŒçš„å—è¯»å–ï¼ˆçŸ©é˜µ A ä¸­ 9 æ¬¡ï¼ŒçŸ©é˜µ B ä¸­ 81 æ¬¡ï¼‰ã€‚å¦‚æœæˆ‘ä»¬ä½¿ç”¨â€œåˆ†ç»„æ’åºâ€ï¼Œè¾“å‡ºçŸ©é˜µçš„ 3x3 å—å°†â€œè¿ç»­â€è®¡ç®—ï¼Œè¿™éœ€è¦ 54 æ¬¡ä¸åŒçš„å—è¯»å–ï¼ˆçŸ©é˜µ A ä¸­ 27 æ¬¡ï¼ŒçŸ©é˜µ B ä¸­ 27 æ¬¡ï¼‰ã€‚

![](https://files.mdnice.com/user/59/5496ef37-6b0c-4cfc-b836-94e9c83ee1f1.png)

æ³¨æ„ï¼šåœ¨æ–‡æ¡£ä¸­ï¼Œåˆ†ç»„ç§°ä¸ºâ€œsuper-groupingâ€ã€‚
å¥½çš„ï¼Œæˆ‘ä»¬å¦‚ä½•å‘Šè¯‰ Triton ä»¥ä½•ç§é¡ºåºå¤„ç†å—ï¼Ÿç­”æ¡ˆæ˜¯ï¼šæˆ‘ä»¬è·å– pidsï¼Œæ”¹å˜å®ƒä»¬ï¼Œå¹¶å°†å®ƒä»¬ç”¨ä½œåŸå§‹ pidsã€‚

è®©æˆ‘ä»¬é€šè¿‡ä¸€ä¸ªæœ€å°ç¤ºä¾‹æ¥è¯´æ˜è¿™ä¸€åŸåˆ™ï¼š

```python
def process_item(id): print(f"I'm processing item {id}")

for i in range(5): process_item(i)
```

```shell
I'm processing item 0
I'm processing item 1
I'm processing item 2
I'm processing item 3
I'm processing item 4
```

```python
def change_id(old_id): return 5-old_id

for i in range(5): process_item(change_id(i))
```

```shell
I'm processing item 5
I'm processing item 4
I'm processing item 3
I'm processing item 2
I'm processing item 1
```

å°±è¿™æ ·ï¼Œé¡¹ç›®ä»¥ä¸åŒçš„é¡ºåºå¤„ç†äº†ã€‚

é‚£ä¹ˆï¼Œç”¨äºæ›´å¿«çŸ©é˜µä¹˜æ³•çš„ pid å˜æ¢å‡½æ•°åº”è¯¥æ˜¯ä»€ä¹ˆæ ·çš„ï¼Ÿå®ƒåº”è¯¥å°†å·¦çŸ©é˜µè½¬æ¢ä¸ºå³çŸ©é˜µã€‚

![](https://files.mdnice.com/user/59/c6b84de9-6cca-4393-83b1-747e80b78eea.png)

åœ¨å·¦ä¾§ï¼Œæ˜¾ç¤ºäº†é»˜è®¤çš„é¡ºåºï¼ˆç§°ä¸ºâ€œè¡Œä¼˜å…ˆâ€ï¼‰ã€‚è¯·æ³¨æ„ï¼Œæˆ‘ä»¬å¤„ç†çš„æ˜¯å—ã€‚æˆ‘ä»¬æ— æ³•å®‰æ’å•ä¸ªå•å…ƒæ ¼çš„å¤„ç†é¡ºåºï¼Œåªèƒ½å®‰æ’å—çš„é¡ºåºã€‚åœ¨å›¾ä¸­ï¼Œæˆ‘ä»¬çš„è¾“å‡ºçŸ©é˜µ C æœ‰ `5x7 = 35` ä¸ªå•å…ƒæ ¼ï¼Œä½†åªæœ‰ `cdiv(5,1) x cdiv(7,2) = 5x4 = 20` ä¸ªå—ã€‚

åœ¨å³ä¾§ï¼Œæ³¨æ„å‰ 9 ä¸ªå¤„ç†çš„å—æ˜¯æˆ‘ä»¬æƒ³è¦çš„ `3x3` ç½‘æ ¼ï¼æˆ‘ä»¬åœ¨ä¸€åˆ—ä¸­å¤„ç† 3 ä¸ªå—ã€‚ç„¶åå‰è¿›ä¸€åˆ—ï¼Œå†æ¬¡å¤„ç† 3 ä¸ªå—ï¼Œå¦‚æ­¤å¾ªç¯ã€‚æ©™è‰²çº¿æ˜¾ç¤ºäº†å‰è¿›çš„ä½ç½®ã€‚è¿™ä¸ªæ“ä½œç§°ä¸º **"swizzling"**ã€‚

é¡ºä¾¿è¯´ä¸€ä¸‹ï¼Œä½ å¯ä»¥å½“ç„¶æ”¹å˜æ•°å­— 3ã€‚å®ƒè¢«ç§°ä¸º `group_size`ã€‚

ä½ ä¸éœ€è¦è‡ªå·±ç¼–å†™ swizzlingï¼Œå› ä¸º Triton æä¾›äº†ä¸€ä¸ª `triton.language.swizzle2d` å‡½æ•°ã€‚

ä¸ºäº†çœŸæ­£ç†è§£ `swizzle2d`ï¼Œæˆ‘ä»¬å¿«é€ŸéªŒè¯å®ƒæ˜¯å¦æŒ‰é¢„æœŸå·¥ä½œã€‚ç„¶åæˆ‘ä»¬å°†åœ¨æ›´å¿«çš„çŸ©é˜µä¹˜æ³•kernelä¸­ç»§ç»­ä½¿ç”¨å®ƒã€‚

é™„å¸¦ç›®æ ‡ï¼šåœ¨ä¸€ä¸ª `5x4` çš„çŸ©é˜µä¸Šä½¿ç”¨ `swizzle2d`ï¼Œè¯¥çŸ©é˜µçš„å…ƒç´ æŒ‰è¡Œä¼˜å…ˆé¡ºåºæ’åˆ—ä¸º `0 ... 19`ã€‚æˆ‘ä»¬åº”è¯¥å¾—åˆ°ä¸€ä¸ªå…ƒç´ æŒ‰åˆ†ç»„é¡ºåºæ’åˆ—çš„çŸ©é˜µã€‚

```python
@triton.jit
def swizzle_k(x_ptr, z_ptr, group_sz: tl.constexpr):
    # è·å–å½“å‰çº¿ç¨‹å—çš„ ID
    pid_m, pid_n = tl.program_id(0), tl.program_id(1)
    # è·å–çº¿ç¨‹å—çš„æ€»æ•°
    num_pid_m, num_pid_n = tl.num_programs(0), tl.num_programs(1)

    # ä½¿ç”¨ Triton çš„ swizzle2d å‡½æ•°é‡æ–°æ’åˆ—çº¿ç¨‹å—çš„ ID
    # æ³¨æ„ï¼šåœ¨ CPU æ¨¡æ‹Ÿæ—¶ï¼Œtl.swizzle2d å¯èƒ½æ— æ³•æ­£å¸¸å·¥ä½œ
    pid_m_, pid_n_ = tl.swizzle2d(pid_m, pid_n, num_pid_m, num_pid_n, group_sz)
    
    # è®¡ç®—åŸå§‹çº¿ç¨‹å—çš„åç§»é‡
    offs_m = get_1d_offset(1, n_prev_chunks=pid_m)
    offs_n = get_1d_offset(1, n_prev_chunks=pid_n)
    
    # è®¡ç®—åŸå§‹çº¿ç¨‹å—çš„ 2D åç§»é‡å’Œæ©ç 
    offs = get_2d_offset(offs_m, offs_n, stride_0=num_pid_n)
    mask = get_2d_mask(offs_m, offs_n, max_0=num_pid_m, max_1=num_pid_n )

    # è®¡ç®—é‡æ–°æ’åˆ—åçš„çº¿ç¨‹å—çš„åç§»é‡
    offs_sw_m = get_1d_offset(1, n_prev_chunks=pid_m_)
    offs_sw_n = get_1d_offset(1, n_prev_chunks=pid_n_)
    
    # è®¡ç®—é‡æ–°æ’åˆ—åçš„çº¿ç¨‹å—çš„ 2D åç§»é‡å’Œæ©ç 
    offs_sw = get_2d_offset(offs_sw_m, offs_sw_n, stride_0=num_pid_n)
    mask_sw = get_2d_mask(offs_sw_m, offs_sw_n, max_0=num_pid_m, max_1=num_pid_n)
    
    # ä»åŸå§‹çŸ©é˜µä¸­åŠ è½½æ•°æ®
    x = tl.load(x_ptr + offs, mask=mask)
    # å°†æ•°æ®å­˜å‚¨åˆ°é‡æ–°æ’åˆ—åçš„çŸ©é˜µä¸­
    tl.store(z_ptr + offs_sw, x, mask=mask_sw)
```

```python
blocks_m, blocks_n = 5,4

x = torch.arange(blocks_m*blocks_n, device='cuda').view(blocks_m,blocks_n)
x
```

```shell
tensor([[ 0,  1,  2,  3],
        [ 4,  5,  6,  7],
        [ 8,  9, 10, 11],
        [12, 13, 14, 15],
        [16, 17, 18, 19]], device='cuda:0')
```

```python
z = -torch.ones_like(x) # empty matrix, with -1 denoting empty
z
```

```shell
tensor([[-1, -1, -1, -1],
        [-1, -1, -1, -1],
        [-1, -1, -1, -1],
        [-1, -1, -1, -1],
        [-1, -1, -1, -1]], device='cuda:0')
```

```python
# swizzle x into z
swizzle_k[(blocks_m,blocks_n)](x,z, group_sz=3);
z
```

```shell
tensor([[ 0,  3,  6,  9],
        [ 1,  4,  7, 10],
        [ 2,  5,  8, 11],
        [12, 14, 16, 18],
        [13, 15, 17, 19]], device='cuda:0')
```

çœ‹èµ·æ¥ä¸é”™ï¼

___


ç°åœ¨æˆ‘ä»¬æ¥å®ç° grouped çŸ©é˜µä¹˜æ³•kernelï¼Œè¿™å°†æ¯”æ™®é€šçš„çŸ©é˜µä¹˜æ³•æ›´å¿«ã€‚

```python
@triton.jit
def grouped_matmul_k(
    a_ptr, b_ptr, c_ptr,  # æŒ‡å‘çŸ©é˜µ A, B, C çš„æŒ‡é’ˆ
    m, n, k,  # çŸ©é˜µçš„ç»´åº¦
    stride_am, stride_ak,  # çŸ©é˜µ A çš„æ­¥é•¿
    stride_bk, stride_bn,  # çŸ©é˜µ B çš„æ­¥é•¿
    stride_cm, stride_cn,  # çŸ©é˜µ C çš„æ­¥é•¿
    bm: tl.constexpr, bn: tl.constexpr, bk: tl.constexpr, group_sz: tl.constexpr  # å—å¤§å°å’Œåˆ†ç»„å¤§å°
):
    pid_m, pid_n = tl.program_id(0), tl.program_id(1)  # è·å–å½“å‰çº¿ç¨‹å—çš„ ID
    num_pid_m, num_pid_n = tl.num_programs(0), tl.num_programs(1)  # è·å–çº¿ç¨‹å—çš„æ€»æ•°
    # ç¡®å®šå—åœ¨åˆ†ç»„æ’åºä¸­çš„ä½ç½® - é‡æ–°æ’åˆ—ï¼
    pid_m, pid_n = tl.swizzle2d(pid_m, pid_n, num_pid_m, num_pid_n, group_sz)  # å¥‡æ€ªçš„åœ°æ–¹ï¼štl.swizzle2d åœ¨ CPU æ¨¡æ‹Ÿæ—¶ä¸èµ·ä½œç”¨
    # æ²¿ m/n/k ç»´åº¦çš„å—
    rm = get_1d_offset(size=bm, n_prev_chunks=pid_m)  # è®¡ç®— m ç»´åº¦çš„åç§»
    rn = get_1d_offset(size=bn, n_prev_chunks=pid_n)  # è®¡ç®— n ç»´åº¦çš„åç§»
    rk = get_1d_offset(size=bk, n_prev_chunks=0)  # è®¡ç®— k ç»´åº¦çš„åç§»
    # çŸ©é˜µ A å’Œ B çš„ç›¸å…³åç§»
    offs_a = a_ptr + get_2d_offset(rm, rk, stride_am, stride_ak)  # è®¡ç®—çŸ©é˜µ A çš„åç§»
    offs_b = b_ptr + get_2d_offset(rk, rn, stride_bk, stride_bn)  # è®¡ç®—çŸ©é˜µ B çš„åç§»
    # åˆå§‹åŒ–å¹¶è¿­ä»£æ›´æ–°ç´¯åŠ å™¨
    acc = tl.zeros((bm, bn), dtype=tl.float32)  # åˆå§‹åŒ–ç´¯åŠ å™¨
    for _ in range(0, k, bk):
        # todo umer: åŠ è½½ a & b æ—¶æ˜¯å¦éœ€è¦æ©ç ï¼Ÿ
        a = tl.load(offs_a)  # åŠ è½½çŸ©é˜µ A çš„å—
        b = tl.load(offs_b)  # åŠ è½½çŸ©é˜µ B çš„å—
        acc += tl.dot(a, b, allow_tf32=False)  # å—çº§åˆ«çš„çŸ©é˜µä¹˜æ³•ï¼›å¥‡æ€ªçš„åœ°æ–¹ï¼šå¯¹äºè¾ƒæ—§çš„ GPUï¼Œallow_tf32 å¿…é¡»è®¾ç½®ä¸º Falseï¼Œå¦åˆ™æ— æ³•ç¼–è¯‘
        # å¢åŠ åç§»ï¼Œä»¥ä¾¿ä¸‹ä¸€æ¬¡è¿­ä»£åŠ è½½ä¸‹ä¸€ä¸ªå—
        offs_a += bk * stride_ak
        offs_b += bk * stride_bk
    c = c_ptr + get_2d_offset(rm, rn, stride_cm, stride_cn)  # è®¡ç®—çŸ©é˜µ C çš„åç§»
    mask = get_2d_mask(rm, rn, m, n)  # è®¡ç®—æ©ç 
    tl.store(c, acc, mask=mask)  # å°†ç´¯åŠ å™¨çš„ç»“æœå­˜å‚¨åˆ°çŸ©é˜µ C ä¸­
```

```python
grouped_matmul = partial(matmul, matmul_k_fn=grouped_matmul_k)
```

```python
a = torch.ones((3, 4), dtype=torch.float32, device='cuda')
b = torch.ones((4, 5), dtype=torch.float32, device='cuda')
```

```python
grouped_matmul(a,b, group_sz=4)
```

```shell
tensor([[4., 4., 4., 4., 4.],
        [4., 4., 4., 4., 4.],
        [4., 4., 4., 4., 4.]], device='cuda:0', dtype=torch.float16)
```

è®©æˆ‘ä»¬å¯¹ PyTorch çš„å®ç°è¿›è¡Œå•å…ƒæµ‹è¯•

```python
torch.manual_seed(0)
a = torch.randn((512, 512), device='cuda', dtype=torch.float16)
b = torch.randn((512, 512), device='cuda', dtype=torch.float16)
triton_output = grouped_matmul(a, b, group_sz=32)
torch_output = torch.matmul(a, b)
if torch.allclose(triton_output, torch_output, atol=5e-2, rtol=0):
    print("âœ… Triton and Torch match")
else:
    print("âŒ Triton and Torch differ")
```

âœ… Triton and Torch match

# æ€§èƒ½æµ‹è¯•

Triton è‡ªå¸¦æ€§èƒ½æµ‹è¯•å·¥å…·ã€‚ä»¥ä¸‹æ˜¯ä¸€ä¸ªä½¿ç”¨ç¤ºä¾‹ã€‚

```python
# adapted from https://triton-lang.org/main/getting-started/tutorials/01-vector-add.html
@triton.testing.perf_report(
    triton.testing.Benchmark(
        x_names=['square_matrix_size'],  # ç”¨äºç»˜å›¾çš„ x è½´å‚æ•°åç§°ã€‚
        x_vals=[2**i for i in range(5, 12, 1)],  # `x_name` çš„ä¸åŒå¯èƒ½å€¼ã€‚
        x_log=True,  # x è½´ä¸ºå¯¹æ•°åˆ»åº¦ã€‚
        line_arg='provider',  # å¯¹åº”äºç»˜å›¾ä¸­ä¸åŒçº¿æ¡çš„å‚æ•°åç§°ã€‚
        line_vals=['naive', 'grouped', 'torch'],  # `line_arg` çš„å¯èƒ½å€¼ã€‚
        line_names=['Naive', 'Grouped', 'Torch'],  # çº¿æ¡çš„æ ‡ç­¾åç§°ã€‚
        styles=[('blue', '-'), ('green', '-'), ('orange','-')],  # çº¿æ¡æ ·å¼ã€‚
        ylabel='GB/s',  # y è½´çš„æ ‡ç­¾åç§°ã€‚
        plot_name='matmul-performance',  # ç»˜å›¾çš„åç§°ï¼Œä¹Ÿç”¨ä½œä¿å­˜ç»˜å›¾çš„æ–‡ä»¶åã€‚
        args={},  # ä¸åœ¨ `x_names` å’Œ `y_name` ä¸­çš„å‡½æ•°å‚æ•°å€¼ã€‚
    ))
def benchmark(square_matrix_size, provider):
    sz = square_matrix_size  # çŸ©é˜µçš„å¤§å°
    a = torch.rand((sz, sz), device='cuda', dtype=torch.float32)  # ç”ŸæˆéšæœºçŸ©é˜µ a
    b = torch.rand((sz, sz), device='cuda', dtype=torch.float32)  # ç”ŸæˆéšæœºçŸ©é˜µ b
    quantiles = [0.5, 0.2, 0.8]  # ç”¨äºæ€§èƒ½æµ‹è¯•çš„åˆ†ä½æ•°
    if provider == 'naive':  # å¦‚æœä½¿ç”¨ naive æ–¹æ³•
        ms, min_ms, max_ms = triton.testing.do_bench(lambda: naive_matmul(a, b), quantiles=quantiles)  # æ‰§è¡Œæ€§èƒ½æµ‹è¯•
    if provider == 'grouped':  # å¦‚æœä½¿ç”¨ grouped æ–¹æ³•
        ms, min_ms, max_ms = triton.testing.do_bench(lambda: grouped_matmul(a, b, group_sz=8), quantiles=quantiles)  # æ‰§è¡Œæ€§èƒ½æµ‹è¯•
    if provider == 'torch':  # å¦‚æœä½¿ç”¨ PyTorch æ–¹æ³•
        ms, min_ms, max_ms = triton.testing.do_bench(lambda: torch.matmul(a, b), quantiles=quantiles)  # æ‰§è¡Œæ€§èƒ½æµ‹è¯•
    gbps = lambda ms: 12 * sz / ms * 1e-6  # è®¡ç®—å¸¦å®½ï¼ˆGB/sï¼‰
    return gbps(ms), gbps(max_ms), gbps(min_ms)  # è¿”å›å¸¦å®½å€¼
```

> ä¸ªäººæ„Ÿè§‰è¿™é‡Œçš„gbpså…¬å¼æœ‰é”™è¯¯ï¼Œåº”è¯¥æ˜¯12 * sz^2 / ms * 1e-6 æ‰å¯¹ï¼Ÿä¸‹é¢ç»™å‡ºäº†Deepseek v2.5çš„æ¨å¯¼ï¼š

![](https://files.mdnice.com/user/59/b5c9cade-b0b5-4fe9-b39e-7ae1ed8d0d21.png)

```python
benchmark.run(print_data=True, show_plots=True)
```

![](https://files.mdnice.com/user/59/6d39aaa8-8b02-4587-9a4a-f9ea211ac010.png)

```shell
matmul-performance:
   square_matrix_size     Naive   Grouped     Torch
0                32.0  0.085106  0.085106  0.053691
1                64.0  0.129730  0.125000  0.107143
2               128.0  0.159468  0.154341  0.170515
3               256.0  0.097909  0.099071  0.125654
4               512.0  0.030346  0.030361  0.111079
5              1024.0  0.006971  0.007279  0.034461
6              2048.0  0.001405  0.001749  0.006355
```

æ³¨ Umer: æˆ‘æœ¬ä»¥ä¸ºéšç€çŸ©é˜µå¤§å°çš„å¢åŠ ï¼ŒGB/s ä¼šå¢åŠ ã€‚ä¸ºä»€ä¹ˆæ²¡æœ‰ï¼Ÿå¯èƒ½æ˜¯å› ä¸ºå…±äº«å†…å­˜å·²æ»¡ï¼Œæ‰€ä»¥kernelèŠ±è´¹äº†è¶Šæ¥è¶Šå¤šçš„æ—¶é—´é‡æ–°åŠ è½½æ•°æ®ã€‚

è®©æˆ‘ä»¬å°è¯•ä¸åŒçš„å—å¤§å°ï¼š

```python
@triton.testing.perf_report(
    triton.testing.Benchmark(
        x_names=['batch_size'], x_vals=[2**i for i in range(4, 7, 1)], x_log=True,
        line_arg='provider', line_vals=['naive', 'grouped', 'torch'], line_names=['Naive', 'Grouped', 'Torch'],
        styles=[('blue', '-'), ('green', '-'), ('orange','-')],
        ylabel='GB/s', plot_name='matmul-performance', args={}
    ))
def benchmark(batch_size, provider):
    sz = 512
    a = torch.rand((sz, sz), device='cuda', dtype=torch.float32)
    b = torch.rand((sz, sz), device='cuda', dtype=torch.float32)
    quantiles = [0.5, 0.2, 0.8]
    if provider == 'naive':   ms, min_ms, max_ms = triton.testing.do_bench(lambda: naive_matmul(a, b, bs=batch_size), quantiles=quantiles)
    if provider == 'grouped': ms, min_ms, max_ms = triton.testing.do_bench(lambda: grouped_matmul(a, b, bs=batch_size, group_sz=8), quantiles=quantiles)
    if provider == 'torch':   ms, min_ms, max_ms = triton.testing.do_bench(lambda: torch.matmul(a,b), quantiles=quantiles)
    gbps = lambda ms: 12 * sz / ms * 1e-6
    return gbps(ms), gbps(max_ms), gbps(min_ms)

benchmark.run(print_data=True, show_plots=True)
```

![](https://files.mdnice.com/user/59/d6839175-8259-4a99-9ff8-c75b2ea74f4d.png)

```shell
matmul-performance:
   batch_size     Naive   Grouped     Torch
0        16.0  0.030404  0.030433  0.111111
1        32.0  0.060683  0.061127  0.111111
2        64.0  0.083660  0.084026  0.111111
```

æ›´å¤§çš„å—å¤§å°ä¼¼ä¹æ›´å¥½ã€‚è®©æˆ‘ä»¬å†æ¬¡ä¸ PyTorch è¿›è¡Œæ¯”è¾ƒï¼Œä½¿ç”¨æ›´å¤§çš„å—å¤§å°ã€‚

```python
@triton.testing.perf_report(
    triton.testing.Benchmark(
        x_names=['square_matrix_size'], x_vals=[2**i for i in range(5, 12, 1)], x_log=True,
        line_arg='provider', line_vals=['naive', 'grouped', 'torch'], line_names=['Naive', 'Grouped', 'Torch'],
        styles=[('blue', '-'), ('green', '-'), ('orange','-')],
        ylabel='GB/s', plot_name='matmul-performance', args={}
    ))
def benchmark(square_matrix_size, provider):
    sz = square_matrix_size
    a = torch.rand((sz, sz), device='cuda', dtype=torch.float32)
    b = torch.rand((sz, sz), device='cuda', dtype=torch.float32)
    quantiles = [0.5, 0.2, 0.8]
    if provider == 'naive':   ms, min_ms, max_ms = triton.testing.do_bench(lambda: naive_matmul(a, b, bs=64), quantiles=quantiles)
    if provider == 'grouped': ms, min_ms, max_ms = triton.testing.do_bench(lambda: grouped_matmul(a, b, group_sz=8, bs=64), quantiles=quantiles)
    if provider == 'torch':   ms, min_ms, max_ms = triton.testing.do_bench(lambda: torch.matmul(a,b), quantiles=quantiles)
    gbps = lambda ms: 12 * sz / ms * 1e-6
    return gbps(ms), gbps(max_ms), gbps(min_ms)

benchmark.run(print_data=True, show_plots=True)
```

![](https://files.mdnice.com/user/59/eb9d5d16-376d-4cbc-b575-be02d3b5997d.png)

```shell
matmul-performance:
   square_matrix_size     Naive   Grouped     Torch
0                32.0  0.039867  0.038710  0.053215
1                64.0  0.077922  0.071006  0.106667
2               128.0  0.109091  0.107143  0.169912
3               256.0  0.137733  0.136364  0.126150
4               512.0  0.084731  0.083916  0.111047
5              1024.0  0.021879  0.025362  0.034691
6              2048.0  0.005257  0.005919  0.007440
```

è¿™å‡å°‘äº†è¾ƒå¤§çŸ©é˜µå°ºå¯¸ä¸‹ä¸ PyTorch çš„æ€§èƒ½å·®è·ï¼Œä½† PyTorch ä»ç„¶æ›´å¥½ã€‚

æç¤ºï¼šå¯¹äºæ€§èƒ½åˆ†æï¼Œæˆ‘ä»¬å¯ä»¥ä½¿ç”¨ Nsight Compute æ¥åˆ†ææˆ‘ä»¬çš„kernelï¼š
`ncu --target-processes all your_python_file.py`

# è‡ªåŠ¨è°ƒä¼˜

æ”¹ç¼–è‡ª https://triton-lang.org/main/getting-started/tutorials/03-matrix-multiplication.html

å…ƒå‚æ•°ï¼ˆä¾‹å¦‚å—å¤§å°ï¼‰å’Œç¼–è¯‘é€‰é¡¹ï¼ˆä¾‹å¦‚ `num_warps`ï¼‰çš„é€‰æ‹©ä¼šå½±å“kernelçš„é€Ÿåº¦ã€‚Triton å…è®¸ä½ ä¼ é€’ä¸€ä¸ªå¯èƒ½é€‰æ‹©çš„åˆ—è¡¨ï¼Œè¿è¡Œæ‰€æœ‰è¿™äº›é€‰æ‹©ï¼Œç„¶åä¸ºæœ€å¿«çš„é€‰æ‹©ç¼–è¯‘kernelã€‚è¿™ç§°ä¸º `è‡ªåŠ¨è°ƒä¼˜`ã€‚

å¦‚æœé—®é¢˜çš„å¤§å°å‘ç”Ÿå˜åŒ–ï¼ˆä¾‹å¦‚çŸ©é˜µå¤§å°å˜åŒ–ï¼‰ï¼Œå°†ä¸ºæ–°çš„é—®é¢˜å¤§å°è¿›è¡Œæ–°çš„è‡ªåŠ¨è°ƒä¼˜ã€‚

```python
@triton.autotune(
    # Choices of configs to auto-tune over
    configs=[
        triton.Config({'bm': 128, 'bn': 256, 'bk': 64, 'group_sz': 8}, num_stages=3, num_warps=8),
        triton.Config({'bm': 64, 'bn': 256, 'bk': 32, 'group_sz': 8}, num_stages=4, num_warps=4),
        triton.Config({'bm': 128, 'bn': 128, 'bk': 32, 'group_sz': 8}, num_stages=4, num_warps=4),
        triton.Config({'bm': 128, 'bn': 64, 'bk': 32, 'group_sz': 8}, num_stages=4, num_warps=4),
        triton.Config({'bm': 64, 'bn': 128, 'bk': 32, 'group_sz': 8}, num_stages=4, num_warps=4),
        triton.Config({'bm': 128, 'bn': 32, 'bk': 32, 'group_sz': 8}, num_stages=4, num_warps=4),
        triton.Config({'bm': 64, 'bn': 32, 'bk': 32, 'group_sz': 8}, num_stages=5, num_warps=2),
        triton.Config({'bm': 32, 'bn': 64, 'bk': 32, 'group_sz': 8}, num_stages=5, num_warps=2),
    ],
    # Definition of problem size. If it changes, a new auto-tune is run for the new problem size.
    key=['m', 'n', 'k'],
)
@triton.jit
def grouped_autotuned_matmul_k(
    a_ptr, b_ptr, c_ptr,
    m, n, k,
    stride_am, stride_ak, 
    stride_bk, stride_bn,
    stride_cm, stride_cn,
    bm: tl.constexpr, bn: tl.constexpr, bk: tl.constexpr, group_sz: tl.constexpr
):
    pid_m = tl.program_id(0)
    pid_n = tl.program_id(1)
    num_pid_m = tl.num_programs(0)
    num_pid_n = tl.num_programs(1)
    # determine location of block in grouped ordering
    pid_m, pid_n = tl.swizzle2d(pid_m, pid_n, num_pid_m, num_pid_n, group_sz)  # Weirdness: tl.swizzle2d doesn't work when simulating on CPU
    # chunks along m/n/k dimensions
    rm = get_1d_offset(size=bm, n_prev_chunks=pid_m)
    rn = get_1d_offset(size=bn, n_prev_chunks=pid_n)
    rk = get_1d_offset(size=bk, n_prev_chunks=0)
    # relevant offsets of a, b
    offs_a = a_ptr + get_2d_offset(rm, rk, stride_am, stride_ak)
    offs_b = b_ptr + get_2d_offset(rk, rn, stride_bk, stride_bn)
    # initialize and iteratively update accumulator
    acc = tl.zeros((bm, bn), dtype=tl.float32)
    for _ in range(0, k, bk):
        # todo umer: don't we need mask when loading a & b?
        a = tl.load(offs_a)
        b = tl.load(offs_b)
        acc += tl.dot(a, b, allow_tf32=False) # block level matrix multiplication ; Weirdness: allow_tf32 must be set to False for older GPUs, otherwise won't compile
        # increase offets, so next iteration loads next chunks
        offs_a += bk * stride_ak
        offs_b += bk * stride_bk
    c = c_ptr + get_2d_offset(rm, rn, stride_cm, stride_cn)
    mask = get_2d_mask(rm, rn, m, n)
    tl.store(c, acc, mask=mask)

def grouped_autotuned_matmul(a, b):
    matmul_k_fn = grouped_autotuned_matmul_k
    
    assert a.shape[1] == b.shape[0], "matrix dims not compatible for matmul"
    check_tensors_gpu_ready(a, b)
    (m, k), (_, n) = a.shape, b.shape
    c = torch.empty((m, n), device=a.device, dtype=torch.float16)
    grid = lambda meta: (triton.cdiv(m, meta['bm']),  triton.cdiv(n, meta['bn']))
    matmul_k_fn[grid](
        a, b, c,
        m, n, k,
        a.stride(0), a.stride(1),
        b.stride(0), b.stride(1),
        c.stride(0), c.stride(1),
        # bm=bs, bn=bs, bk=bs, <- will be autotuned
        # **group_sz <- will be autotuned
    )
    return c
```

```
a,b = torch.ones(3,4, device='cuda'), torch.ones(4,5, device='cuda')
a@b
```

```shell
tensor([[4., 4., 4., 4., 4.],
        [4., 4., 4., 4., 4.],
        [4., 4., 4., 4., 4.]], device='cuda:0')
```

æ³¨æ„ï¼šæœ‰æ—¶ä»¥ä¸‹è¡Œä¼šè¿”å›é”™è¯¯çš„ç»“æœï¼Œè€Œä¸”æˆ‘æ— æ³•å¯é åœ°é‡ç°è¿™ä¸ªé—®é¢˜ã€‚å¦‚æœæ‚¨èƒ½é‡ç°ï¼Œè¯·é€šè¿‡ Twitter (@UmerHAdil) å‘Šè¯‰æˆ‘ï¼ğŸ™ğŸ½

```python
grouped_autotuned_matmul(a,b)
```

```shell
tensor([[4., 4., 4., 4., 4.],
        [4., 4., 4., 4., 4.],
        [4., 4., 4., 4., 4.]], device='cuda:0', dtype=torch.float16)
```

å…³äºè‡ªåŠ¨è°ƒä¼˜çš„é…ç½®å»ºè®®ã€æŠ€å·§å’Œå¯å‘å¼æ–¹æ³•ï¼Œè¯·å‚è§ [Mark Saroufim çš„æ¼”è®² "CUDA Performance Checklist"](https://www.youtube.com/watch?v=SGhfUhlowB4)ã€‚å…¶ä¸­çš„è®¸å¤šå†…å®¹ä¹Ÿé€‚ç”¨äº Tritonã€‚

è®©æˆ‘ä»¬å†æ¬¡è¿è¡ŒåŸºå‡†æµ‹è¯•ã€‚è¿™å°†èŠ±è´¹å¾ˆå¤šæ—¶é—´ï¼Œå› ä¸ºæˆ‘ä»¬å°†ä¸ºæ¯ä¸ªåŸºå‡†æµ‹è¯•å‚æ•°é€‰æ‹©è¿›è¡Œè‡ªåŠ¨è°ƒä¼˜ï¼ˆå³ï¼Œå¯¹æˆ‘ä»¬æ¥è¯´æ˜¯ 12-5=7 æ¬¡ï¼‰ã€‚

```python
@triton.testing.perf_report(
    triton.testing.Benchmark(
        x_names=['square_matrix_size'], x_vals=[2**i for i in range(5, 12, 1)], x_log=True,
        line_arg='provider', line_vals=['naive', 'grouped', 'grouped-autotuned', 'torch'], line_names=['Naive', 'Grouped', 'Grouped & Auto-Tuned','Torch'],
        styles=[('blue', '-'), ('green', '-'), ('green', '--'), ('orange','-')],
        ylabel='GB/s', plot_name='matmul-performance', args={}
    ))
def benchmark(square_matrix_size, provider):
    sz = square_matrix_size
    a = torch.rand((sz, sz), device='cuda', dtype=torch.float32)
    b = torch.rand((sz, sz), device='cuda', dtype=torch.float32)
    quantiles = [0.5, 0.2, 0.8]
    if provider == 'naive':   ms, min_ms, max_ms = triton.testing.do_bench(lambda: naive_matmul(a, b, bs=64), quantiles=quantiles)
    if provider == 'grouped': ms, min_ms, max_ms = triton.testing.do_bench(lambda: grouped_matmul(a, b, group_sz=8, bs=64), quantiles=quantiles)
    if provider == 'grouped-autotuned': ms, min_ms, max_ms = triton.testing.do_bench(lambda: grouped_autotuned_matmul(a, b), quantiles=quantiles)
    if provider == 'torch':   ms, min_ms, max_ms = triton.testing.do_bench(lambda: torch.matmul(a,b), quantiles=quantiles)
    gbps = lambda ms: 12 * sz / ms * 1e-6
    return gbps(ms), gbps(max_ms), gbps(min_ms)

benchmark.run(print_data=True, show_plots=True)
```

![](https://files.mdnice.com/user/59/d5f8a2ad-2729-47db-9db1-2201271bdd63.png)

```shell
matmul-performance:
   square_matrix_size     Naive   Grouped  Grouped & Auto-Tuned     Torch
0                32.0  0.040067  0.037500              0.062176  0.054795
1                64.0  0.077170  0.074303              0.091954  0.104803
2               128.0  0.110218  0.107143              0.117936  0.169912
3               256.0  0.139738  0.136364              0.137339  0.126482
4               512.0  0.083953  0.082937              0.066864  0.110983
5              1024.0  0.023112  0.025932              0.020007  0.033520
6              2048.0  0.005235  0.005912              0.004629  0.007076
```

___
<h1>è¿™å°±æ˜¯å…¨éƒ¨å†…å®¹ï¼æ­å–œä½ å®Œæˆäº†æœ¬æ•™ç¨‹ - Good workï¼ğŸ¥³</h1>

æˆ‘å¼ºçƒˆå»ºè®®ä½ è‡ªå·±ç¼–å†™ä¸€äº› Triton kernelã€‚ä¾‹å¦‚ï¼Œä½ å¯ä»¥å°è¯•è¿™äº› Triton è°œé¢˜ï¼šhttps://github.com/srush/Triton-Puzzlesï¼Œç”± [Sasha Rush](https://twitter.com/srush_nlp)ã€Tejas Ramesh å’Œ [Keren Zhou](https://twitter.com/ZhouKeren) æä¾›ã€‚

è¿™é‡Œæœ‰ä¸€äº›ä¸­çº§å’Œé«˜çº§ææ–™ï¼š
- å®˜æ–¹æ–‡æ¡£ï¼šhttps://triton-lang.org/
- LightLLM ä»“åº“åŒ…å«äº†è®¸å¤šå®é™…çš„ Triton kernelï¼šhttps://github.com/ModelTC/lightllm/tree/main/lightllm/common/basemodel/triton_kernel
- Unsloth ä»“åº“ä¹ŸåŒ…å«äº†è®¸å¤šå®é™…çš„ Triton kernelï¼šhttps://github.com/unslothai/unsloth/tree/main/unsloth/kernels
å¦‚æœä½ å¯¹ GPU ç¼–ç¨‹å’Œæ€§èƒ½ä¼˜åŒ–æ„Ÿå…´è¶£ï¼Œ[cuda mode Discord](https://discord.gg/cudamode) å¯èƒ½å¯¹ä½ æœ‰å¸®åŠ©ã€‚æœ¬æ•™ç¨‹æ˜¯ä½œä¸ºä»–ä»¬ç²¾å½©çš„ [è®²åº§ç³»åˆ—](https://www.youtube.com/@CUDAMODE) çš„ä¸€éƒ¨åˆ†ç¼–å†™çš„ã€‚


